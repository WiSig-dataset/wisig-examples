{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import scipy,scipy.spatial\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "from  data_utilities import *\n",
    "# from definitions import *\n",
    "# from run_train_eval_net import run_train_eval_net,run_eval_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'ManySig'\n",
    "dataset_path='../../orbit_rf_dataset/data/compact_pkl_datasets/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = [compact_dataset['rx_list'][0]]\n",
    "\n",
    "equalized = 1\n",
    "\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 2)]          0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 256, 2, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 256, 2, 8)         56        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 2, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 2, 16)        784       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 2, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 2, 16)         1552      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 1, 32)         1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 1, 16)         1552      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 39,778\n",
      "Trainable params: 39,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " def create_net():\n",
    "\n",
    "    inputs = Input(shape=(256,2))\n",
    "    x = Reshape((256,2,1))(inputs)\n",
    "    x = Conv2D(8,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "    x = Conv2D(32,(3,1),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,1),activation='relu',padding = 'same')(x)\n",
    "    #x = resnet(x,64,(3,2),'6')\n",
    "    #x = MaxPool2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = Dense(100, activation='relu', kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    # x = Dropout(0.3)(x)\n",
    "    x = Dense(80, activation='relu',kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(n_tx, activation='softmax',kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    ops = x\n",
    "\n",
    "    classifier = Model(inputs,ops)\n",
    "    classifier.compile(loss='categorical_crossentropy',metrics=['categorical_accuracy'],optimizer=keras.optimizers.Adam(0.0005))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "classifier = create_net()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(classifier):\n",
    "    pred = classifier.predict(sig_dfTest)\n",
    "    acc = np.mean(np.argmax(pred,1)==txidNum_dfTest)\n",
    "\n",
    "    test_indx = ()\n",
    "    for indx in range(len(tx_list)):\n",
    "        cls_indx = np.where(txidNum_dfTest == indx)\n",
    "        test_indx = test_indx + (cls_indx[0][:n_test_samples],)\n",
    "    test_indx = np.concatenate(test_indx) \n",
    "    acc_bal = np.mean(np.argmax(pred[test_indx,:],1)==txidNum_dfTest[test_indx])\n",
    "    return acc,acc_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "nday: 0  \n",
      "Train on 4800 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "4640/4800 [============================>.] - ETA: 0s - loss: 1.1227 - categorical_accuracy: 0.6200\n",
      "Epoch 00001: val_loss improved from inf to 0.24237, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 280us/sample - loss: 1.0965 - categorical_accuracy: 0.6281 - val_loss: 0.2424 - val_categorical_accuracy: 0.9683\n",
      "Epoch 2/100\n",
      "4640/4800 [============================>.] - ETA: 0s - loss: 0.2492 - categorical_accuracy: 0.9185\n",
      "Epoch 00002: val_loss improved from 0.24237 to 0.09642, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 208us/sample - loss: 0.2461 - categorical_accuracy: 0.9198 - val_loss: 0.0964 - val_categorical_accuracy: 0.9867\n",
      "Epoch 3/100\n",
      "4576/4800 [===========================>..] - ETA: 0s - loss: 0.1075 - categorical_accuracy: 0.9766\n",
      "Epoch 00003: val_loss improved from 0.09642 to 0.03803, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 199us/sample - loss: 0.1056 - categorical_accuracy: 0.9773 - val_loss: 0.0380 - val_categorical_accuracy: 0.9967\n",
      "Epoch 4/100\n",
      "4672/4800 [============================>.] - ETA: 0s - loss: 0.0520 - categorical_accuracy: 0.9940\n",
      "Epoch 00004: val_loss improved from 0.03803 to 0.02639, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 206us/sample - loss: 0.0516 - categorical_accuracy: 0.9940 - val_loss: 0.0264 - val_categorical_accuracy: 0.9967\n",
      "Epoch 5/100\n",
      "4704/4800 [============================>.] - ETA: 0s - loss: 0.0404 - categorical_accuracy: 0.9957\n",
      "Epoch 00005: val_loss improved from 0.02639 to 0.01937, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 195us/sample - loss: 0.0410 - categorical_accuracy: 0.9956 - val_loss: 0.0194 - val_categorical_accuracy: 0.9983\n",
      "Epoch 6/100\n",
      "4704/4800 [============================>.] - ETA: 0s - loss: 0.0337 - categorical_accuracy: 0.9979\n",
      "Epoch 00006: val_loss improved from 0.01937 to 0.01687, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 195us/sample - loss: 0.0335 - categorical_accuracy: 0.9979 - val_loss: 0.0169 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "4640/4800 [============================>.] - ETA: 0s - loss: 0.0315 - categorical_accuracy: 0.9974\n",
      "Epoch 00007: val_loss did not improve from 0.01687\n",
      "4800/4800 [==============================] - 1s 185us/sample - loss: 0.0315 - categorical_accuracy: 0.9973 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "4736/4800 [============================>.] - ETA: 0s - loss: 0.0287 - categorical_accuracy: 0.9970\n",
      "Epoch 00008: val_loss did not improve from 0.01687\n",
      "4800/4800 [==============================] - 1s 194us/sample - loss: 0.0286 - categorical_accuracy: 0.9971 - val_loss: 0.0176 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "4576/4800 [===========================>..] - ETA: 0s - loss: 0.0267 - categorical_accuracy: 0.9974\n",
      "Epoch 00009: val_loss improved from 0.01687 to 0.01564, saving model to t_weights_1\n",
      "4800/4800 [==============================] - 1s 187us/sample - loss: 0.0266 - categorical_accuracy: 0.9975 - val_loss: 0.0156 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "4480/4800 [===========================>..] - ETA: 0s - loss: 0.0207 - categorical_accuracy: 0.9989\n",
      "Epoch 00010: val_loss did not improve from 0.01564\n",
      "4800/4800 [==============================] - 1s 189us/sample - loss: 0.0205 - categorical_accuracy: 0.9990 - val_loss: 0.0158 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "4544/4800 [===========================>..] - ETA: 0s - loss: 0.0207 - categorical_accuracy: 0.9987\n",
      "Epoch 00011: val_loss did not improve from 0.01564\n",
      "4800/4800 [==============================] - 1s 199us/sample - loss: 0.0205 - categorical_accuracy: 0.9987 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "4704/4800 [============================>.] - ETA: 0s - loss: 0.0215 - categorical_accuracy: 0.9981\n",
      "Epoch 00012: val_loss did not improve from 0.01564\n",
      "4800/4800 [==============================] - 1s 194us/sample - loss: 0.0214 - categorical_accuracy: 0.9981 - val_loss: 0.0171 - val_categorical_accuracy: 0.9983\n",
      "Epoch 13/100\n",
      "4672/4800 [============================>.] - ETA: 0s - loss: 0.0197 - categorical_accuracy: 0.9989\n",
      "Epoch 00013: val_loss did not improve from 0.01564\n",
      "4800/4800 [==============================] - 1s 185us/sample - loss: 0.0197 - categorical_accuracy: 0.9990 - val_loss: 0.0169 - val_categorical_accuracy: 0.9983\n",
      "Epoch 14/100\n",
      "4608/4800 [===========================>..] - ETA: 0s - loss: 0.0199 - categorical_accuracy: 0.9991\n",
      "Epoch 00014: val_loss did not improve from 0.01564\n",
      "4800/4800 [==============================] - 1s 184us/sample - loss: 0.0198 - categorical_accuracy: 0.9992 - val_loss: 0.0178 - val_categorical_accuracy: 0.9983\n",
      "0.99833333 0.9585\n",
      "\n",
      "\n",
      "nday: 1  \n",
      "Train on 9600 samples, validate on 1200 samples\n",
      "Epoch 1/100\n",
      "9376/9600 [============================>.] - ETA: 0s - loss: 0.6536 - categorical_accuracy: 0.7737\n",
      "Epoch 00001: val_loss improved from inf to 0.06126, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 230us/sample - loss: 0.6410 - categorical_accuracy: 0.7783 - val_loss: 0.0613 - val_categorical_accuracy: 0.9942\n",
      "Epoch 2/100\n",
      "9312/9600 [============================>.] - ETA: 0s - loss: 0.0660 - categorical_accuracy: 0.9894\n",
      "Epoch 00002: val_loss improved from 0.06126 to 0.03109, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 200us/sample - loss: 0.0652 - categorical_accuracy: 0.9894 - val_loss: 0.0311 - val_categorical_accuracy: 0.9950\n",
      "Epoch 3/100\n",
      "9408/9600 [============================>.] - ETA: 0s - loss: 0.0382 - categorical_accuracy: 0.9952\n",
      "Epoch 00003: val_loss did not improve from 0.03109\n",
      "9600/9600 [==============================] - 2s 191us/sample - loss: 0.0379 - categorical_accuracy: 0.9953 - val_loss: 0.0387 - val_categorical_accuracy: 0.9925\n",
      "Epoch 4/100\n",
      "9376/9600 [============================>.] - ETA: 0s - loss: 0.0314 - categorical_accuracy: 0.9968\n",
      "Epoch 00004: val_loss did not improve from 0.03109\n",
      "9600/9600 [==============================] - 2s 188us/sample - loss: 0.0313 - categorical_accuracy: 0.9968 - val_loss: 0.0342 - val_categorical_accuracy: 0.9950\n",
      "Epoch 5/100\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.0299 - categorical_accuracy: 0.9974\n",
      "Epoch 00005: val_loss improved from 0.03109 to 0.02783, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 198us/sample - loss: 0.0298 - categorical_accuracy: 0.9974 - val_loss: 0.0278 - val_categorical_accuracy: 0.9967\n",
      "Epoch 6/100\n",
      "9312/9600 [============================>.] - ETA: 0s - loss: 0.0246 - categorical_accuracy: 0.9984\n",
      "Epoch 00006: val_loss did not improve from 0.02783\n",
      "9600/9600 [==============================] - 2s 194us/sample - loss: 0.0245 - categorical_accuracy: 0.9984 - val_loss: 0.0291 - val_categorical_accuracy: 0.9967\n",
      "Epoch 7/100\n",
      "9248/9600 [===========================>..] - ETA: 0s - loss: 0.0230 - categorical_accuracy: 0.9983\n",
      "Epoch 00007: val_loss improved from 0.02783 to 0.02649, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 198us/sample - loss: 0.0228 - categorical_accuracy: 0.9983 - val_loss: 0.0265 - val_categorical_accuracy: 0.9967\n",
      "Epoch 8/100\n",
      "9376/9600 [============================>.] - ETA: 0s - loss: 0.0200 - categorical_accuracy: 0.9989\n",
      "Epoch 00008: val_loss improved from 0.02649 to 0.02423, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 199us/sample - loss: 0.0199 - categorical_accuracy: 0.9990 - val_loss: 0.0242 - val_categorical_accuracy: 0.9992\n",
      "Epoch 9/100\n",
      "9536/9600 [============================>.] - ETA: 0s - loss: 0.0193 - categorical_accuracy: 0.9988\n",
      "Epoch 00009: val_loss improved from 0.02423 to 0.02402, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 197us/sample - loss: 0.0193 - categorical_accuracy: 0.9989 - val_loss: 0.0240 - val_categorical_accuracy: 0.9983\n",
      "Epoch 10/100\n",
      "9472/9600 [============================>.] - ETA: 0s - loss: 0.0200 - categorical_accuracy: 0.9986\n",
      "Epoch 00010: val_loss improved from 0.02402 to 0.02316, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 205us/sample - loss: 0.0200 - categorical_accuracy: 0.9986 - val_loss: 0.0232 - val_categorical_accuracy: 0.9992\n",
      "Epoch 11/100\n",
      "9536/9600 [============================>.] - ETA: 0s - loss: 0.0178 - categorical_accuracy: 0.9992\n",
      "Epoch 00011: val_loss improved from 0.02316 to 0.02210, saving model to t_weights_1\n",
      "9600/9600 [==============================] - 2s 202us/sample - loss: 0.0178 - categorical_accuracy: 0.9992 - val_loss: 0.0221 - val_categorical_accuracy: 0.9983\n",
      "Epoch 12/100\n",
      "9312/9600 [============================>.] - ETA: 0s - loss: 0.0176 - categorical_accuracy: 0.9990\n",
      "Epoch 00012: val_loss did not improve from 0.02210\n",
      "9600/9600 [==============================] - 2s 193us/sample - loss: 0.0193 - categorical_accuracy: 0.9985 - val_loss: 0.0343 - val_categorical_accuracy: 0.9967\n",
      "Epoch 13/100\n",
      "9504/9600 [============================>.] - ETA: 0s - loss: 0.0180 - categorical_accuracy: 0.9995\n",
      "Epoch 00013: val_loss did not improve from 0.02210\n",
      "9600/9600 [==============================] - 2s 184us/sample - loss: 0.0180 - categorical_accuracy: 0.9995 - val_loss: 0.0316 - val_categorical_accuracy: 0.9975\n",
      "Epoch 14/100\n",
      "9440/9600 [============================>.] - ETA: 0s - loss: 0.0162 - categorical_accuracy: 0.9997\n",
      "Epoch 00014: val_loss did not improve from 0.02210\n",
      "9600/9600 [==============================] - 2s 192us/sample - loss: 0.0162 - categorical_accuracy: 0.9997 - val_loss: 0.0226 - val_categorical_accuracy: 0.9992\n",
      "Epoch 15/100\n",
      "9504/9600 [============================>.] - ETA: 0s - loss: 0.0151 - categorical_accuracy: 0.9998\n",
      "Epoch 00015: val_loss did not improve from 0.02210\n",
      "9600/9600 [==============================] - 2s 186us/sample - loss: 0.0155 - categorical_accuracy: 0.9997 - val_loss: 0.0265 - val_categorical_accuracy: 0.9983\n",
      "Epoch 16/100\n",
      "9408/9600 [============================>.] - ETA: 0s - loss: 0.0156 - categorical_accuracy: 0.9994\n",
      "Epoch 00016: val_loss did not improve from 0.02210\n",
      "9600/9600 [==============================] - 2s 193us/sample - loss: 0.0156 - categorical_accuracy: 0.9994 - val_loss: 0.0242 - val_categorical_accuracy: 0.9983\n",
      "1.0 0.974\n",
      "\n",
      "\n",
      "nday: 2  \n",
      "Train on 14400 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.5511 - categorical_accuracy: 0.8096\n",
      "Epoch 00001: val_loss improved from inf to 0.03504, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 212us/sample - loss: 0.5465 - categorical_accuracy: 0.8113 - val_loss: 0.0350 - val_categorical_accuracy: 0.9961\n",
      "Epoch 2/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0441 - categorical_accuracy: 0.9933\n",
      "Epoch 00002: val_loss improved from 0.03504 to 0.01732, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0440 - categorical_accuracy: 0.9933 - val_loss: 0.0173 - val_categorical_accuracy: 0.9989\n",
      "Epoch 3/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0275 - categorical_accuracy: 0.9968\n",
      "Epoch 00003: val_loss improved from 0.01732 to 0.01393, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 191us/sample - loss: 0.0274 - categorical_accuracy: 0.9968 - val_loss: 0.0139 - val_categorical_accuracy: 0.9994\n",
      "Epoch 4/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0252 - categorical_accuracy: 0.9976\n",
      "Epoch 00004: val_loss improved from 0.01393 to 0.01356, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 195us/sample - loss: 0.0251 - categorical_accuracy: 0.9976 - val_loss: 0.0136 - val_categorical_accuracy: 0.9994\n",
      "Epoch 5/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0218 - categorical_accuracy: 0.9977\n",
      "Epoch 00005: val_loss did not improve from 0.01356\n",
      "14400/14400 [==============================] - 3s 183us/sample - loss: 0.0217 - categorical_accuracy: 0.9978 - val_loss: 0.0139 - val_categorical_accuracy: 0.9994\n",
      "Epoch 6/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0175 - categorical_accuracy: 0.9989\n",
      "Epoch 00006: val_loss improved from 0.01356 to 0.01315, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 194us/sample - loss: 0.0175 - categorical_accuracy: 0.9989 - val_loss: 0.0132 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "14144/14400 [============================>.] - ETA: 0s - loss: 0.0169 - categorical_accuracy: 0.9989\n",
      "Epoch 00007: val_loss improved from 0.01315 to 0.01286, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0168 - categorical_accuracy: 0.9990 - val_loss: 0.0129 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0153 - categorical_accuracy: 0.9992\n",
      "Epoch 00008: val_loss did not improve from 0.01286\n",
      "14400/14400 [==============================] - 3s 189us/sample - loss: 0.0153 - categorical_accuracy: 0.9992 - val_loss: 0.0149 - val_categorical_accuracy: 0.9989\n",
      "Epoch 9/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.0164 - categorical_accuracy: 0.9991\n",
      "Epoch 00009: val_loss improved from 0.01286 to 0.01215, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 193us/sample - loss: 0.0163 - categorical_accuracy: 0.9991 - val_loss: 0.0121 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0150 - categorical_accuracy: 0.9994\n",
      "Epoch 00010: val_loss did not improve from 0.01215\n",
      "14400/14400 [==============================] - 3s 190us/sample - loss: 0.0150 - categorical_accuracy: 0.9994 - val_loss: 0.0139 - val_categorical_accuracy: 0.9994\n",
      "Epoch 11/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0148 - categorical_accuracy: 0.9993\n",
      "Epoch 00011: val_loss improved from 0.01215 to 0.01165, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 197us/sample - loss: 0.0148 - categorical_accuracy: 0.9993 - val_loss: 0.0117 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "14144/14400 [============================>.] - ETA: 0s - loss: 0.0140 - categorical_accuracy: 0.9992\n",
      "Epoch 00012: val_loss improved from 0.01165 to 0.01142, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 189us/sample - loss: 0.0140 - categorical_accuracy: 0.9992 - val_loss: 0.0114 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "14240/14400 [============================>.] - ETA: 0s - loss: 0.0116 - categorical_accuracy: 0.9999\n",
      "Epoch 00013: val_loss did not improve from 0.01142\n",
      "14400/14400 [==============================] - 3s 187us/sample - loss: 0.0118 - categorical_accuracy: 0.9997 - val_loss: 0.0166 - val_categorical_accuracy: 0.9994\n",
      "Epoch 14/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0135 - categorical_accuracy: 0.9994\n",
      "Epoch 00014: val_loss did not improve from 0.01142\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0135 - categorical_accuracy: 0.9994 - val_loss: 0.0133 - val_categorical_accuracy: 0.9994\n",
      "Epoch 15/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0126 - categorical_accuracy: 0.9993\n",
      "Epoch 00015: val_loss did not improve from 0.01142\n",
      "14400/14400 [==============================] - 3s 191us/sample - loss: 0.0125 - categorical_accuracy: 0.9993 - val_loss: 0.0123 - val_categorical_accuracy: 0.9994\n",
      "Epoch 16/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0158 - categorical_accuracy: 0.9991\n",
      "Epoch 00016: val_loss improved from 0.01142 to 0.01001, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 187us/sample - loss: 0.0158 - categorical_accuracy: 0.9990 - val_loss: 0.0100 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0104 - categorical_accuracy: 0.9999\n",
      "Epoch 00017: val_loss did not improve from 0.01001\n",
      "14400/14400 [==============================] - 3s 180us/sample - loss: 0.0104 - categorical_accuracy: 0.9999 - val_loss: 0.0101 - val_categorical_accuracy: 0.9994\n",
      "Epoch 18/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.0117 - categorical_accuracy: 0.9994\n",
      "Epoch 00018: val_loss improved from 0.01001 to 0.00956, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 186us/sample - loss: 0.0116 - categorical_accuracy: 0.9994 - val_loss: 0.0096 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "13952/14400 [============================>.] - ETA: 0s - loss: 0.0105 - categorical_accuracy: 0.9998\n",
      "Epoch 00019: val_loss improved from 0.00956 to 0.00918, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 184us/sample - loss: 0.0105 - categorical_accuracy: 0.9998 - val_loss: 0.0092 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0088 - categorical_accuracy: 0.9999\n",
      "Epoch 00020: val_loss improved from 0.00918 to 0.00828, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0088 - categorical_accuracy: 0.9999 - val_loss: 0.0083 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0090 - categorical_accuracy: 0.9997\n",
      "Epoch 00021: val_loss improved from 0.00828 to 0.00772, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 181us/sample - loss: 0.0090 - categorical_accuracy: 0.9997 - val_loss: 0.0077 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.0105 - categorical_accuracy: 0.9993\n",
      "Epoch 00022: val_loss improved from 0.00772 to 0.00766, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 181us/sample - loss: 0.0105 - categorical_accuracy: 0.9993 - val_loss: 0.0077 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0088 - categorical_accuracy: 0.9995\n",
      "Epoch 00023: val_loss did not improve from 0.00766\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0089 - categorical_accuracy: 0.9995 - val_loss: 0.0091 - val_categorical_accuracy: 0.9994\n",
      "Epoch 24/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0090 - categorical_accuracy: 0.9996\n",
      "Epoch 00024: val_loss improved from 0.00766 to 0.00729, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 184us/sample - loss: 0.0090 - categorical_accuracy: 0.9996 - val_loss: 0.0073 - val_categorical_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0076 - categorical_accuracy: 0.9998\n",
      "Epoch 00025: val_loss improved from 0.00729 to 0.00693, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 183us/sample - loss: 0.0076 - categorical_accuracy: 0.9998 - val_loss: 0.0069 - val_categorical_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0068 - categorical_accuracy: 0.9999\n",
      "Epoch 00026: val_loss improved from 0.00693 to 0.00630, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 179us/sample - loss: 0.0067 - categorical_accuracy: 0.9999 - val_loss: 0.0063 - val_categorical_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0085 - categorical_accuracy: 0.9992\n",
      "Epoch 00027: val_loss did not improve from 0.00630\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0085 - categorical_accuracy: 0.9992 - val_loss: 0.0068 - val_categorical_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0075 - categorical_accuracy: 0.9995\n",
      "Epoch 00028: val_loss did not improve from 0.00630\n",
      "14400/14400 [==============================] - 3s 186us/sample - loss: 0.0075 - categorical_accuracy: 0.9995 - val_loss: 0.0064 - val_categorical_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0061 - categorical_accuracy: 1.0000\n",
      "Epoch 00029: val_loss improved from 0.00630 to 0.00576, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 197us/sample - loss: 0.0061 - categorical_accuracy: 1.0000 - val_loss: 0.0058 - val_categorical_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0057 - categorical_accuracy: 1.0000\n",
      "Epoch 00030: val_loss improved from 0.00576 to 0.00528, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 190us/sample - loss: 0.0057 - categorical_accuracy: 1.0000 - val_loss: 0.0053 - val_categorical_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0051 - categorical_accuracy: 1.0000\n",
      "Epoch 00031: val_loss improved from 0.00528 to 0.00476, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0051 - categorical_accuracy: 1.0000 - val_loss: 0.0048 - val_categorical_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0061 - categorical_accuracy: 0.9997\n",
      "Epoch 00032: val_loss did not improve from 0.00476\n",
      "14400/14400 [==============================] - 3s 190us/sample - loss: 0.0061 - categorical_accuracy: 0.9997 - val_loss: 0.0081 - val_categorical_accuracy: 0.9994\n",
      "Epoch 33/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0064 - categorical_accuracy: 0.9995\n",
      "Epoch 00033: val_loss did not improve from 0.00476\n",
      "14400/14400 [==============================] - 3s 194us/sample - loss: 0.0063 - categorical_accuracy: 0.9995 - val_loss: 0.0048 - val_categorical_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0046 - categorical_accuracy: 0.9999\n",
      "Epoch 00034: val_loss did not improve from 0.00476\n",
      "14400/14400 [==============================] - 3s 186us/sample - loss: 0.0046 - categorical_accuracy: 0.9999 - val_loss: 0.0048 - val_categorical_accuracy: 0.9994\n",
      "Epoch 35/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0044 - categorical_accuracy: 0.9999\n",
      "Epoch 00035: val_loss did not improve from 0.00476\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0044 - categorical_accuracy: 0.9999 - val_loss: 0.0062 - val_categorical_accuracy: 0.9994\n",
      "Epoch 36/100\n",
      "14080/14400 [============================>.] - ETA: 0s - loss: 0.0053 - categorical_accuracy: 0.9999\n",
      "Epoch 00036: val_loss improved from 0.00476 to 0.00396, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 197us/sample - loss: 0.0053 - categorical_accuracy: 0.9999 - val_loss: 0.0040 - val_categorical_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0048 - categorical_accuracy: 0.9998\n",
      "Epoch 00037: val_loss did not improve from 0.00396\n",
      "14400/14400 [==============================] - 3s 184us/sample - loss: 0.0048 - categorical_accuracy: 0.9998 - val_loss: 0.0077 - val_categorical_accuracy: 0.9994\n",
      "Epoch 38/100\n",
      "14080/14400 [============================>.] - ETA: 0s - loss: 0.0048 - categorical_accuracy: 0.9997\n",
      "Epoch 00038: val_loss improved from 0.00396 to 0.00387, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 186us/sample - loss: 0.0048 - categorical_accuracy: 0.9997 - val_loss: 0.0039 - val_categorical_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0055 - categorical_accuracy: 0.9996\n",
      "Epoch 00039: val_loss did not improve from 0.00387\n",
      "14400/14400 [==============================] - 3s 186us/sample - loss: 0.0058 - categorical_accuracy: 0.9996 - val_loss: 0.0040 - val_categorical_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.0046 - categorical_accuracy: 0.9998\n",
      "Epoch 00040: val_loss did not improve from 0.00387\n",
      "14400/14400 [==============================] - 3s 191us/sample - loss: 0.0046 - categorical_accuracy: 0.9998 - val_loss: 0.0040 - val_categorical_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0038 - categorical_accuracy: 1.0000\n",
      "Epoch 00041: val_loss improved from 0.00387 to 0.00351, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 187us/sample - loss: 0.0038 - categorical_accuracy: 1.0000 - val_loss: 0.0035 - val_categorical_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0039 - categorical_accuracy: 0.9999\n",
      "Epoch 00042: val_loss did not improve from 0.00351\n",
      "14400/14400 [==============================] - 3s 187us/sample - loss: 0.0039 - categorical_accuracy: 0.9999 - val_loss: 0.0066 - val_categorical_accuracy: 0.9994\n",
      "Epoch 43/100\n",
      "14080/14400 [============================>.] - ETA: 0s - loss: 0.0044 - categorical_accuracy: 0.9999\n",
      "Epoch 00043: val_loss improved from 0.00351 to 0.00346, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 190us/sample - loss: 0.0044 - categorical_accuracy: 0.9999 - val_loss: 0.0035 - val_categorical_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0045 - categorical_accuracy: 0.9998\n",
      "Epoch 00044: val_loss did not improve from 0.00346\n",
      "14400/14400 [==============================] - 3s 189us/sample - loss: 0.0045 - categorical_accuracy: 0.9998 - val_loss: 0.0190 - val_categorical_accuracy: 0.9989\n",
      "Epoch 45/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0059 - categorical_accuracy: 0.9996\n",
      "Epoch 00045: val_loss did not improve from 0.00346\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0059 - categorical_accuracy: 0.9997 - val_loss: 0.0042 - val_categorical_accuracy: 0.9994\n",
      "Epoch 46/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0036 - categorical_accuracy: 0.9999\n",
      "Epoch 00046: val_loss did not improve from 0.00346\n",
      "14400/14400 [==============================] - 3s 192us/sample - loss: 0.0036 - categorical_accuracy: 0.9999 - val_loss: 0.0035 - val_categorical_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0032 - categorical_accuracy: 1.0000\n",
      "Epoch 00047: val_loss did not improve from 0.00346\n",
      "14400/14400 [==============================] - 3s 185us/sample - loss: 0.0032 - categorical_accuracy: 1.0000 - val_loss: 0.0035 - val_categorical_accuracy: 0.9994\n",
      "Epoch 48/100\n",
      "14240/14400 [============================>.] - ETA: 0s - loss: 0.0030 - categorical_accuracy: 1.0000\n",
      "Epoch 00048: val_loss improved from 0.00346 to 0.00273, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 186us/sample - loss: 0.0030 - categorical_accuracy: 1.0000 - val_loss: 0.0027 - val_categorical_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.0028 - categorical_accuracy: 1.0000\n",
      "Epoch 00049: val_loss improved from 0.00273 to 0.00264, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 184us/sample - loss: 0.0028 - categorical_accuracy: 1.0000 - val_loss: 0.0026 - val_categorical_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0036 - categorical_accuracy: 0.9999\n",
      "Epoch 00050: val_loss did not improve from 0.00264\n",
      "14400/14400 [==============================] - 3s 190us/sample - loss: 0.0035 - categorical_accuracy: 0.9999 - val_loss: 0.0036 - val_categorical_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0048 - categorical_accuracy: 0.9996\n",
      "Epoch 00051: val_loss did not improve from 0.00264\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0048 - categorical_accuracy: 0.9997 - val_loss: 0.0036 - val_categorical_accuracy: 0.9994\n",
      "Epoch 52/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0032 - categorical_accuracy: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.00264\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0032 - categorical_accuracy: 0.9998 - val_loss: 0.0031 - val_categorical_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "14240/14400 [============================>.] - ETA: 0s - loss: 0.0027 - categorical_accuracy: 1.0000\n",
      "Epoch 00053: val_loss did not improve from 0.00264\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 0.0035 - val_categorical_accuracy: 0.9994\n",
      "Epoch 54/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0026 - categorical_accuracy: 0.9999\n",
      "Epoch 00054: val_loss improved from 0.00264 to 0.00235, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0026 - categorical_accuracy: 0.9999 - val_loss: 0.0024 - val_categorical_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0046 - categorical_accuracy: 0.9994\n",
      "Epoch 00055: val_loss did not improve from 0.00235\n",
      "14400/14400 [==============================] - 3s 179us/sample - loss: 0.0046 - categorical_accuracy: 0.9994 - val_loss: 0.0051 - val_categorical_accuracy: 0.9989\n",
      "Epoch 56/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0027 - categorical_accuracy: 1.0000\n",
      "Epoch 00056: val_loss did not improve from 0.00235\n",
      "14400/14400 [==============================] - 3s 178us/sample - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 0.0027 - val_categorical_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "14272/14400 [============================>.] - ETA: 0s - loss: 0.0026 - categorical_accuracy: 0.9999\n",
      "Epoch 00057: val_loss did not improve from 0.00235\n",
      "14400/14400 [==============================] - 3s 180us/sample - loss: 0.0026 - categorical_accuracy: 0.9999 - val_loss: 0.0025 - val_categorical_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "14144/14400 [============================>.] - ETA: 0s - loss: 0.0026 - categorical_accuracy: 0.9999\n",
      "Epoch 00058: val_loss improved from 0.00235 to 0.00219, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 188us/sample - loss: 0.0026 - categorical_accuracy: 0.9999 - val_loss: 0.0022 - val_categorical_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0022 - categorical_accuracy: 1.0000\n",
      "Epoch 00059: val_loss improved from 0.00219 to 0.00201, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 185us/sample - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "14240/14400 [============================>.] - ETA: 0s - loss: 0.0022 - categorical_accuracy: 1.0000\n",
      "Epoch 00060: val_loss improved from 0.00201 to 0.00196, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 180us/sample - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "14368/14400 [============================>.] - ETA: 0s - loss: 0.0020 - categorical_accuracy: 1.0000\n",
      "Epoch 00061: val_loss improved from 0.00196 to 0.00180, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 0.0018 - val_categorical_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "14176/14400 [============================>.] - ETA: 0s - loss: 0.0030 - categorical_accuracy: 0.9996\n",
      "Epoch 00062: val_loss did not improve from 0.00180\n",
      "14400/14400 [==============================] - 3s 181us/sample - loss: 0.0030 - categorical_accuracy: 0.9996 - val_loss: 0.0022 - val_categorical_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0024 - categorical_accuracy: 0.9999\n",
      "Epoch 00063: val_loss did not improve from 0.00180\n",
      "14400/14400 [==============================] - 3s 182us/sample - loss: 0.0024 - categorical_accuracy: 0.9999 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 00064: val_loss did not improve from 0.00180\n",
      "14400/14400 [==============================] - 3s 181us/sample - loss: 0.0021 - categorical_accuracy: 1.0000 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "14336/14400 [============================>.] - ETA: 0s - loss: 0.0020 - categorical_accuracy: 0.9999\n",
      "Epoch 00065: val_loss improved from 0.00180 to 0.00174, saving model to t_weights_1\n",
      "14400/14400 [==============================] - 3s 180us/sample - loss: 0.0020 - categorical_accuracy: 0.9999 - val_loss: 0.0017 - val_categorical_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "14144/14400 [============================>.] - ETA: 0s - loss: 0.0029 - categorical_accuracy: 0.9996\n",
      "Epoch 00066: val_loss did not improve from 0.00174\n",
      "14400/14400 [==============================] - 3s 179us/sample - loss: 0.0029 - categorical_accuracy: 0.9997 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "14112/14400 [============================>.] - ETA: 0s - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 00067: val_loss did not improve from 0.00174\n",
      "14400/14400 [==============================] - 3s 175us/sample - loss: 0.0021 - categorical_accuracy: 1.0000 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "14208/14400 [============================>.] - ETA: 0s - loss: 0.0022 - categorical_accuracy: 0.9999\n",
      "Epoch 00068: val_loss did not improve from 0.00174\n",
      "14400/14400 [==============================] - 3s 178us/sample - loss: 0.0022 - categorical_accuracy: 0.9999 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "14144/14400 [============================>.] - ETA: 0s - loss: 0.0032 - categorical_accuracy: 0.9998\n",
      "Epoch 00069: val_loss did not improve from 0.00174\n",
      "14400/14400 [==============================] - 3s 184us/sample - loss: 0.0032 - categorical_accuracy: 0.9998 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "14304/14400 [============================>.] - ETA: 0s - loss: 0.0021 - categorical_accuracy: 0.9999\n",
      "Epoch 00070: val_loss did not improve from 0.00174\n",
      "14400/14400 [==============================] - 3s 177us/sample - loss: 0.0021 - categorical_accuracy: 0.9999 - val_loss: 0.0018 - val_categorical_accuracy: 1.0000\n",
      "0.9988889 0.99866664\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "continue_training = True\n",
    "nreal = 5\n",
    "\n",
    "real_list = list(range(nreal))\n",
    "\n",
    "\n",
    "\n",
    "patience = 5\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "capture_date_test_list = capture_date_list[-1]\n",
    "\n",
    "dataset_test = merge_compact_dataset(compact_dataset,capture_date_test_list,tx_list,rx_list, equalized=equalized)\n",
    "    \n",
    "test_augset_dfDay,_,_ =  prepare_dataset(dataset_test,tx_list,\n",
    "                                                        val_frac=0, test_frac=0)\n",
    "[sig_dfTest,txidNum_dfTest,txid_dfTest,cls_weights] = test_augset_dfDay\n",
    "\n",
    "smTest_results_real = []\n",
    "dfTest_results_real = []\n",
    "\n",
    "\n",
    "for nday in range(3):\n",
    "    print(\"\");print(\"\")\n",
    "    print(\"nday: {}  \".format(nday))\n",
    "    fname_w = 'weights/d008_{:04d}.hd5'.format(nday)\n",
    "    rx_train_list= rx_list\n",
    "\n",
    "    dataset = merge_compact_dataset(compact_dataset,capture_date_list[:nday+1],tx_list,rx_list, equalized=equalized)\n",
    "    \n",
    "\n",
    "\n",
    "    train_augset,val_augset,test_augset_smRx =  prepare_dataset(dataset,tx_list,\n",
    "                                                        val_frac=0.1, test_frac=0.1)\n",
    "    [sig_train,txidNum_train,txid_train,cls_weights] = train_augset\n",
    "    [sig_valid,txidNum_valid,txid_valid,_] = val_augset\n",
    "    [sig_smTest,txidNum_smTest,txid_smTest,cls_weights] = test_augset_smRx\n",
    "\n",
    "    if continue_training:\n",
    "        skip = os.path.isfile(fname_w)\n",
    "    else:\n",
    "        skip = False\n",
    "    classifier = create_net()\n",
    "    if TRAIN and not skip:\n",
    "        filepath = 't_weights_'+GPU\n",
    "        c=[ keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "          keras.callbacks.EarlyStopping(monitor='val_loss',  patience=patience)]\n",
    "        history = classifier.fit(sig_train,txid_train,class_weight=cls_weights,\n",
    "                                 validation_data=(sig_valid , txid_valid),callbacks=c, epochs=n_epochs)\n",
    "        classifier.load_weights(filepath)\n",
    "        classifier.save_weights(fname_w,save_format=\"h5\")\n",
    "    else:\n",
    "        classifier.load_weights(fname_w)\n",
    "\n",
    "    smTest_r = classifier.evaluate(sig_smTest,txid_smTest,verbose=0)[1]\n",
    "    dfTest_r = classifier.evaluate(sig_dfTest,txid_dfTest,verbose=0)[1]\n",
    "\n",
    "\n",
    "    print(smTest_r,dfTest_r)\n",
    "    smTest_results_real.append(smTest_r)\n",
    "    dfTest_results_real.append(dfTest_r)\n",
    "    K.clear_session()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 4)\n",
      "[0.99833333, 1.0, 0.9988889]\n",
      "[0.9585, 0.974, 0.99866664]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,4),smTest_results_real)\n",
    "plt.plot(range(1,4),dfTest_results_real)\n",
    "plt.xlabel('No of Training Days')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Same Day','Diff Day'])\n",
    "print(range(1,4))\n",
    "print(smTest_results_real)\n",
    "print(dfTest_results_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
