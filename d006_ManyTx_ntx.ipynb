{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import scipy,scipy.spatial\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "from  data_utilities import *\n",
    "# from definitions import *\n",
    "# from run_train_eval_net import run_train_eval_net,run_eval_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 18\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'ManyTx'\n",
    "dataset_path='../../orbit_rf_dataset/data/compact_pkl_datasets/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "\n",
    "equalized = 0\n",
    "\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 2)]          0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 256, 2, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 256, 2, 8)         56        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 2, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 2, 16)        784       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 2, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 2, 16)         1552      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 1, 32)         1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 1, 16)         1552      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 405       \n",
      "=================================================================\n",
      "Total params: 39,697\n",
      "Trainable params: 39,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " def create_net(ntx_i):\n",
    "\n",
    "    inputs = Input(shape=(256,2))\n",
    "    x = Reshape((256,2,1))(inputs)\n",
    "    x = Conv2D(8,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "    x = Conv2D(32,(3,1),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,1),activation='relu',padding = 'same')(x)\n",
    "    #x = resnet(x,64,(3,2),'6')\n",
    "    #x = MaxPool2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = Dense(100, activation='relu', kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    # x = Dropout(0.3)(x)\n",
    "    x = Dense(80, activation='relu',kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(ntx_i, activation='softmax',kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    ops = x\n",
    "\n",
    "    classifier = Model(inputs,ops)\n",
    "    classifier.compile(loss='categorical_crossentropy',metrics=['categorical_accuracy'],optimizer=keras.optimizers.Adam(0.0005))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "classifier = create_net(5)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(classifier):\n",
    "    pred = classifier.predict(sig_dfTest)\n",
    "    acc = np.mean(np.argmax(pred,1)==txidNum_dfTest)\n",
    "\n",
    "    test_indx = ()\n",
    "    for indx in range(len(tx_list)):\n",
    "        cls_indx = np.where(txidNum_dfTest == indx)\n",
    "        test_indx = test_indx + (cls_indx[0][:n_test_samples],)\n",
    "    test_indx = np.concatenate(test_indx) \n",
    "    acc_bal = np.mean(np.argmax(pred[test_indx,:],1)==txidNum_dfTest[test_indx])\n",
    "    return acc,acc_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 45, 80, 115, 150]\n",
      "\n",
      "\n",
      "ntx_i: 10  \n",
      "0.8155456\n",
      "\n",
      "\n",
      "ntx_i: 45  \n",
      "Train on 121513 samples, validate on 15189 samples\n",
      "Epoch 1/100\n",
      "121376/121513 [============================>.] - ETA: 0s - loss: 3.4345 - categorical_accuracy: 0.0665\n",
      "Epoch 00001: val_loss improved from inf to 3.05380, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 22s 177us/sample - loss: 3.4341 - categorical_accuracy: 0.0666 - val_loss: 3.0538 - val_categorical_accuracy: 0.1274\n",
      "Epoch 2/100\n",
      "121024/121513 [============================>.] - ETA: 0s - loss: 2.8348 - categorical_accuracy: 0.1699\n",
      "Epoch 00002: val_loss improved from 3.05380 to 2.47703, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 20s 168us/sample - loss: 2.8339 - categorical_accuracy: 0.1701 - val_loss: 2.4770 - val_categorical_accuracy: 0.2719\n",
      "Epoch 3/100\n",
      "121312/121513 [============================>.] - ETA: 0s - loss: 2.4040 - categorical_accuracy: 0.2862\n",
      "Epoch 00003: val_loss improved from 2.47703 to 2.05583, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 22s 183us/sample - loss: 2.4042 - categorical_accuracy: 0.2862 - val_loss: 2.0558 - val_categorical_accuracy: 0.4046\n",
      "Epoch 4/100\n",
      "121440/121513 [============================>.] - ETA: 0s - loss: 2.0898 - categorical_accuracy: 0.3829\n",
      "Epoch 00004: val_loss improved from 2.05583 to 1.80568, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 20s 168us/sample - loss: 2.0899 - categorical_accuracy: 0.3829 - val_loss: 1.8057 - val_categorical_accuracy: 0.4727\n",
      "Epoch 5/100\n",
      "121504/121513 [============================>.] - ETA: 0s - loss: 1.9112 - categorical_accuracy: 0.4359\n",
      "Epoch 00005: val_loss improved from 1.80568 to 1.65807, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 175us/sample - loss: 1.9112 - categorical_accuracy: 0.4359 - val_loss: 1.6581 - val_categorical_accuracy: 0.5239\n",
      "Epoch 6/100\n",
      "121408/121513 [============================>.] - ETA: 0s - loss: 1.7909 - categorical_accuracy: 0.4723\n",
      "Epoch 00006: val_loss improved from 1.65807 to 1.55144, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 23s 190us/sample - loss: 1.7911 - categorical_accuracy: 0.4723 - val_loss: 1.5514 - val_categorical_accuracy: 0.5553\n",
      "Epoch 7/100\n",
      "121440/121513 [============================>.] - ETA: 0s - loss: 1.7005 - categorical_accuracy: 0.4974\n",
      "Epoch 00007: val_loss improved from 1.55144 to 1.50627, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 20s 166us/sample - loss: 1.7005 - categorical_accuracy: 0.4974 - val_loss: 1.5063 - val_categorical_accuracy: 0.5621\n",
      "Epoch 8/100\n",
      "121056/121513 [============================>.] - ETA: 0s - loss: 1.6302 - categorical_accuracy: 0.5173\n",
      "Epoch 00008: val_loss improved from 1.50627 to 1.43762, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 22s 179us/sample - loss: 1.6302 - categorical_accuracy: 0.5173 - val_loss: 1.4376 - val_categorical_accuracy: 0.5886\n",
      "Epoch 9/100\n",
      "121344/121513 [============================>.] - ETA: 0s - loss: 1.5786 - categorical_accuracy: 0.5348\n",
      "Epoch 00009: val_loss improved from 1.43762 to 1.38555, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 23s 189us/sample - loss: 1.5785 - categorical_accuracy: 0.5348 - val_loss: 1.3856 - val_categorical_accuracy: 0.6033\n",
      "Epoch 10/100\n",
      "121056/121513 [============================>.] - ETA: 0s - loss: 1.5307 - categorical_accuracy: 0.5487\n",
      "Epoch 00010: val_loss did not improve from 1.38555\n",
      "121513/121513 [==============================] - 21s 170us/sample - loss: 1.5312 - categorical_accuracy: 0.5486 - val_loss: 1.4267 - val_categorical_accuracy: 0.5840\n",
      "Epoch 11/100\n",
      "121280/121513 [============================>.] - ETA: 0s - loss: 1.4946 - categorical_accuracy: 0.5576\n",
      "Epoch 00011: val_loss improved from 1.38555 to 1.35934, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 176us/sample - loss: 1.4942 - categorical_accuracy: 0.5577 - val_loss: 1.3593 - val_categorical_accuracy: 0.6068\n",
      "Epoch 12/100\n",
      "121440/121513 [============================>.] - ETA: 0s - loss: 1.4625 - categorical_accuracy: 0.5685\n",
      "Epoch 00012: val_loss improved from 1.35934 to 1.28654, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 23s 185us/sample - loss: 1.4624 - categorical_accuracy: 0.5685 - val_loss: 1.2865 - val_categorical_accuracy: 0.6371\n",
      "Epoch 13/100\n",
      "121472/121513 [============================>.] - ETA: 0s - loss: 1.4371 - categorical_accuracy: 0.5758\n",
      "Epoch 00013: val_loss did not improve from 1.28654\n",
      "121513/121513 [==============================] - 21s 173us/sample - loss: 1.4372 - categorical_accuracy: 0.5758 - val_loss: 1.3008 - val_categorical_accuracy: 0.6248\n",
      "Epoch 14/100\n",
      "121440/121513 [============================>.] - ETA: 0s - loss: 1.4105 - categorical_accuracy: 0.5837\n",
      "Epoch 00014: val_loss improved from 1.28654 to 1.24073, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 174us/sample - loss: 1.4106 - categorical_accuracy: 0.5837 - val_loss: 1.2407 - val_categorical_accuracy: 0.6482\n",
      "Epoch 15/100\n",
      "121312/121513 [============================>.] - ETA: 0s - loss: 1.3896 - categorical_accuracy: 0.5895\n",
      "Epoch 00015: val_loss improved from 1.24073 to 1.22540, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 23s 191us/sample - loss: 1.3894 - categorical_accuracy: 0.5895 - val_loss: 1.2254 - val_categorical_accuracy: 0.6582\n",
      "Epoch 16/100\n",
      "121504/121513 [============================>.] - ETA: 0s - loss: 1.3736 - categorical_accuracy: 0.5954\n",
      "Epoch 00016: val_loss improved from 1.22540 to 1.22205, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 177us/sample - loss: 1.3736 - categorical_accuracy: 0.5954 - val_loss: 1.2221 - val_categorical_accuracy: 0.6530\n",
      "Epoch 17/100\n",
      "121216/121513 [============================>.] - ETA: 0s - loss: 1.3507 - categorical_accuracy: 0.6025\n",
      "Epoch 00017: val_loss improved from 1.22205 to 1.20764, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 171us/sample - loss: 1.3504 - categorical_accuracy: 0.6025 - val_loss: 1.2076 - val_categorical_accuracy: 0.6551\n",
      "Epoch 18/100\n",
      "121248/121513 [============================>.] - ETA: 0s - loss: 1.3427 - categorical_accuracy: 0.6047\n",
      "Epoch 00018: val_loss did not improve from 1.20764\n",
      "121513/121513 [==============================] - 22s 183us/sample - loss: 1.3429 - categorical_accuracy: 0.6048 - val_loss: 1.2399 - val_categorical_accuracy: 0.6475\n",
      "Epoch 19/100\n",
      "121280/121513 [============================>.] - ETA: 0s - loss: 1.3255 - categorical_accuracy: 0.6096\n",
      "Epoch 00019: val_loss improved from 1.20764 to 1.17294, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 175us/sample - loss: 1.3256 - categorical_accuracy: 0.6096 - val_loss: 1.1729 - val_categorical_accuracy: 0.6733\n",
      "Epoch 20/100\n",
      "121344/121513 [============================>.] - ETA: 0s - loss: 1.3122 - categorical_accuracy: 0.6128\n",
      "Epoch 00020: val_loss did not improve from 1.17294\n",
      "121513/121513 [==============================] - 21s 173us/sample - loss: 1.3123 - categorical_accuracy: 0.6128 - val_loss: 1.1818 - val_categorical_accuracy: 0.6658\n",
      "Epoch 21/100\n",
      "121280/121513 [============================>.] - ETA: 0s - loss: 1.3028 - categorical_accuracy: 0.6172\n",
      "Epoch 00021: val_loss improved from 1.17294 to 1.16610, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 22s 182us/sample - loss: 1.3027 - categorical_accuracy: 0.6173 - val_loss: 1.1661 - val_categorical_accuracy: 0.6727\n",
      "Epoch 22/100\n",
      "121472/121513 [============================>.] - ETA: 0s - loss: 1.2865 - categorical_accuracy: 0.6227\n",
      "Epoch 00022: val_loss improved from 1.16610 to 1.16026, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 176us/sample - loss: 1.2865 - categorical_accuracy: 0.6227 - val_loss: 1.1603 - val_categorical_accuracy: 0.6747\n",
      "Epoch 23/100\n",
      "121344/121513 [============================>.] - ETA: 0s - loss: 1.2741 - categorical_accuracy: 0.6260\n",
      "Epoch 00023: val_loss did not improve from 1.16026\n",
      "121513/121513 [==============================] - 22s 178us/sample - loss: 1.2742 - categorical_accuracy: 0.6259 - val_loss: 1.1690 - val_categorical_accuracy: 0.6728\n",
      "Epoch 24/100\n",
      "121408/121513 [============================>.] - ETA: 0s - loss: 1.2694 - categorical_accuracy: 0.6266\n",
      "Epoch 00024: val_loss improved from 1.16026 to 1.14438, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 22s 178us/sample - loss: 1.2692 - categorical_accuracy: 0.6266 - val_loss: 1.1444 - val_categorical_accuracy: 0.6810\n",
      "Epoch 25/100\n",
      "121440/121513 [============================>.] - ETA: 0s - loss: 1.2636 - categorical_accuracy: 0.6302\n",
      "Epoch 00025: val_loss did not improve from 1.14438\n",
      "121513/121513 [==============================] - 22s 178us/sample - loss: 1.2636 - categorical_accuracy: 0.6303 - val_loss: 1.1883 - val_categorical_accuracy: 0.6650\n",
      "Epoch 26/100\n",
      "121344/121513 [============================>.] - ETA: 0s - loss: 1.2459 - categorical_accuracy: 0.6370\n",
      "Epoch 00026: val_loss did not improve from 1.14438\n",
      "121513/121513 [==============================] - 21s 177us/sample - loss: 1.2460 - categorical_accuracy: 0.6370 - val_loss: 1.1570 - val_categorical_accuracy: 0.6733\n",
      "Epoch 27/100\n",
      "121312/121513 [============================>.] - ETA: 0s - loss: 1.2399 - categorical_accuracy: 0.6377\n",
      "Epoch 00027: val_loss improved from 1.14438 to 1.11619, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 22s 178us/sample - loss: 1.2402 - categorical_accuracy: 0.6377 - val_loss: 1.1162 - val_categorical_accuracy: 0.6896\n",
      "Epoch 28/100\n",
      "121408/121513 [============================>.] - ETA: 0s - loss: 1.2344 - categorical_accuracy: 0.6400\n",
      "Epoch 00028: val_loss did not improve from 1.11619\n",
      "121513/121513 [==============================] - 21s 175us/sample - loss: 1.2344 - categorical_accuracy: 0.6400 - val_loss: 1.1306 - val_categorical_accuracy: 0.6891\n",
      "Epoch 29/100\n",
      "121344/121513 [============================>.] - ETA: 0s - loss: 1.2317 - categorical_accuracy: 0.6421\n",
      "Epoch 00029: val_loss improved from 1.11619 to 1.10410, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 173us/sample - loss: 1.2318 - categorical_accuracy: 0.6421 - val_loss: 1.1041 - val_categorical_accuracy: 0.6907\n",
      "Epoch 30/100\n",
      "121184/121513 [============================>.] - ETA: 0s - loss: 1.2284 - categorical_accuracy: 0.6431\n",
      "Epoch 00030: val_loss improved from 1.10410 to 1.10372, saving model to t_weights_1\n",
      "121513/121513 [==============================] - 21s 175us/sample - loss: 1.2283 - categorical_accuracy: 0.6431 - val_loss: 1.1037 - val_categorical_accuracy: 0.6967\n",
      "Epoch 31/100\n",
      "121344/121513 [============================>.] - ETA: 0s - loss: 1.2190 - categorical_accuracy: 0.6465\n",
      "Epoch 00031: val_loss did not improve from 1.10372\n",
      "121513/121513 [==============================] - 21s 171us/sample - loss: 1.2191 - categorical_accuracy: 0.6465 - val_loss: 1.1125 - val_categorical_accuracy: 0.6909\n",
      "Epoch 32/100\n",
      "121248/121513 [============================>.] - ETA: 0s - loss: 1.2096 - categorical_accuracy: 0.6494\n",
      "Epoch 00032: val_loss did not improve from 1.10372\n",
      "121513/121513 [==============================] - 21s 171us/sample - loss: 1.2097 - categorical_accuracy: 0.6494 - val_loss: 1.1104 - val_categorical_accuracy: 0.6916\n",
      "Epoch 33/100\n",
      "121440/121513 [============================>.] - ETA: 0s - loss: 1.2052 - categorical_accuracy: 0.6511\n",
      "Epoch 00033: val_loss did not improve from 1.10372\n",
      "121513/121513 [==============================] - 22s 183us/sample - loss: 1.2052 - categorical_accuracy: 0.6511 - val_loss: 1.1989 - val_categorical_accuracy: 0.6591\n",
      "Epoch 34/100\n",
      "121472/121513 [============================>.] - ETA: 0s - loss: 1.2010 - categorical_accuracy: 0.6521\n",
      "Epoch 00034: val_loss did not improve from 1.10372\n",
      "121513/121513 [==============================] - 20s 165us/sample - loss: 1.2010 - categorical_accuracy: 0.6521 - val_loss: 1.1262 - val_categorical_accuracy: 0.6826\n",
      "Epoch 35/100\n",
      "121088/121513 [============================>.] - ETA: 0s - loss: 1.1957 - categorical_accuracy: 0.6535\n",
      "Epoch 00035: val_loss did not improve from 1.10372\n",
      "121513/121513 [==============================] - 21s 172us/sample - loss: 1.1959 - categorical_accuracy: 0.6535 - val_loss: 1.1692 - val_categorical_accuracy: 0.6680\n",
      "0.69589835\n",
      "\n",
      "\n",
      "ntx_i: 80  \n",
      "Train on 217939 samples, validate on 27242 samples\n",
      "Epoch 1/100\n",
      "217888/217939 [============================>.] - ETA: 0s - loss: 3.8531 - categorical_accuracy: 0.0590\n",
      "Epoch 00001: val_loss improved from inf to 3.22134, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 171us/sample - loss: 3.8530 - categorical_accuracy: 0.0590 - val_loss: 3.2213 - val_categorical_accuracy: 0.1640\n",
      "Epoch 2/100\n",
      "217728/217939 [============================>.] - ETA: 0s - loss: 2.9804 - categorical_accuracy: 0.2107\n",
      "Epoch 00002: val_loss improved from 3.22134 to 2.45727, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 36s 165us/sample - loss: 2.9802 - categorical_accuracy: 0.2107 - val_loss: 2.4573 - val_categorical_accuracy: 0.3439\n",
      "Epoch 3/100\n",
      "217792/217939 [============================>.] - ETA: 0s - loss: 2.5166 - categorical_accuracy: 0.3051\n",
      "Epoch 00003: val_loss improved from 2.45727 to 2.22537, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 168us/sample - loss: 2.5164 - categorical_accuracy: 0.3051 - val_loss: 2.2254 - val_categorical_accuracy: 0.3821\n",
      "Epoch 4/100\n",
      "217824/217939 [============================>.] - ETA: 0s - loss: 2.3401 - categorical_accuracy: 0.3424\n",
      "Epoch 00004: val_loss improved from 2.22537 to 2.00936, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 168us/sample - loss: 2.3400 - categorical_accuracy: 0.3424 - val_loss: 2.0094 - val_categorical_accuracy: 0.4442\n",
      "Epoch 5/100\n",
      "217856/217939 [============================>.] - ETA: 0s - loss: 2.2433 - categorical_accuracy: 0.3665\n",
      "Epoch 00005: val_loss improved from 2.00936 to 1.95141, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 38s 172us/sample - loss: 2.2434 - categorical_accuracy: 0.3665 - val_loss: 1.9514 - val_categorical_accuracy: 0.4538\n",
      "Epoch 6/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 2.1815 - categorical_accuracy: 0.3816\n",
      "Epoch 00006: val_loss improved from 1.95141 to 1.92439, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 39s 181us/sample - loss: 2.1816 - categorical_accuracy: 0.3816 - val_loss: 1.9244 - val_categorical_accuracy: 0.4595\n",
      "Epoch 7/100\n",
      "217632/217939 [============================>.] - ETA: 0s - loss: 2.1339 - categorical_accuracy: 0.3946\n",
      "Epoch 00007: val_loss improved from 1.92439 to 1.87188, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 38s 173us/sample - loss: 2.1343 - categorical_accuracy: 0.3945 - val_loss: 1.8719 - val_categorical_accuracy: 0.4778\n",
      "Epoch 8/100\n",
      "217728/217939 [============================>.] - ETA: 0s - loss: 2.0993 - categorical_accuracy: 0.4012\n",
      "Epoch 00008: val_loss improved from 1.87188 to 1.81632, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 39s 181us/sample - loss: 2.0994 - categorical_accuracy: 0.4012 - val_loss: 1.8163 - val_categorical_accuracy: 0.4932\n",
      "Epoch 9/100\n",
      "217632/217939 [============================>.] - ETA: 0s - loss: 2.0680 - categorical_accuracy: 0.4097\n",
      "Epoch 00009: val_loss improved from 1.81632 to 1.79814, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 38s 175us/sample - loss: 2.0680 - categorical_accuracy: 0.4097 - val_loss: 1.7981 - val_categorical_accuracy: 0.4958\n",
      "Epoch 10/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 2.0442 - categorical_accuracy: 0.4163\n",
      "Epoch 00010: val_loss improved from 1.79814 to 1.75289, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 39s 179us/sample - loss: 2.0443 - categorical_accuracy: 0.4163 - val_loss: 1.7529 - val_categorical_accuracy: 0.5147\n",
      "Epoch 11/100\n",
      "217792/217939 [============================>.] - ETA: 0s - loss: 2.0192 - categorical_accuracy: 0.4232\n",
      "Epoch 00011: val_loss did not improve from 1.75289\n",
      "217939/217939 [==============================] - 39s 179us/sample - loss: 2.0192 - categorical_accuracy: 0.4232 - val_loss: 1.7758 - val_categorical_accuracy: 0.5011\n",
      "Epoch 12/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 2.0013 - categorical_accuracy: 0.4278\n",
      "Epoch 00012: val_loss improved from 1.75289 to 1.72128, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 39s 178us/sample - loss: 2.0013 - categorical_accuracy: 0.4278 - val_loss: 1.7213 - val_categorical_accuracy: 0.5144\n",
      "Epoch 13/100\n",
      "217600/217939 [============================>.] - ETA: 0s - loss: 1.9854 - categorical_accuracy: 0.4316\n",
      "Epoch 00013: val_loss did not improve from 1.72128\n",
      "217939/217939 [==============================] - 38s 176us/sample - loss: 1.9856 - categorical_accuracy: 0.4316 - val_loss: 1.7495 - val_categorical_accuracy: 0.5081\n",
      "Epoch 14/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 1.9668 - categorical_accuracy: 0.4380\n",
      "Epoch 00014: val_loss did not improve from 1.72128\n",
      "217939/217939 [==============================] - 40s 182us/sample - loss: 1.9668 - categorical_accuracy: 0.4380 - val_loss: 1.7250 - val_categorical_accuracy: 0.5104\n",
      "Epoch 15/100\n",
      "217632/217939 [============================>.] - ETA: 0s - loss: 1.9584 - categorical_accuracy: 0.4395\n",
      "Epoch 00015: val_loss improved from 1.72128 to 1.68900, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 170us/sample - loss: 1.9586 - categorical_accuracy: 0.4395 - val_loss: 1.6890 - val_categorical_accuracy: 0.5273\n",
      "Epoch 16/100\n",
      "217696/217939 [============================>.] - ETA: 0s - loss: 1.9399 - categorical_accuracy: 0.4439\n",
      "Epoch 00016: val_loss did not improve from 1.68900\n",
      "217939/217939 [==============================] - 37s 170us/sample - loss: 1.9399 - categorical_accuracy: 0.4439 - val_loss: 1.6993 - val_categorical_accuracy: 0.5189\n",
      "Epoch 17/100\n",
      "217664/217939 [============================>.] - ETA: 0s - loss: 1.9331 - categorical_accuracy: 0.4472\n",
      "Epoch 00017: val_loss did not improve from 1.68900\n",
      "217939/217939 [==============================] - 37s 168us/sample - loss: 1.9331 - categorical_accuracy: 0.4472 - val_loss: 1.6928 - val_categorical_accuracy: 0.5254\n",
      "Epoch 18/100\n",
      "217824/217939 [============================>.] - ETA: 0s - loss: 1.9191 - categorical_accuracy: 0.4507\n",
      "Epoch 00018: val_loss did not improve from 1.68900\n",
      "217939/217939 [==============================] - 37s 167us/sample - loss: 1.9192 - categorical_accuracy: 0.4507 - val_loss: 1.7021 - val_categorical_accuracy: 0.5255\n",
      "Epoch 19/100\n",
      "217728/217939 [============================>.] - ETA: 0s - loss: 1.9094 - categorical_accuracy: 0.4530\n",
      "Epoch 00019: val_loss improved from 1.68900 to 1.63260, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 36s 167us/sample - loss: 1.9093 - categorical_accuracy: 0.4530 - val_loss: 1.6326 - val_categorical_accuracy: 0.5419\n",
      "Epoch 20/100\n",
      "217728/217939 [============================>.] - ETA: 0s - loss: 1.9036 - categorical_accuracy: 0.4554\n",
      "Epoch 00020: val_loss did not improve from 1.63260\n",
      "217939/217939 [==============================] - 37s 169us/sample - loss: 1.9037 - categorical_accuracy: 0.4554 - val_loss: 1.6365 - val_categorical_accuracy: 0.5465\n",
      "Epoch 21/100\n",
      "217824/217939 [============================>.] - ETA: 0s - loss: 1.8941 - categorical_accuracy: 0.4574\n",
      "Epoch 00021: val_loss did not improve from 1.63260\n",
      "217939/217939 [==============================] - 37s 168us/sample - loss: 1.8942 - categorical_accuracy: 0.4573 - val_loss: 1.6705 - val_categorical_accuracy: 0.5319\n",
      "Epoch 22/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 1.8841 - categorical_accuracy: 0.4609\n",
      "Epoch 00022: val_loss improved from 1.63260 to 1.62084, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 36s 167us/sample - loss: 1.8841 - categorical_accuracy: 0.4609 - val_loss: 1.6208 - val_categorical_accuracy: 0.5485\n",
      "Epoch 23/100\n",
      "217696/217939 [============================>.] - ETA: 0s - loss: 1.8800 - categorical_accuracy: 0.4612\n",
      "Epoch 00023: val_loss did not improve from 1.62084\n",
      "217939/217939 [==============================] - 37s 168us/sample - loss: 1.8802 - categorical_accuracy: 0.4612 - val_loss: 1.6267 - val_categorical_accuracy: 0.5452\n",
      "Epoch 24/100\n",
      "217760/217939 [============================>.] - ETA: 0s - loss: 1.8710 - categorical_accuracy: 0.4636\n",
      "Epoch 00024: val_loss improved from 1.62084 to 1.61954, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 170us/sample - loss: 1.8711 - categorical_accuracy: 0.4636 - val_loss: 1.6195 - val_categorical_accuracy: 0.5483\n",
      "Epoch 25/100\n",
      "217760/217939 [============================>.] - ETA: 0s - loss: 1.8648 - categorical_accuracy: 0.4651\n",
      "Epoch 00025: val_loss improved from 1.61954 to 1.61772, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 36s 166us/sample - loss: 1.8648 - categorical_accuracy: 0.4651 - val_loss: 1.6177 - val_categorical_accuracy: 0.5478\n",
      "Epoch 26/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 1.8547 - categorical_accuracy: 0.4694\n",
      "Epoch 00026: val_loss improved from 1.61772 to 1.61312, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 170us/sample - loss: 1.8546 - categorical_accuracy: 0.4694 - val_loss: 1.6131 - val_categorical_accuracy: 0.5520\n",
      "Epoch 27/100\n",
      "217888/217939 [============================>.] - ETA: 0s - loss: 1.8537 - categorical_accuracy: 0.4691\n",
      "Epoch 00027: val_loss improved from 1.61312 to 1.61206, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 36s 167us/sample - loss: 1.8537 - categorical_accuracy: 0.4691 - val_loss: 1.6121 - val_categorical_accuracy: 0.5532\n",
      "Epoch 28/100\n",
      "217664/217939 [============================>.] - ETA: 0s - loss: 1.8438 - categorical_accuracy: 0.4723\n",
      "Epoch 00028: val_loss improved from 1.61206 to 1.59234, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 169us/sample - loss: 1.8437 - categorical_accuracy: 0.4723 - val_loss: 1.5923 - val_categorical_accuracy: 0.5515\n",
      "Epoch 29/100\n",
      "217792/217939 [============================>.] - ETA: 0s - loss: 1.8408 - categorical_accuracy: 0.4735\n",
      "Epoch 00029: val_loss improved from 1.59234 to 1.56546, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 170us/sample - loss: 1.8407 - categorical_accuracy: 0.4735 - val_loss: 1.5655 - val_categorical_accuracy: 0.5695\n",
      "Epoch 30/100\n",
      "217888/217939 [============================>.] - ETA: 0s - loss: 1.8344 - categorical_accuracy: 0.4747\n",
      "Epoch 00030: val_loss did not improve from 1.56546\n",
      "217939/217939 [==============================] - 37s 171us/sample - loss: 1.8345 - categorical_accuracy: 0.4747 - val_loss: 1.5750 - val_categorical_accuracy: 0.5615\n",
      "Epoch 31/100\n",
      "217824/217939 [============================>.] - ETA: 0s - loss: 1.8248 - categorical_accuracy: 0.4770\n",
      "Epoch 00031: val_loss improved from 1.56546 to 1.55809, saving model to t_weights_1\n",
      "217939/217939 [==============================] - 37s 170us/sample - loss: 1.8250 - categorical_accuracy: 0.4770 - val_loss: 1.5581 - val_categorical_accuracy: 0.5678\n",
      "Epoch 32/100\n",
      "217856/217939 [============================>.] - ETA: 0s - loss: 1.8257 - categorical_accuracy: 0.4771\n",
      "Epoch 00032: val_loss did not improve from 1.55809\n",
      "217939/217939 [==============================] - 38s 175us/sample - loss: 1.8257 - categorical_accuracy: 0.4771 - val_loss: 1.7704 - val_categorical_accuracy: 0.4978\n",
      "Epoch 33/100\n",
      "217760/217939 [============================>.] - ETA: 0s - loss: 1.8183 - categorical_accuracy: 0.4778\n",
      "Epoch 00033: val_loss did not improve from 1.55809\n",
      "217939/217939 [==============================] - 38s 173us/sample - loss: 1.8182 - categorical_accuracy: 0.4778 - val_loss: 1.6047 - val_categorical_accuracy: 0.5522\n",
      "Epoch 34/100\n",
      "217920/217939 [============================>.] - ETA: 0s - loss: 1.8167 - categorical_accuracy: 0.4786\n",
      "Epoch 00034: val_loss did not improve from 1.55809\n",
      "217939/217939 [==============================] - 39s 177us/sample - loss: 1.8167 - categorical_accuracy: 0.4786 - val_loss: 1.5944 - val_categorical_accuracy: 0.5521\n",
      "Epoch 35/100\n",
      "217856/217939 [============================>.] - ETA: 0s - loss: 1.8089 - categorical_accuracy: 0.4816\n",
      "Epoch 00035: val_loss did not improve from 1.55809\n",
      "217939/217939 [==============================] - 38s 173us/sample - loss: 1.8089 - categorical_accuracy: 0.4816 - val_loss: 1.5642 - val_categorical_accuracy: 0.5663\n",
      "Epoch 36/100\n",
      "217760/217939 [============================>.] - ETA: 0s - loss: 1.8072 - categorical_accuracy: 0.4815\n",
      "Epoch 00036: val_loss did not improve from 1.55809\n",
      "217939/217939 [==============================] - 37s 171us/sample - loss: 1.8072 - categorical_accuracy: 0.4815 - val_loss: 1.5745 - val_categorical_accuracy: 0.5634\n",
      "0.566258\n",
      "\n",
      "\n",
      "ntx_i: 115  \n",
      "Train on 314033 samples, validate on 39254 samples\n",
      "Epoch 1/100\n",
      "313856/314033 [============================>.] - ETA: 0s - loss: 3.8706 - categorical_accuracy: 0.0960\n",
      "Epoch 00001: val_loss improved from inf to 2.93862, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 3.8702 - categorical_accuracy: 0.0961 - val_loss: 2.9386 - val_categorical_accuracy: 0.2532\n",
      "Epoch 2/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.9025 - categorical_accuracy: 0.2430\n",
      "Epoch 00002: val_loss improved from 2.93862 to 2.50192, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.9024 - categorical_accuracy: 0.2430 - val_loss: 2.5019 - val_categorical_accuracy: 0.3404\n",
      "Epoch 3/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.6592 - categorical_accuracy: 0.2902\n",
      "Epoch 00003: val_loss improved from 2.50192 to 2.32107, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 2.6591 - categorical_accuracy: 0.2902 - val_loss: 2.3211 - val_categorical_accuracy: 0.3773\n",
      "Epoch 4/100\n",
      "313824/314033 [============================>.] - ETA: 0s - loss: 2.5345 - categorical_accuracy: 0.3178\n",
      "Epoch 00004: val_loss improved from 2.32107 to 2.24109, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 56s 178us/sample - loss: 2.5344 - categorical_accuracy: 0.3179 - val_loss: 2.2411 - val_categorical_accuracy: 0.3926\n",
      "Epoch 5/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 2.4524 - categorical_accuracy: 0.3365\n",
      "Epoch 00005: val_loss improved from 2.24109 to 2.17146, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 173us/sample - loss: 2.4524 - categorical_accuracy: 0.3365 - val_loss: 2.1715 - val_categorical_accuracy: 0.4143\n",
      "Epoch 6/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.3981 - categorical_accuracy: 0.3495\n",
      "Epoch 00006: val_loss improved from 2.17146 to 2.11417, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 177us/sample - loss: 2.3980 - categorical_accuracy: 0.3495 - val_loss: 2.1142 - val_categorical_accuracy: 0.4300\n",
      "Epoch 7/100\n",
      "313920/314033 [============================>.] - ETA: 0s - loss: 2.3560 - categorical_accuracy: 0.3589\n",
      "Epoch 00007: val_loss improved from 2.11417 to 2.09051, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 2.3560 - categorical_accuracy: 0.3589 - val_loss: 2.0905 - val_categorical_accuracy: 0.4359\n",
      "Epoch 8/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.3201 - categorical_accuracy: 0.3678\n",
      "Epoch 00008: val_loss improved from 2.09051 to 2.04280, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 171us/sample - loss: 2.3200 - categorical_accuracy: 0.3678 - val_loss: 2.0428 - val_categorical_accuracy: 0.4475\n",
      "Epoch 9/100\n",
      "313920/314033 [============================>.] - ETA: 0s - loss: 2.2941 - categorical_accuracy: 0.3751\n",
      "Epoch 00009: val_loss improved from 2.04280 to 2.02518, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 56s 177us/sample - loss: 2.2942 - categorical_accuracy: 0.3751 - val_loss: 2.0252 - val_categorical_accuracy: 0.4534\n",
      "Epoch 10/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.2685 - categorical_accuracy: 0.3805\n",
      "Epoch 00010: val_loss improved from 2.02518 to 1.96442, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 2.2686 - categorical_accuracy: 0.3805 - val_loss: 1.9644 - val_categorical_accuracy: 0.4707\n",
      "Epoch 11/100\n",
      "313920/314033 [============================>.] - ETA: 0s - loss: 2.2422 - categorical_accuracy: 0.3887\n",
      "Epoch 00011: val_loss improved from 1.96442 to 1.96379, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.2421 - categorical_accuracy: 0.3887 - val_loss: 1.9638 - val_categorical_accuracy: 0.4691\n",
      "Epoch 12/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.2249 - categorical_accuracy: 0.3934\n",
      "Epoch 00012: val_loss did not improve from 1.96379\n",
      "314033/314033 [==============================] - 56s 177us/sample - loss: 2.2249 - categorical_accuracy: 0.3934 - val_loss: 2.0247 - val_categorical_accuracy: 0.4508\n",
      "Epoch 13/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.2097 - categorical_accuracy: 0.3977\n",
      "Epoch 00013: val_loss did not improve from 1.96379\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.2097 - categorical_accuracy: 0.3976 - val_loss: 1.9733 - val_categorical_accuracy: 0.4641\n",
      "Epoch 14/100\n",
      "313824/314033 [============================>.] - ETA: 0s - loss: 2.1917 - categorical_accuracy: 0.4021\n",
      "Epoch 00014: val_loss improved from 1.96379 to 1.95943, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.1918 - categorical_accuracy: 0.4020 - val_loss: 1.9594 - val_categorical_accuracy: 0.4674\n",
      "Epoch 15/100\n",
      "313856/314033 [============================>.] - ETA: 0s - loss: 2.1796 - categorical_accuracy: 0.4059\n",
      "Epoch 00015: val_loss improved from 1.95943 to 1.95312, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 52s 167us/sample - loss: 2.1795 - categorical_accuracy: 0.4059 - val_loss: 1.9531 - val_categorical_accuracy: 0.4667\n",
      "Epoch 16/100\n",
      "313824/314033 [============================>.] - ETA: 0s - loss: 2.1661 - categorical_accuracy: 0.4089\n",
      "Epoch 00016: val_loss improved from 1.95312 to 1.88434, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 51s 163us/sample - loss: 2.1661 - categorical_accuracy: 0.4089 - val_loss: 1.8843 - val_categorical_accuracy: 0.4931\n",
      "Epoch 17/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.1538 - categorical_accuracy: 0.4137\n",
      "Epoch 00017: val_loss did not improve from 1.88434\n",
      "314033/314033 [==============================] - 53s 167us/sample - loss: 2.1539 - categorical_accuracy: 0.4137 - val_loss: 1.8950 - val_categorical_accuracy: 0.4892\n",
      "Epoch 18/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.1454 - categorical_accuracy: 0.4152\n",
      "Epoch 00018: val_loss did not improve from 1.88434\n",
      "314033/314033 [==============================] - 51s 163us/sample - loss: 2.1456 - categorical_accuracy: 0.4152 - val_loss: 1.9151 - val_categorical_accuracy: 0.4809\n",
      "Epoch 19/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.1358 - categorical_accuracy: 0.4167\n",
      "Epoch 00019: val_loss did not improve from 1.88434\n",
      "314033/314033 [==============================] - 53s 169us/sample - loss: 2.1359 - categorical_accuracy: 0.4167 - val_loss: 1.9058 - val_categorical_accuracy: 0.4826\n",
      "Epoch 20/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 2.1241 - categorical_accuracy: 0.4196\n",
      "Epoch 00020: val_loss improved from 1.88434 to 1.85393, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 52s 165us/sample - loss: 2.1241 - categorical_accuracy: 0.4196 - val_loss: 1.8539 - val_categorical_accuracy: 0.5053\n",
      "Epoch 21/100\n",
      "313952/314033 [============================>.] - ETA: 0s - loss: 2.1187 - categorical_accuracy: 0.4214\n",
      "Epoch 00021: val_loss did not improve from 1.85393\n",
      "314033/314033 [==============================] - 52s 166us/sample - loss: 2.1186 - categorical_accuracy: 0.4214 - val_loss: 1.9034 - val_categorical_accuracy: 0.4797\n",
      "Epoch 22/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.1084 - categorical_accuracy: 0.4236\n",
      "Epoch 00022: val_loss improved from 1.85393 to 1.83221, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 173us/sample - loss: 2.1085 - categorical_accuracy: 0.4236 - val_loss: 1.8322 - val_categorical_accuracy: 0.5082\n",
      "Epoch 23/100\n",
      "313920/314033 [============================>.] - ETA: 0s - loss: 2.1029 - categorical_accuracy: 0.4241\n",
      "Epoch 00023: val_loss improved from 1.83221 to 1.82437, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 174us/sample - loss: 2.1029 - categorical_accuracy: 0.4241 - val_loss: 1.8244 - val_categorical_accuracy: 0.5083\n",
      "Epoch 24/100\n",
      "313856/314033 [============================>.] - ETA: 0s - loss: 2.0945 - categorical_accuracy: 0.4274\n",
      "Epoch 00024: val_loss improved from 1.82437 to 1.81853, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 2.0944 - categorical_accuracy: 0.4275 - val_loss: 1.8185 - val_categorical_accuracy: 0.5118\n",
      "Epoch 25/100\n",
      "313920/314033 [============================>.] - ETA: 0s - loss: 2.0857 - categorical_accuracy: 0.4302\n",
      "Epoch 00025: val_loss improved from 1.81853 to 1.80798, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.0857 - categorical_accuracy: 0.4302 - val_loss: 1.8080 - val_categorical_accuracy: 0.5158\n",
      "Epoch 26/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.0838 - categorical_accuracy: 0.4302\n",
      "Epoch 00026: val_loss did not improve from 1.80798\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.0837 - categorical_accuracy: 0.4303 - val_loss: 1.8560 - val_categorical_accuracy: 0.4990\n",
      "Epoch 27/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.0753 - categorical_accuracy: 0.4337\n",
      "Epoch 00027: val_loss did not improve from 1.80798\n",
      "314033/314033 [==============================] - 52s 165us/sample - loss: 2.0753 - categorical_accuracy: 0.4337 - val_loss: 1.8873 - val_categorical_accuracy: 0.4855\n",
      "Epoch 28/100\n",
      "313920/314033 [============================>.] - ETA: 0s - loss: 2.0695 - categorical_accuracy: 0.4342\n",
      "Epoch 00028: val_loss did not improve from 1.80798\n",
      "314033/314033 [==============================] - 55s 177us/sample - loss: 2.0695 - categorical_accuracy: 0.4342 - val_loss: 1.8086 - val_categorical_accuracy: 0.5157\n",
      "Epoch 29/100\n",
      "313824/314033 [============================>.] - ETA: 0s - loss: 2.0631 - categorical_accuracy: 0.4356\n",
      "Epoch 00029: val_loss did not improve from 1.80798\n",
      "314033/314033 [==============================] - 56s 178us/sample - loss: 2.0631 - categorical_accuracy: 0.4356 - val_loss: 1.9387 - val_categorical_accuracy: 0.4652\n",
      "Epoch 30/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.0575 - categorical_accuracy: 0.4373\n",
      "Epoch 00030: val_loss improved from 1.80798 to 1.78813, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 52s 166us/sample - loss: 2.0575 - categorical_accuracy: 0.4373 - val_loss: 1.7881 - val_categorical_accuracy: 0.5204\n",
      "Epoch 31/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.0562 - categorical_accuracy: 0.4382\n",
      "Epoch 00031: val_loss did not improve from 1.78813\n",
      "314033/314033 [==============================] - 55s 176us/sample - loss: 2.0562 - categorical_accuracy: 0.4382 - val_loss: 1.8070 - val_categorical_accuracy: 0.5207\n",
      "Epoch 32/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.0505 - categorical_accuracy: 0.4395\n",
      "Epoch 00032: val_loss improved from 1.78813 to 1.77589, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 177us/sample - loss: 2.0505 - categorical_accuracy: 0.4396 - val_loss: 1.7759 - val_categorical_accuracy: 0.5226\n",
      "Epoch 33/100\n",
      "313952/314033 [============================>.] - ETA: 0s - loss: 2.0486 - categorical_accuracy: 0.4402\n",
      "Epoch 00033: val_loss did not improve from 1.77589\n",
      "314033/314033 [==============================] - 55s 176us/sample - loss: 2.0486 - categorical_accuracy: 0.4402 - val_loss: 1.7806 - val_categorical_accuracy: 0.5216\n",
      "Epoch 34/100\n",
      "313952/314033 [============================>.] - ETA: 0s - loss: 2.0400 - categorical_accuracy: 0.4426\n",
      "Epoch 00034: val_loss did not improve from 1.77589\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.0399 - categorical_accuracy: 0.4426 - val_loss: 1.8325 - val_categorical_accuracy: 0.5036\n",
      "Epoch 35/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 2.0374 - categorical_accuracy: 0.4434\n",
      "Epoch 00035: val_loss improved from 1.77589 to 1.77448, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.0374 - categorical_accuracy: 0.4435 - val_loss: 1.7745 - val_categorical_accuracy: 0.5233\n",
      "Epoch 36/100\n",
      "313600/314033 [============================>.] - ETA: 0s - loss: 2.0335 - categorical_accuracy: 0.4436\n",
      "Epoch 00036: val_loss did not improve from 1.77448\n",
      "314033/314033 [==============================] - 54s 172us/sample - loss: 2.0336 - categorical_accuracy: 0.4436 - val_loss: 1.8094 - val_categorical_accuracy: 0.5114\n",
      "Epoch 37/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 2.0301 - categorical_accuracy: 0.4441\n",
      "Epoch 00037: val_loss improved from 1.77448 to 1.75438, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 56s 178us/sample - loss: 2.0301 - categorical_accuracy: 0.4441 - val_loss: 1.7544 - val_categorical_accuracy: 0.5322\n",
      "Epoch 38/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.0262 - categorical_accuracy: 0.4464\n",
      "Epoch 00038: val_loss did not improve from 1.75438\n",
      "314033/314033 [==============================] - 56s 177us/sample - loss: 2.0261 - categorical_accuracy: 0.4464 - val_loss: 1.7659 - val_categorical_accuracy: 0.5214\n",
      "Epoch 39/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 2.0250 - categorical_accuracy: 0.4453\n",
      "Epoch 00039: val_loss did not improve from 1.75438\n",
      "314033/314033 [==============================] - 54s 170us/sample - loss: 2.0250 - categorical_accuracy: 0.4453 - val_loss: 1.7563 - val_categorical_accuracy: 0.5290\n",
      "Epoch 40/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.0195 - categorical_accuracy: 0.4473\n",
      "Epoch 00040: val_loss did not improve from 1.75438\n",
      "314033/314033 [==============================] - 55s 174us/sample - loss: 2.0195 - categorical_accuracy: 0.4473 - val_loss: 1.7719 - val_categorical_accuracy: 0.5257\n",
      "Epoch 41/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.0172 - categorical_accuracy: 0.4482\n",
      "Epoch 00041: val_loss did not improve from 1.75438\n",
      "314033/314033 [==============================] - 54s 174us/sample - loss: 2.0172 - categorical_accuracy: 0.4481 - val_loss: 1.8201 - val_categorical_accuracy: 0.5044\n",
      "Epoch 42/100\n",
      "313952/314033 [============================>.] - ETA: 0s - loss: 2.0126 - categorical_accuracy: 0.4488\n",
      "Epoch 00042: val_loss improved from 1.75438 to 1.75176, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 2.0127 - categorical_accuracy: 0.4487 - val_loss: 1.7518 - val_categorical_accuracy: 0.5283\n",
      "Epoch 43/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.0110 - categorical_accuracy: 0.4493\n",
      "Epoch 00043: val_loss did not improve from 1.75176\n",
      "314033/314033 [==============================] - 55s 176us/sample - loss: 2.0110 - categorical_accuracy: 0.4493 - val_loss: 1.7969 - val_categorical_accuracy: 0.5133\n",
      "Epoch 44/100\n",
      "313984/314033 [============================>.] - ETA: 0s - loss: 2.0070 - categorical_accuracy: 0.4510\n",
      "Epoch 00044: val_loss improved from 1.75176 to 1.74467, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 56s 178us/sample - loss: 2.0070 - categorical_accuracy: 0.4510 - val_loss: 1.7447 - val_categorical_accuracy: 0.5338\n",
      "Epoch 45/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 2.0036 - categorical_accuracy: 0.4510\n",
      "Epoch 00045: val_loss improved from 1.74467 to 1.74328, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 53s 169us/sample - loss: 2.0036 - categorical_accuracy: 0.4510 - val_loss: 1.7433 - val_categorical_accuracy: 0.5329\n",
      "Epoch 46/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 2.0029 - categorical_accuracy: 0.4517\n",
      "Epoch 00046: val_loss did not improve from 1.74328\n",
      "314033/314033 [==============================] - 55s 174us/sample - loss: 2.0029 - categorical_accuracy: 0.4517 - val_loss: 1.7783 - val_categorical_accuracy: 0.5233\n",
      "Epoch 47/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 1.9977 - categorical_accuracy: 0.4523\n",
      "Epoch 00047: val_loss did not improve from 1.74328\n",
      "314033/314033 [==============================] - 54s 173us/sample - loss: 1.9977 - categorical_accuracy: 0.4523 - val_loss: 1.7436 - val_categorical_accuracy: 0.5341\n",
      "Epoch 48/100\n",
      "313472/314033 [============================>.] - ETA: 0s - loss: 1.9957 - categorical_accuracy: 0.4542\n",
      "Epoch 00048: val_loss improved from 1.74328 to 1.73478, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 53s 168us/sample - loss: 1.9957 - categorical_accuracy: 0.4542 - val_loss: 1.7348 - val_categorical_accuracy: 0.5308\n",
      "Epoch 49/100\n",
      "313728/314033 [============================>.] - ETA: 0s - loss: 1.9914 - categorical_accuracy: 0.4556\n",
      "Epoch 00049: val_loss improved from 1.73478 to 1.72686, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 1.9915 - categorical_accuracy: 0.4555 - val_loss: 1.7269 - val_categorical_accuracy: 0.5366\n",
      "Epoch 50/100\n",
      "313792/314033 [============================>.] - ETA: 0s - loss: 1.9893 - categorical_accuracy: 0.4554\n",
      "Epoch 00050: val_loss did not improve from 1.72686\n",
      "314033/314033 [==============================] - 55s 175us/sample - loss: 1.9894 - categorical_accuracy: 0.4554 - val_loss: 1.7486 - val_categorical_accuracy: 0.5296\n",
      "Epoch 51/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 1.9890 - categorical_accuracy: 0.4555\n",
      "Epoch 00051: val_loss did not improve from 1.72686\n",
      "314033/314033 [==============================] - 53s 170us/sample - loss: 1.9890 - categorical_accuracy: 0.4555 - val_loss: 1.7461 - val_categorical_accuracy: 0.5263\n",
      "Epoch 52/100\n",
      "313888/314033 [============================>.] - ETA: 0s - loss: 1.9830 - categorical_accuracy: 0.4564\n",
      "Epoch 00052: val_loss improved from 1.72686 to 1.71649, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 171us/sample - loss: 1.9830 - categorical_accuracy: 0.4564 - val_loss: 1.7165 - val_categorical_accuracy: 0.5448\n",
      "Epoch 53/100\n",
      "313824/314033 [============================>.] - ETA: 0s - loss: 1.9787 - categorical_accuracy: 0.4584\n",
      "Epoch 00053: val_loss did not improve from 1.71649\n",
      "314033/314033 [==============================] - 55s 174us/sample - loss: 1.9787 - categorical_accuracy: 0.4584 - val_loss: 1.7350 - val_categorical_accuracy: 0.5317\n",
      "Epoch 54/100\n",
      "313952/314033 [============================>.] - ETA: 0s - loss: 1.9771 - categorical_accuracy: 0.4587\n",
      "Epoch 00054: val_loss improved from 1.71649 to 1.69155, saving model to t_weights_1\n",
      "314033/314033 [==============================] - 54s 171us/sample - loss: 1.9771 - categorical_accuracy: 0.4587 - val_loss: 1.6915 - val_categorical_accuracy: 0.5485\n",
      "Epoch 55/100\n",
      "313856/314033 [============================>.] - ETA: 0s - loss: 1.9727 - categorical_accuracy: 0.4607\n",
      "Epoch 00055: val_loss did not improve from 1.69155\n",
      "314033/314033 [==============================] - 53s 169us/sample - loss: 1.9726 - categorical_accuracy: 0.4607 - val_loss: 1.7562 - val_categorical_accuracy: 0.5228\n",
      "Epoch 56/100\n",
      "313600/314033 [============================>.] - ETA: 0s - loss: 1.9685 - categorical_accuracy: 0.4605\n",
      "Epoch 00056: val_loss did not improve from 1.69155\n",
      "314033/314033 [==============================] - 53s 170us/sample - loss: 1.9685 - categorical_accuracy: 0.4605 - val_loss: 1.7042 - val_categorical_accuracy: 0.5425\n",
      "Epoch 57/100\n",
      "313856/314033 [============================>.] - ETA: 0s - loss: 1.9651 - categorical_accuracy: 0.4613\n",
      "Epoch 00057: val_loss did not improve from 1.69155\n",
      "314033/314033 [==============================] - 53s 170us/sample - loss: 1.9652 - categorical_accuracy: 0.4612 - val_loss: 1.7504 - val_categorical_accuracy: 0.5277\n",
      "Epoch 58/100\n",
      "313952/314033 [============================>.] - ETA: 0s - loss: 1.9654 - categorical_accuracy: 0.4628\n",
      "Epoch 00058: val_loss did not improve from 1.69155\n",
      "314033/314033 [==============================] - 53s 170us/sample - loss: 1.9654 - categorical_accuracy: 0.4628 - val_loss: 1.7500 - val_categorical_accuracy: 0.5252\n",
      "Epoch 59/100\n",
      "314016/314033 [============================>.] - ETA: 0s - loss: 1.9603 - categorical_accuracy: 0.4634\n",
      "Epoch 00059: val_loss did not improve from 1.69155\n",
      "314033/314033 [==============================] - 55s 174us/sample - loss: 1.9603 - categorical_accuracy: 0.4634 - val_loss: 1.6919 - val_categorical_accuracy: 0.5515\n",
      "0.549167\n",
      "\n",
      "\n",
      "ntx_i: 150  \n",
      "Train on 409213 samples, validate on 51151 samples\n",
      "Epoch 1/100\n",
      "408960/409213 [============================>.] - ETA: 0s - loss: 4.1548 - categorical_accuracy: 0.0750\n",
      "Epoch 00001: val_loss improved from inf to 3.23208, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 73s 179us/sample - loss: 4.1543 - categorical_accuracy: 0.0751 - val_loss: 3.2321 - val_categorical_accuracy: 0.2073\n",
      "Epoch 2/100\n",
      "408928/409213 [============================>.] - ETA: 0s - loss: 3.1307 - categorical_accuracy: 0.2121\n",
      "Epoch 00002: val_loss improved from 3.23208 to 2.71365, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 71s 172us/sample - loss: 3.1307 - categorical_accuracy: 0.2121 - val_loss: 2.7136 - val_categorical_accuracy: 0.3038\n",
      "Epoch 3/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.8216 - categorical_accuracy: 0.2742\n",
      "Epoch 00003: val_loss improved from 2.71365 to 2.52200, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 70s 171us/sample - loss: 2.8216 - categorical_accuracy: 0.2742 - val_loss: 2.5220 - val_categorical_accuracy: 0.3493\n",
      "Epoch 4/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.6921 - categorical_accuracy: 0.3026\n",
      "Epoch 00004: val_loss improved from 2.52200 to 2.37129, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 70s 171us/sample - loss: 2.6921 - categorical_accuracy: 0.3026 - val_loss: 2.3713 - val_categorical_accuracy: 0.3884\n",
      "Epoch 5/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.6136 - categorical_accuracy: 0.3205\n",
      "Epoch 00005: val_loss improved from 2.37129 to 2.30081, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 69s 169us/sample - loss: 2.6136 - categorical_accuracy: 0.3205 - val_loss: 2.3008 - val_categorical_accuracy: 0.4071\n",
      "Epoch 6/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.5599 - categorical_accuracy: 0.3330\n",
      "Epoch 00006: val_loss improved from 2.30081 to 2.26181, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 71s 174us/sample - loss: 2.5599 - categorical_accuracy: 0.3330 - val_loss: 2.2618 - val_categorical_accuracy: 0.4156\n",
      "Epoch 7/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.5135 - categorical_accuracy: 0.3429\n",
      "Epoch 00007: val_loss improved from 2.26181 to 2.17716, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 70s 171us/sample - loss: 2.5134 - categorical_accuracy: 0.3429 - val_loss: 2.1772 - val_categorical_accuracy: 0.4390\n",
      "Epoch 8/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.4785 - categorical_accuracy: 0.3509\n",
      "Epoch 00008: val_loss improved from 2.17716 to 2.16085, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 70s 172us/sample - loss: 2.4786 - categorical_accuracy: 0.3509 - val_loss: 2.1608 - val_categorical_accuracy: 0.4459\n",
      "Epoch 9/100\n",
      "408928/409213 [============================>.] - ETA: 0s - loss: 2.4518 - categorical_accuracy: 0.3579\n",
      "Epoch 00009: val_loss improved from 2.16085 to 2.11101, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 69s 169us/sample - loss: 2.4518 - categorical_accuracy: 0.3579 - val_loss: 2.1110 - val_categorical_accuracy: 0.4610\n",
      "Epoch 10/100\n",
      "408704/409213 [============================>.] - ETA: 0s - loss: 2.4265 - categorical_accuracy: 0.3639\n",
      "Epoch 00010: val_loss did not improve from 2.11101\n",
      "409213/409213 [==============================] - 69s 169us/sample - loss: 2.4265 - categorical_accuracy: 0.3639 - val_loss: 2.1304 - val_categorical_accuracy: 0.4496\n",
      "Epoch 11/100\n",
      "408928/409213 [============================>.] - ETA: 0s - loss: 2.4082 - categorical_accuracy: 0.3679\n",
      "Epoch 00011: val_loss improved from 2.11101 to 2.10058, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 72s 175us/sample - loss: 2.4082 - categorical_accuracy: 0.3679 - val_loss: 2.1006 - val_categorical_accuracy: 0.4602\n",
      "Epoch 12/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.3920 - categorical_accuracy: 0.3719\n",
      "Epoch 00012: val_loss improved from 2.10058 to 2.08537, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 71s 173us/sample - loss: 2.3920 - categorical_accuracy: 0.3720 - val_loss: 2.0854 - val_categorical_accuracy: 0.4616\n",
      "Epoch 13/100\n",
      "409152/409213 [============================>.] - ETA: 0s - loss: 2.3729 - categorical_accuracy: 0.3765\n",
      "Epoch 00013: val_loss did not improve from 2.08537\n",
      "409213/409213 [==============================] - 71s 173us/sample - loss: 2.3729 - categorical_accuracy: 0.3765 - val_loss: 2.1512 - val_categorical_accuracy: 0.4399\n",
      "Epoch 14/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.3580 - categorical_accuracy: 0.3792\n",
      "Epoch 00014: val_loss did not improve from 2.08537\n",
      "409213/409213 [==============================] - 71s 174us/sample - loss: 2.3580 - categorical_accuracy: 0.3792 - val_loss: 2.1200 - val_categorical_accuracy: 0.4447\n",
      "Epoch 15/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.3479 - categorical_accuracy: 0.3813\n",
      "Epoch 00015: val_loss improved from 2.08537 to 2.04804, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 72s 177us/sample - loss: 2.3479 - categorical_accuracy: 0.3813 - val_loss: 2.0480 - val_categorical_accuracy: 0.4740\n",
      "Epoch 16/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.3361 - categorical_accuracy: 0.3850\n",
      "Epoch 00016: val_loss improved from 2.04804 to 2.04235, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 72s 177us/sample - loss: 2.3361 - categorical_accuracy: 0.3850 - val_loss: 2.0423 - val_categorical_accuracy: 0.4721\n",
      "Epoch 17/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.3228 - categorical_accuracy: 0.3876\n",
      "Epoch 00017: val_loss improved from 2.04235 to 2.03755, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 72s 177us/sample - loss: 2.3228 - categorical_accuracy: 0.3876 - val_loss: 2.0375 - val_categorical_accuracy: 0.4738\n",
      "Epoch 18/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.3167 - categorical_accuracy: 0.3891\n",
      "Epoch 00018: val_loss did not improve from 2.03755\n",
      "409213/409213 [==============================] - 71s 174us/sample - loss: 2.3168 - categorical_accuracy: 0.3891 - val_loss: 2.0489 - val_categorical_accuracy: 0.4663\n",
      "Epoch 19/100\n",
      "408928/409213 [============================>.] - ETA: 0s - loss: 2.3077 - categorical_accuracy: 0.3916\n",
      "Epoch 00019: val_loss improved from 2.03755 to 1.97770, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 72s 176us/sample - loss: 2.3077 - categorical_accuracy: 0.3916 - val_loss: 1.9777 - val_categorical_accuracy: 0.4921\n",
      "Epoch 20/100\n",
      "409120/409213 [============================>.] - ETA: 0s - loss: 2.2972 - categorical_accuracy: 0.3945\n",
      "Epoch 00020: val_loss did not improve from 1.97770\n",
      "409213/409213 [==============================] - 72s 176us/sample - loss: 2.2972 - categorical_accuracy: 0.3945 - val_loss: 1.9949 - val_categorical_accuracy: 0.4863\n",
      "Epoch 21/100\n",
      "408928/409213 [============================>.] - ETA: 0s - loss: 2.2924 - categorical_accuracy: 0.3955\n",
      "Epoch 00021: val_loss did not improve from 1.97770\n",
      "409213/409213 [==============================] - 72s 176us/sample - loss: 2.2925 - categorical_accuracy: 0.3955 - val_loss: 2.0171 - val_categorical_accuracy: 0.4809\n",
      "Epoch 22/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.2848 - categorical_accuracy: 0.3966\n",
      "Epoch 00022: val_loss improved from 1.97770 to 1.97166, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 73s 179us/sample - loss: 2.2848 - categorical_accuracy: 0.3966 - val_loss: 1.9717 - val_categorical_accuracy: 0.4952\n",
      "Epoch 23/100\n",
      "409024/409213 [============================>.] - ETA: 0s - loss: 2.2770 - categorical_accuracy: 0.3995\n",
      "Epoch 00023: val_loss improved from 1.97166 to 1.95482, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 72s 177us/sample - loss: 2.2770 - categorical_accuracy: 0.3995 - val_loss: 1.9548 - val_categorical_accuracy: 0.4959\n",
      "Epoch 24/100\n",
      "409088/409213 [============================>.] - ETA: 0s - loss: 2.2711 - categorical_accuracy: 0.4008\n",
      "Epoch 00024: val_loss did not improve from 1.95482\n",
      "409213/409213 [==============================] - 71s 174us/sample - loss: 2.2711 - categorical_accuracy: 0.4008 - val_loss: 1.9866 - val_categorical_accuracy: 0.4864\n",
      "Epoch 25/100\n",
      "408928/409213 [============================>.] - ETA: 0s - loss: 2.2643 - categorical_accuracy: 0.4022\n",
      "Epoch 00025: val_loss did not improve from 1.95482\n",
      "409213/409213 [==============================] - 70s 172us/sample - loss: 2.2642 - categorical_accuracy: 0.4022 - val_loss: 1.9764 - val_categorical_accuracy: 0.4857\n",
      "Epoch 26/100\n",
      "408800/409213 [============================>.] - ETA: 0s - loss: 2.2569 - categorical_accuracy: 0.4033\n",
      "Epoch 00026: val_loss did not improve from 1.95482\n",
      "409213/409213 [==============================] - 71s 175us/sample - loss: 2.2569 - categorical_accuracy: 0.4033 - val_loss: 1.9661 - val_categorical_accuracy: 0.4943\n",
      "Epoch 27/100\n",
      "409152/409213 [============================>.] - ETA: 0s - loss: 2.2544 - categorical_accuracy: 0.4053\n",
      "Epoch 00027: val_loss did not improve from 1.95482\n",
      "409213/409213 [==============================] - 72s 177us/sample - loss: 2.2544 - categorical_accuracy: 0.4053 - val_loss: 1.9594 - val_categorical_accuracy: 0.4950\n",
      "Epoch 28/100\n",
      "409152/409213 [============================>.] - ETA: 0s - loss: 2.2482 - categorical_accuracy: 0.4066\n",
      "Epoch 00028: val_loss improved from 1.95482 to 1.94740, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 75s 183us/sample - loss: 2.2481 - categorical_accuracy: 0.4067 - val_loss: 1.9474 - val_categorical_accuracy: 0.4990\n",
      "Epoch 29/100\n",
      "408960/409213 [============================>.] - ETA: 0s - loss: 2.2432 - categorical_accuracy: 0.4081\n",
      "Epoch 00029: val_loss did not improve from 1.94740\n",
      "409213/409213 [==============================] - 78s 192us/sample - loss: 2.2433 - categorical_accuracy: 0.4081 - val_loss: 1.9575 - val_categorical_accuracy: 0.4968\n",
      "Epoch 30/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.2359 - categorical_accuracy: 0.4094\n",
      "Epoch 00030: val_loss did not improve from 1.94740\n",
      "409213/409213 [==============================] - 79s 193us/sample - loss: 2.2359 - categorical_accuracy: 0.4094 - val_loss: 1.9743 - val_categorical_accuracy: 0.4864\n",
      "Epoch 31/100\n",
      "409024/409213 [============================>.] - ETA: 0s - loss: 2.2356 - categorical_accuracy: 0.4097\n",
      "Epoch 00031: val_loss did not improve from 1.94740\n",
      "409213/409213 [==============================] - 78s 192us/sample - loss: 2.2355 - categorical_accuracy: 0.4097 - val_loss: 1.9755 - val_categorical_accuracy: 0.4854\n",
      "Epoch 32/100\n",
      "409120/409213 [============================>.] - ETA: 0s - loss: 2.2251 - categorical_accuracy: 0.4125\n",
      "Epoch 00032: val_loss did not improve from 1.94740\n",
      "409213/409213 [==============================] - 80s 196us/sample - loss: 2.2251 - categorical_accuracy: 0.4125 - val_loss: 1.9547 - val_categorical_accuracy: 0.4967\n",
      "Epoch 33/100\n",
      "409152/409213 [============================>.] - ETA: 0s - loss: 2.2215 - categorical_accuracy: 0.4126\n",
      "Epoch 00033: val_loss improved from 1.94740 to 1.90138, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 88s 214us/sample - loss: 2.2215 - categorical_accuracy: 0.4126 - val_loss: 1.9014 - val_categorical_accuracy: 0.5149\n",
      "Epoch 34/100\n",
      "409024/409213 [============================>.] - ETA: 0s - loss: 2.2180 - categorical_accuracy: 0.4142\n",
      "Epoch 00034: val_loss did not improve from 1.90138\n",
      "409213/409213 [==============================] - 85s 209us/sample - loss: 2.2180 - categorical_accuracy: 0.4142 - val_loss: 1.9662 - val_categorical_accuracy: 0.4859\n",
      "Epoch 35/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.2168 - categorical_accuracy: 0.4143\n",
      "Epoch 00035: val_loss improved from 1.90138 to 1.88472, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 81s 198us/sample - loss: 2.2169 - categorical_accuracy: 0.4143 - val_loss: 1.8847 - val_categorical_accuracy: 0.5169\n",
      "Epoch 36/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.2097 - categorical_accuracy: 0.4154\n",
      "Epoch 00036: val_loss did not improve from 1.88472\n",
      "409213/409213 [==============================] - 82s 200us/sample - loss: 2.2098 - categorical_accuracy: 0.4154 - val_loss: 1.9039 - val_categorical_accuracy: 0.5131\n",
      "Epoch 37/100\n",
      "409120/409213 [============================>.] - ETA: 0s - loss: 2.2070 - categorical_accuracy: 0.4162\n",
      "Epoch 00037: val_loss did not improve from 1.88472\n",
      "409213/409213 [==============================] - 81s 199us/sample - loss: 2.2070 - categorical_accuracy: 0.4162 - val_loss: 1.9142 - val_categorical_accuracy: 0.5037\n",
      "Epoch 38/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.2023 - categorical_accuracy: 0.4179\n",
      "Epoch 00038: val_loss improved from 1.88472 to 1.87883, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 81s 197us/sample - loss: 2.2022 - categorical_accuracy: 0.4180 - val_loss: 1.8788 - val_categorical_accuracy: 0.5205\n",
      "Epoch 39/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.2008 - categorical_accuracy: 0.4187\n",
      "Epoch 00039: val_loss did not improve from 1.87883\n",
      "409213/409213 [==============================] - 81s 198us/sample - loss: 2.2008 - categorical_accuracy: 0.4186 - val_loss: 1.9169 - val_categorical_accuracy: 0.5049\n",
      "Epoch 40/100\n",
      "408960/409213 [============================>.] - ETA: 0s - loss: 2.1951 - categorical_accuracy: 0.4198\n",
      "Epoch 00040: val_loss did not improve from 1.87883\n",
      "409213/409213 [==============================] - 81s 198us/sample - loss: 2.1951 - categorical_accuracy: 0.4198 - val_loss: 1.8888 - val_categorical_accuracy: 0.5160\n",
      "Epoch 41/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.1919 - categorical_accuracy: 0.4203\n",
      "Epoch 00041: val_loss did not improve from 1.87883\n",
      "409213/409213 [==============================] - 81s 198us/sample - loss: 2.1918 - categorical_accuracy: 0.4203 - val_loss: 1.8944 - val_categorical_accuracy: 0.5114\n",
      "Epoch 42/100\n",
      "409120/409213 [============================>.] - ETA: 0s - loss: 2.1859 - categorical_accuracy: 0.4221\n",
      "Epoch 00042: val_loss improved from 1.87883 to 1.87741, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 86s 209us/sample - loss: 2.1859 - categorical_accuracy: 0.4221 - val_loss: 1.8774 - val_categorical_accuracy: 0.5180\n",
      "Epoch 43/100\n",
      "409152/409213 [============================>.] - ETA: 0s - loss: 2.1862 - categorical_accuracy: 0.4225\n",
      "Epoch 00043: val_loss improved from 1.87741 to 1.87129, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 89s 216us/sample - loss: 2.1861 - categorical_accuracy: 0.4225 - val_loss: 1.8713 - val_categorical_accuracy: 0.5196\n",
      "Epoch 44/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.1776 - categorical_accuracy: 0.4243\n",
      "Epoch 00044: val_loss improved from 1.87129 to 1.86337, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 88s 216us/sample - loss: 2.1776 - categorical_accuracy: 0.4243 - val_loss: 1.8634 - val_categorical_accuracy: 0.5204\n",
      "Epoch 45/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.1758 - categorical_accuracy: 0.4248\n",
      "Epoch 00045: val_loss improved from 1.86337 to 1.86154, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 86s 209us/sample - loss: 2.1758 - categorical_accuracy: 0.4248 - val_loss: 1.8615 - val_categorical_accuracy: 0.5228\n",
      "Epoch 46/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.1767 - categorical_accuracy: 0.4241\n",
      "Epoch 00046: val_loss did not improve from 1.86154\n",
      "409213/409213 [==============================] - 85s 208us/sample - loss: 2.1767 - categorical_accuracy: 0.4241 - val_loss: 1.8629 - val_categorical_accuracy: 0.5253\n",
      "Epoch 47/100\n",
      "409088/409213 [============================>.] - ETA: 0s - loss: 2.1694 - categorical_accuracy: 0.4260\n",
      "Epoch 00047: val_loss improved from 1.86154 to 1.83572, saving model to t_weights_1\n",
      "409213/409213 [==============================] - 86s 210us/sample - loss: 2.1694 - categorical_accuracy: 0.4260 - val_loss: 1.8357 - val_categorical_accuracy: 0.5316\n",
      "Epoch 48/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.1621 - categorical_accuracy: 0.4283\n",
      "Epoch 00048: val_loss did not improve from 1.83572\n",
      "409213/409213 [==============================] - 89s 219us/sample - loss: 2.1622 - categorical_accuracy: 0.4283 - val_loss: 1.8849 - val_categorical_accuracy: 0.5127\n",
      "Epoch 49/100\n",
      "409056/409213 [============================>.] - ETA: 0s - loss: 2.1624 - categorical_accuracy: 0.4283\n",
      "Epoch 00049: val_loss did not improve from 1.83572\n",
      "409213/409213 [==============================] - 87s 213us/sample - loss: 2.1624 - categorical_accuracy: 0.4284 - val_loss: 1.8874 - val_categorical_accuracy: 0.5088\n",
      "Epoch 50/100\n",
      "409152/409213 [============================>.] - ETA: 0s - loss: 2.1575 - categorical_accuracy: 0.4288\n",
      "Epoch 00050: val_loss did not improve from 1.83572\n",
      "409213/409213 [==============================] - 87s 214us/sample - loss: 2.1575 - categorical_accuracy: 0.4288 - val_loss: 1.8537 - val_categorical_accuracy: 0.5308\n",
      "Epoch 51/100\n",
      "408992/409213 [============================>.] - ETA: 0s - loss: 2.1569 - categorical_accuracy: 0.4304\n",
      "Epoch 00051: val_loss did not improve from 1.83572\n",
      "409213/409213 [==============================] - 88s 214us/sample - loss: 2.1569 - categorical_accuracy: 0.4304 - val_loss: 1.8700 - val_categorical_accuracy: 0.5207\n",
      "Epoch 52/100\n",
      "409184/409213 [============================>.] - ETA: 0s - loss: 2.1520 - categorical_accuracy: 0.4303\n",
      "Epoch 00052: val_loss did not improve from 1.83572\n",
      "409213/409213 [==============================] - 87s 212us/sample - loss: 2.1520 - categorical_accuracy: 0.4303 - val_loss: 1.8467 - val_categorical_accuracy: 0.5253\n",
      "0.53099644\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "continue_training = True\n",
    "nreal = 5\n",
    "\n",
    "real_list = list(range(nreal))\n",
    "\n",
    "ntx_list = [10, 45, 80, 115,150]\n",
    "print(ntx_list)\n",
    "\n",
    "patience = 5\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "smTest_results_real = []\n",
    "dfTest_results_real = []\n",
    "dfTestBal_results_real = []\n",
    "\n",
    "for ntx_i in ntx_list:\n",
    "    print(\"\");print(\"\")\n",
    "    print(\"ntx_i: {}  \".format(ntx_i))\n",
    "    fname_w = 'weights/d006_{:04d}.hd5'.format(ntx_i)\n",
    "    tx_train_list= tx_list[0:ntx_i]\n",
    "\n",
    "    dataset = merge_compact_dataset(compact_dataset,capture_date_list,tx_train_list,rx_list, equalized=equalized)\n",
    "    \n",
    "\n",
    "\n",
    "    train_augset,val_augset,test_augset_smRx =  prepare_dataset(dataset,tx_train_list,\n",
    "                                                        val_frac=0.1, test_frac=0.1)\n",
    "    [sig_train,txidNum_train,txid_train,cls_weights] = train_augset\n",
    "    [sig_valid,txidNum_valid,txid_valid,_] = val_augset\n",
    "    [sig_smTest,txidNum_smTest,txid_smTest,cls_weights] = test_augset_smRx\n",
    "\n",
    "    if continue_training:\n",
    "        skip = os.path.isfile(fname_w)\n",
    "    else:\n",
    "        skip = False\n",
    "    classifier = create_net(ntx_i)\n",
    "    if TRAIN and not skip:\n",
    "        filepath = 't_weights_'+GPU\n",
    "        c=[ keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "          keras.callbacks.EarlyStopping(monitor='val_loss',  patience=patience)]\n",
    "        history = classifier.fit(sig_train,txid_train,class_weight=cls_weights,\n",
    "                                 validation_data=(sig_valid , txid_valid),callbacks=c, epochs=n_epochs)\n",
    "        classifier.load_weights(filepath)\n",
    "        classifier.save_weights(fname_w,save_format=\"h5\")\n",
    "    else:\n",
    "        classifier.load_weights(fname_w)\n",
    "\n",
    "    smTest_r = classifier.evaluate(sig_smTest,txid_smTest,verbose=0)[1]\n",
    "\n",
    "\n",
    "    print(smTest_r)\n",
    "    smTest_results_real.append(smTest_r)\n",
    "    K.clear_session()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 45, 80, 115, 150]\n",
      "[0.8155456, 0.69589835, 0.566258, 0.549167, 0.53099644]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ntx_list,smTest_results_real)\n",
    "plt.plot(ntx_list,1/np.array(ntx_list),'--')\n",
    "plt.xlabel('Number of Tx')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Result','Random'])\n",
    "print(ntx_list)\n",
    "print(smTest_results_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
