{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import scipy,scipy.spatial\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "from  data_utilities import *\n",
    "# from definitions import *\n",
    "# from run_train_eval_net import run_train_eval_net,run_eval_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 32\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'ManyRx'\n",
    "dataset_path='../../orbit_rf_dataset/data/compact_pkl_datasets/'\n",
    "\n",
    "compact_dataset = load_compact_pkl_dataset(dataset_path,dataset_name)\n",
    "\n",
    "tx_list = compact_dataset['tx_list']\n",
    "rx_list = compact_dataset['rx_list']\n",
    "\n",
    "equalized = 1\n",
    "\n",
    "capture_date_list = compact_dataset['capture_date_list']\n",
    "capture_date = capture_date_list[0]\n",
    "n_tx = len(tx_list)\n",
    "n_rx = len(rx_list)\n",
    "print(n_tx,n_rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['19-20', '24-13', '19-2', '1-20', '20-20', '20-1', '7-7', '3-19', '23-6', '2-19', '24-5', '14-7', '23-1', '19-1', '8-7', '24-6', '24-16', '1-19', '8-8', '18-19', '13-7', '23-3', '8-14', '23-5', '19-19', '18-2', '7-14', '13-14', '1-1', '23-7', '20-19', '2-1'], ['1-1', '23-1', '24-6', '8-8', '1-19', '23-3', '23-5', '7-14', '19-19', '13-14', '23-6', '7-7', '19-1', '20-1', '20-20', '24-16', '8-14', '19-2', '14-7', '1-20', '13-7', '24-5', '18-2', '2-19', '24-13', '19-20', '3-19', '8-7', '20-19', '2-1', '23-7', '18-19'], ['24-5', '23-7', '18-19', '23-3', '24-6', '8-14', '2-1', '13-7', '19-19', '19-2', '18-2', '1-20', '20-1', '24-16', '3-19', '1-19', '24-13', '23-6', '19-1', '8-7', '20-19', '1-1', '14-7', '20-20', '7-7', '2-19', '23-5', '8-8', '19-20', '13-14', '7-14', '23-1'], ['1-20', '1-19', '20-19', '23-7', '2-1', '20-1', '19-19', '14-7', '23-1', '3-19', '8-8', '7-7', '13-7', '20-20', '24-16', '18-2', '8-7', '23-5', '7-14', '18-19', '24-6', '19-1', '13-14', '2-19', '19-20', '24-5', '23-3', '19-2', '8-14', '23-6', '24-13', '1-1'], ['18-19', '20-1', '13-14', '18-2', '14-7', '20-20', '13-7', '19-20', '2-19', '19-19', '19-1', '1-20', '24-5', '20-19', '8-7', '23-1', '7-7', '8-8', '2-1', '1-19', '3-19', '23-3', '23-6', '23-5', '24-6', '8-14', '24-16', '7-14', '1-1', '19-2', '24-13', '23-7']]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_real = 5\n",
    "rx_list_real = []\n",
    "for i in range(n_real):\n",
    "    np.random.shuffle(rx_list)\n",
    "    rx_list_real.append(np.copy(rx_list).tolist())\n",
    "print(rx_list_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/samer/miniconda3/envs/mod_framework/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 2)]          0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 256, 2, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 256, 2, 8)         56        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 2, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 2, 16)        784       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 2, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 2, 16)         1552      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 1, 32)         1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 1, 16)         1552      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                810       \n",
      "=================================================================\n",
      "Total params: 40,102\n",
      "Trainable params: 40,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " def create_net():\n",
    "\n",
    "\n",
    "    inputs = Input(shape=(256,2))\n",
    "    x = Reshape((256,2,1))(inputs)\n",
    "    x = Conv2D(8,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,2),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "    x = Conv2D(32,(3,1),activation='relu',padding = 'same')(x)\n",
    "    x = MaxPool2D((2,1))(x)\n",
    "    x = Conv2D(16,(3,1),activation='relu',padding = 'same')(x)\n",
    "    #x = resnet(x,64,(3,2),'6')\n",
    "    #x = MaxPool2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = Dense(100, activation='relu', kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    # x = Dropout(0.3)(x)\n",
    "    x = Dense(80, activation='relu',kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(n_tx, activation='softmax',kernel_regularizer = keras.regularizers.l2(0.0001))(x)\n",
    "    ops = x\n",
    "\n",
    "    classifier = Model(inputs,ops)\n",
    "    classifier.compile(loss='categorical_crossentropy',metrics=['categorical_accuracy'],optimizer=keras.optimizers.Adam(0.0005))\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "classifier = create_net()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(classifier):\n",
    "    pred = classifier.predict(sig_dfTest)\n",
    "    acc = np.mean(np.argmax(pred,1)==txidNum_dfTest)\n",
    "\n",
    "    test_indx = ()\n",
    "    for indx in range(len(tx_list)):\n",
    "        cls_indx = np.where(txidNum_dfTest == indx)\n",
    "        test_indx = test_indx + (cls_indx[0][:n_test_samples],)\n",
    "    test_indx = np.concatenate(test_indx) \n",
    "    acc_bal = np.mean(np.argmax(pred[test_indx,:],1)==txidNum_dfTest[test_indx])\n",
    "    return acc,acc_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_rx = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 10, 15, 20, 25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(range( 0,len(rx_list_real[0])-n_test_rx+1,5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/Documents/txid_framework/dataset_scripts/working/data_utilities.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cls_weights = np.max(stat,axis=0)/stat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "nrx: 0 - real: 0 \n",
      "0.93 0.3165\n",
      "\n",
      "\n",
      "nrx: 5 - real: 0 \n",
      "Train on 9440 samples, validate on 1180 samples\n",
      "Epoch 1/100\n",
      "9376/9440 [============================>.] - ETA: 0s - loss: 1.8050 - categorical_accuracy: 0.3344\n",
      "Epoch 00001: val_loss improved from inf to 1.14972, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 261us/sample - loss: 1.8019 - categorical_accuracy: 0.3357 - val_loss: 1.1497 - val_categorical_accuracy: 0.6212\n",
      "Epoch 2/100\n",
      "9216/9440 [============================>.] - ETA: 0s - loss: 1.0347 - categorical_accuracy: 0.6133\n",
      "Epoch 00002: val_loss improved from 1.14972 to 0.76160, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 215us/sample - loss: 1.0334 - categorical_accuracy: 0.6140 - val_loss: 0.7616 - val_categorical_accuracy: 0.7449\n",
      "Epoch 3/100\n",
      "9408/9440 [============================>.] - ETA: 0s - loss: 0.7896 - categorical_accuracy: 0.7061\n",
      "Epoch 00003: val_loss improved from 0.76160 to 0.57286, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 219us/sample - loss: 0.7895 - categorical_accuracy: 0.7060 - val_loss: 0.5729 - val_categorical_accuracy: 0.7907\n",
      "Epoch 4/100\n",
      "9152/9440 [============================>.] - ETA: 0s - loss: 0.6543 - categorical_accuracy: 0.7626\n",
      "Epoch 00004: val_loss improved from 0.57286 to 0.50889, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 214us/sample - loss: 0.6572 - categorical_accuracy: 0.7626 - val_loss: 0.5089 - val_categorical_accuracy: 0.8288\n",
      "Epoch 5/100\n",
      "9248/9440 [============================>.] - ETA: 0s - loss: 0.5721 - categorical_accuracy: 0.8033\n",
      "Epoch 00005: val_loss improved from 0.50889 to 0.43221, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 211us/sample - loss: 0.5687 - categorical_accuracy: 0.8051 - val_loss: 0.4322 - val_categorical_accuracy: 0.8500\n",
      "Epoch 6/100\n",
      "9376/9440 [============================>.] - ETA: 0s - loss: 0.5020 - categorical_accuracy: 0.8285\n",
      "Epoch 00006: val_loss improved from 0.43221 to 0.39560, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 208us/sample - loss: 0.5026 - categorical_accuracy: 0.8283 - val_loss: 0.3956 - val_categorical_accuracy: 0.8669\n",
      "Epoch 7/100\n",
      "9120/9440 [===========================>..] - ETA: 0s - loss: 0.4620 - categorical_accuracy: 0.8418\n",
      "Epoch 00007: val_loss improved from 0.39560 to 0.39190, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 207us/sample - loss: 0.4625 - categorical_accuracy: 0.8413 - val_loss: 0.3919 - val_categorical_accuracy: 0.8627\n",
      "Epoch 8/100\n",
      "9376/9440 [============================>.] - ETA: 0s - loss: 0.4236 - categorical_accuracy: 0.8573\n",
      "Epoch 00008: val_loss improved from 0.39190 to 0.35144, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 209us/sample - loss: 0.4230 - categorical_accuracy: 0.8576 - val_loss: 0.3514 - val_categorical_accuracy: 0.8788\n",
      "Epoch 9/100\n",
      "9248/9440 [============================>.] - ETA: 0s - loss: 0.3951 - categorical_accuracy: 0.8683\n",
      "Epoch 00009: val_loss improved from 0.35144 to 0.33348, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 213us/sample - loss: 0.3969 - categorical_accuracy: 0.8673 - val_loss: 0.3335 - val_categorical_accuracy: 0.8915\n",
      "Epoch 10/100\n",
      "9376/9440 [============================>.] - ETA: 0s - loss: 0.3786 - categorical_accuracy: 0.8747\n",
      "Epoch 00010: val_loss improved from 0.33348 to 0.31532, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 206us/sample - loss: 0.3782 - categorical_accuracy: 0.8746 - val_loss: 0.3153 - val_categorical_accuracy: 0.8847\n",
      "Epoch 11/100\n",
      "9248/9440 [============================>.] - ETA: 0s - loss: 0.3625 - categorical_accuracy: 0.8801\n",
      "Epoch 00011: val_loss did not improve from 0.31532\n",
      "9440/9440 [==============================] - 2s 197us/sample - loss: 0.3615 - categorical_accuracy: 0.8804 - val_loss: 0.3202 - val_categorical_accuracy: 0.8907\n",
      "Epoch 12/100\n",
      "9312/9440 [============================>.] - ETA: 0s - loss: 0.3437 - categorical_accuracy: 0.8857\n",
      "Epoch 00012: val_loss improved from 0.31532 to 0.30655, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 176us/sample - loss: 0.3427 - categorical_accuracy: 0.8864 - val_loss: 0.3066 - val_categorical_accuracy: 0.8915\n",
      "Epoch 13/100\n",
      "9184/9440 [============================>.] - ETA: 0s - loss: 0.3252 - categorical_accuracy: 0.8944\n",
      "Epoch 00013: val_loss improved from 0.30655 to 0.30419, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 206us/sample - loss: 0.3246 - categorical_accuracy: 0.8946 - val_loss: 0.3042 - val_categorical_accuracy: 0.8941\n",
      "Epoch 14/100\n",
      "9280/9440 [============================>.] - ETA: 0s - loss: 0.3170 - categorical_accuracy: 0.8958\n",
      "Epoch 00014: val_loss improved from 0.30419 to 0.29632, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 215us/sample - loss: 0.3173 - categorical_accuracy: 0.8954 - val_loss: 0.2963 - val_categorical_accuracy: 0.9008\n",
      "Epoch 15/100\n",
      "9248/9440 [============================>.] - ETA: 0s - loss: 0.3064 - categorical_accuracy: 0.8982\n",
      "Epoch 00015: val_loss did not improve from 0.29632\n",
      "9440/9440 [==============================] - 2s 207us/sample - loss: 0.3055 - categorical_accuracy: 0.8985 - val_loss: 0.3063 - val_categorical_accuracy: 0.8992\n",
      "Epoch 16/100\n",
      "9376/9440 [============================>.] - ETA: 0s - loss: 0.2982 - categorical_accuracy: 0.9062\n",
      "Epoch 00016: val_loss did not improve from 0.29632\n",
      "9440/9440 [==============================] - 2s 206us/sample - loss: 0.2982 - categorical_accuracy: 0.9061 - val_loss: 0.3036 - val_categorical_accuracy: 0.8949\n",
      "Epoch 17/100\n",
      "9408/9440 [============================>.] - ETA: 0s - loss: 0.2919 - categorical_accuracy: 0.9035\n",
      "Epoch 00017: val_loss improved from 0.29632 to 0.28050, saving model to t_weights_0\n",
      "9440/9440 [==============================] - 2s 216us/sample - loss: 0.2923 - categorical_accuracy: 0.9034 - val_loss: 0.2805 - val_categorical_accuracy: 0.9042\n",
      "Epoch 18/100\n",
      "9184/9440 [============================>.] - ETA: 0s - loss: 0.2767 - categorical_accuracy: 0.9109\n",
      "Epoch 00018: val_loss did not improve from 0.28050\n",
      "9440/9440 [==============================] - 2s 211us/sample - loss: 0.2776 - categorical_accuracy: 0.9110 - val_loss: 0.2886 - val_categorical_accuracy: 0.9068\n",
      "Epoch 19/100\n",
      "9216/9440 [============================>.] - ETA: 0s - loss: 0.2689 - categorical_accuracy: 0.9109\n",
      "Epoch 00019: val_loss did not improve from 0.28050\n",
      "9440/9440 [==============================] - 2s 170us/sample - loss: 0.2697 - categorical_accuracy: 0.9107 - val_loss: 0.3065 - val_categorical_accuracy: 0.8958\n",
      "Epoch 20/100\n",
      "9344/9440 [============================>.] - ETA: 0s - loss: 0.2614 - categorical_accuracy: 0.9189\n",
      "Epoch 00020: val_loss did not improve from 0.28050\n",
      "9440/9440 [==============================] - 2s 195us/sample - loss: 0.2617 - categorical_accuracy: 0.9185 - val_loss: 0.2868 - val_categorical_accuracy: 0.9051\n",
      "Epoch 21/100\n",
      "9376/9440 [============================>.] - ETA: 0s - loss: 0.2624 - categorical_accuracy: 0.9189\n",
      "Epoch 00021: val_loss did not improve from 0.28050\n",
      "9440/9440 [==============================] - 2s 198us/sample - loss: 0.2616 - categorical_accuracy: 0.9191 - val_loss: 0.2986 - val_categorical_accuracy: 0.9051\n",
      "Epoch 22/100\n",
      "9184/9440 [============================>.] - ETA: 0s - loss: 0.2506 - categorical_accuracy: 0.9202\n",
      "Epoch 00022: val_loss did not improve from 0.28050\n",
      "9440/9440 [==============================] - 2s 213us/sample - loss: 0.2497 - categorical_accuracy: 0.9207 - val_loss: 0.2840 - val_categorical_accuracy: 0.9025\n",
      "0.8940678 0.5837\n",
      "\n",
      "\n",
      "nrx: 10 - real: 0 \n",
      "Train on 17166 samples, validate on 2145 samples\n",
      "Epoch 1/100\n",
      "17088/17166 [============================>.] - ETA: 0s - loss: 1.4893 - categorical_accuracy: 0.4150\n",
      "Epoch 00001: val_loss improved from inf to 0.81506, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 240us/sample - loss: 1.4869 - categorical_accuracy: 0.4158 - val_loss: 0.8151 - val_categorical_accuracy: 0.6862\n",
      "Epoch 2/100\n",
      "17152/17166 [============================>.] - ETA: 0s - loss: 0.8199 - categorical_accuracy: 0.6533\n",
      "Epoch 00002: val_loss improved from 0.81506 to 0.60583, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 216us/sample - loss: 0.8197 - categorical_accuracy: 0.6534 - val_loss: 0.6058 - val_categorical_accuracy: 0.7781\n",
      "Epoch 3/100\n",
      "16928/17166 [============================>.] - ETA: 0s - loss: 0.6364 - categorical_accuracy: 0.7538\n",
      "Epoch 00003: val_loss improved from 0.60583 to 0.46922, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 214us/sample - loss: 0.6350 - categorical_accuracy: 0.7549 - val_loss: 0.4692 - val_categorical_accuracy: 0.8289\n",
      "Epoch 4/100\n",
      "16960/17166 [============================>.] - ETA: 0s - loss: 0.5023 - categorical_accuracy: 0.8210\n",
      "Epoch 00004: val_loss improved from 0.46922 to 0.36184, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 217us/sample - loss: 0.5017 - categorical_accuracy: 0.8213 - val_loss: 0.3618 - val_categorical_accuracy: 0.8769\n",
      "Epoch 5/100\n",
      "16928/17166 [============================>.] - ETA: 0s - loss: 0.4158 - categorical_accuracy: 0.8546\n",
      "Epoch 00005: val_loss improved from 0.36184 to 0.30658, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 216us/sample - loss: 0.4161 - categorical_accuracy: 0.8545 - val_loss: 0.3066 - val_categorical_accuracy: 0.9068\n",
      "Epoch 6/100\n",
      "17088/17166 [============================>.] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8801\n",
      "Epoch 00006: val_loss improved from 0.30658 to 0.28113, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 215us/sample - loss: 0.3533 - categorical_accuracy: 0.8802 - val_loss: 0.2811 - val_categorical_accuracy: 0.9100\n",
      "Epoch 7/100\n",
      "17088/17166 [============================>.] - ETA: 0s - loss: 0.3166 - categorical_accuracy: 0.8969\n",
      "Epoch 00007: val_loss improved from 0.28113 to 0.24298, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 218us/sample - loss: 0.3167 - categorical_accuracy: 0.8970 - val_loss: 0.2430 - val_categorical_accuracy: 0.9249\n",
      "Epoch 8/100\n",
      "17024/17166 [============================>.] - ETA: 0s - loss: 0.2844 - categorical_accuracy: 0.9090\n",
      "Epoch 00008: val_loss improved from 0.24298 to 0.23658, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 216us/sample - loss: 0.2842 - categorical_accuracy: 0.9091 - val_loss: 0.2366 - val_categorical_accuracy: 0.9268\n",
      "Epoch 9/100\n",
      "16992/17166 [============================>.] - ETA: 0s - loss: 0.2708 - categorical_accuracy: 0.9138\n",
      "Epoch 00009: val_loss did not improve from 0.23658\n",
      "17166/17166 [==============================] - 4s 215us/sample - loss: 0.2706 - categorical_accuracy: 0.9138 - val_loss: 0.2380 - val_categorical_accuracy: 0.9254\n",
      "Epoch 10/100\n",
      "17120/17166 [============================>.] - ETA: 0s - loss: 0.2530 - categorical_accuracy: 0.9207\n",
      "Epoch 00010: val_loss did not improve from 0.23658\n",
      "17166/17166 [==============================] - 4s 209us/sample - loss: 0.2529 - categorical_accuracy: 0.9207 - val_loss: 0.2441 - val_categorical_accuracy: 0.9249\n",
      "Epoch 11/100\n",
      "16992/17166 [============================>.] - ETA: 0s - loss: 0.2459 - categorical_accuracy: 0.9205\n",
      "Epoch 00011: val_loss did not improve from 0.23658\n",
      "17166/17166 [==============================] - 4s 211us/sample - loss: 0.2455 - categorical_accuracy: 0.9204 - val_loss: 0.2367 - val_categorical_accuracy: 0.9273\n",
      "Epoch 12/100\n",
      "16960/17166 [============================>.] - ETA: 0s - loss: 0.2343 - categorical_accuracy: 0.9265\n",
      "Epoch 00012: val_loss improved from 0.23658 to 0.22365, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 217us/sample - loss: 0.2348 - categorical_accuracy: 0.9265 - val_loss: 0.2236 - val_categorical_accuracy: 0.9282\n",
      "Epoch 13/100\n",
      "17088/17166 [============================>.] - ETA: 0s - loss: 0.2277 - categorical_accuracy: 0.9274\n",
      "Epoch 00013: val_loss improved from 0.22365 to 0.21300, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 3s 198us/sample - loss: 0.2273 - categorical_accuracy: 0.9276 - val_loss: 0.2130 - val_categorical_accuracy: 0.9315\n",
      "Epoch 14/100\n",
      "17056/17166 [============================>.] - ETA: 0s - loss: 0.2266 - categorical_accuracy: 0.9278\n",
      "Epoch 00014: val_loss did not improve from 0.21300\n",
      "17166/17166 [==============================] - 4s 213us/sample - loss: 0.2262 - categorical_accuracy: 0.9279 - val_loss: 0.2135 - val_categorical_accuracy: 0.9319\n",
      "Epoch 15/100\n",
      "17120/17166 [============================>.] - ETA: 0s - loss: 0.2080 - categorical_accuracy: 0.9357\n",
      "Epoch 00015: val_loss did not improve from 0.21300\n",
      "17166/17166 [==============================] - 4s 209us/sample - loss: 0.2080 - categorical_accuracy: 0.9356 - val_loss: 0.2132 - val_categorical_accuracy: 0.9347\n",
      "Epoch 16/100\n",
      "16928/17166 [============================>.] - ETA: 0s - loss: 0.2035 - categorical_accuracy: 0.9365\n",
      "Epoch 00016: val_loss improved from 0.21300 to 0.20825, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 214us/sample - loss: 0.2029 - categorical_accuracy: 0.9368 - val_loss: 0.2082 - val_categorical_accuracy: 0.9338\n",
      "Epoch 17/100\n",
      "16928/17166 [============================>.] - ETA: 0s - loss: 0.2031 - categorical_accuracy: 0.9358\n",
      "Epoch 00017: val_loss improved from 0.20825 to 0.20611, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 214us/sample - loss: 0.2026 - categorical_accuracy: 0.9361 - val_loss: 0.2061 - val_categorical_accuracy: 0.9413\n",
      "Epoch 18/100\n",
      "17024/17166 [============================>.] - ETA: 0s - loss: 0.1900 - categorical_accuracy: 0.9394\n",
      "Epoch 00018: val_loss improved from 0.20611 to 0.19506, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 218us/sample - loss: 0.1903 - categorical_accuracy: 0.9394 - val_loss: 0.1951 - val_categorical_accuracy: 0.9394\n",
      "Epoch 19/100\n",
      "17120/17166 [============================>.] - ETA: 0s - loss: 0.2020 - categorical_accuracy: 0.9363\n",
      "Epoch 00019: val_loss did not improve from 0.19506\n",
      "17166/17166 [==============================] - 4s 211us/sample - loss: 0.2019 - categorical_accuracy: 0.9363 - val_loss: 0.2123 - val_categorical_accuracy: 0.9361\n",
      "Epoch 20/100\n",
      "16960/17166 [============================>.] - ETA: 0s - loss: 0.1858 - categorical_accuracy: 0.9428\n",
      "Epoch 00020: val_loss did not improve from 0.19506\n",
      "17166/17166 [==============================] - 4s 213us/sample - loss: 0.1856 - categorical_accuracy: 0.9429 - val_loss: 0.2101 - val_categorical_accuracy: 0.9385\n",
      "Epoch 21/100\n",
      "17024/17166 [============================>.] - ETA: 0s - loss: 0.1847 - categorical_accuracy: 0.9407\n",
      "Epoch 00021: val_loss improved from 0.19506 to 0.18772, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 218us/sample - loss: 0.1849 - categorical_accuracy: 0.9408 - val_loss: 0.1877 - val_categorical_accuracy: 0.9422\n",
      "Epoch 22/100\n",
      "17024/17166 [============================>.] - ETA: 0s - loss: 0.1792 - categorical_accuracy: 0.9455\n",
      "Epoch 00022: val_loss did not improve from 0.18772\n",
      "17166/17166 [==============================] - 4s 212us/sample - loss: 0.1796 - categorical_accuracy: 0.9453 - val_loss: 0.2077 - val_categorical_accuracy: 0.9375\n",
      "Epoch 23/100\n",
      "16960/17166 [============================>.] - ETA: 0s - loss: 0.1797 - categorical_accuracy: 0.9436\n",
      "Epoch 00023: val_loss did not improve from 0.18772\n",
      "17166/17166 [==============================] - 4s 213us/sample - loss: 0.1798 - categorical_accuracy: 0.9437 - val_loss: 0.1951 - val_categorical_accuracy: 0.9399\n",
      "Epoch 24/100\n",
      "16896/17166 [============================>.] - ETA: 0s - loss: 0.1768 - categorical_accuracy: 0.9466\n",
      "Epoch 00024: val_loss improved from 0.18772 to 0.18302, saving model to t_weights_0\n",
      "17166/17166 [==============================] - 4s 215us/sample - loss: 0.1766 - categorical_accuracy: 0.9466 - val_loss: 0.1830 - val_categorical_accuracy: 0.9464\n",
      "Epoch 25/100\n",
      "17056/17166 [============================>.] - ETA: 0s - loss: 0.1651 - categorical_accuracy: 0.9489\n",
      "Epoch 00025: val_loss did not improve from 0.18302\n",
      "17166/17166 [==============================] - 4s 212us/sample - loss: 0.1656 - categorical_accuracy: 0.9488 - val_loss: 0.2135 - val_categorical_accuracy: 0.9427\n",
      "Epoch 26/100\n",
      "16960/17166 [============================>.] - ETA: 0s - loss: 0.1704 - categorical_accuracy: 0.9463\n",
      "Epoch 00026: val_loss did not improve from 0.18302\n",
      "17166/17166 [==============================] - 4s 213us/sample - loss: 0.1706 - categorical_accuracy: 0.9464 - val_loss: 0.2185 - val_categorical_accuracy: 0.9389\n",
      "Epoch 27/100\n",
      "17024/17166 [============================>.] - ETA: 0s - loss: 0.1607 - categorical_accuracy: 0.9504\n",
      "Epoch 00027: val_loss did not improve from 0.18302\n",
      "17166/17166 [==============================] - 4s 215us/sample - loss: 0.1609 - categorical_accuracy: 0.9504 - val_loss: 0.2004 - val_categorical_accuracy: 0.9455\n",
      "Epoch 28/100\n",
      "17024/17166 [============================>.] - ETA: 0s - loss: 0.1649 - categorical_accuracy: 0.9512\n",
      "Epoch 00028: val_loss did not improve from 0.18302\n",
      "17166/17166 [==============================] - 4s 212us/sample - loss: 0.1647 - categorical_accuracy: 0.9514 - val_loss: 0.2063 - val_categorical_accuracy: 0.9450\n",
      "Epoch 29/100\n",
      "17120/17166 [============================>.] - ETA: 0s - loss: 0.1534 - categorical_accuracy: 0.9532\n",
      "Epoch 00029: val_loss did not improve from 0.18302\n",
      "17166/17166 [==============================] - 4s 212us/sample - loss: 0.1537 - categorical_accuracy: 0.9531 - val_loss: 0.2402 - val_categorical_accuracy: 0.9389\n",
      "0.93426573 0.767\n",
      "\n",
      "\n",
      "nrx: 15 - real: 0 \n",
      "Train on 24865 samples, validate on 3107 samples\n",
      "Epoch 1/100\n",
      "24832/24865 [============================>.] - ETA: 0s - loss: 1.2657 - categorical_accuracy: 0.4987\n",
      "Epoch 00001: val_loss improved from inf to 0.78266, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 6s 233us/sample - loss: 1.2652 - categorical_accuracy: 0.4989 - val_loss: 0.7827 - val_categorical_accuracy: 0.6714\n",
      "Epoch 2/100\n",
      "24704/24865 [============================>.] - ETA: 0s - loss: 0.6812 - categorical_accuracy: 0.7290\n",
      "Epoch 00002: val_loss improved from 0.78266 to 0.50917, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 212us/sample - loss: 0.6801 - categorical_accuracy: 0.7295 - val_loss: 0.5092 - val_categorical_accuracy: 0.8233\n",
      "Epoch 3/100\n",
      "24864/24865 [============================>.] - ETA: 0s - loss: 0.5113 - categorical_accuracy: 0.8103\n",
      "Epoch 00003: val_loss improved from 0.50917 to 0.40397, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 219us/sample - loss: 0.5113 - categorical_accuracy: 0.8103 - val_loss: 0.4040 - val_categorical_accuracy: 0.8677\n",
      "Epoch 4/100\n",
      "24832/24865 [============================>.] - ETA: 0s - loss: 0.4130 - categorical_accuracy: 0.8598\n",
      "Epoch 00004: val_loss improved from 0.40397 to 0.33989, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 220us/sample - loss: 0.4128 - categorical_accuracy: 0.8599 - val_loss: 0.3399 - val_categorical_accuracy: 0.8825\n",
      "Epoch 5/100\n",
      "24800/24865 [============================>.] - ETA: 0s - loss: 0.3358 - categorical_accuracy: 0.8919\n",
      "Epoch 00005: val_loss improved from 0.33989 to 0.28053, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 205us/sample - loss: 0.3356 - categorical_accuracy: 0.8920 - val_loss: 0.2805 - val_categorical_accuracy: 0.9057\n",
      "Epoch 6/100\n",
      "24768/24865 [============================>.] - ETA: 0s - loss: 0.2918 - categorical_accuracy: 0.9081\n",
      "Epoch 00006: val_loss improved from 0.28053 to 0.25762, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 218us/sample - loss: 0.2921 - categorical_accuracy: 0.9080 - val_loss: 0.2576 - val_categorical_accuracy: 0.9186\n",
      "Epoch 7/100\n",
      "24864/24865 [============================>.] - ETA: 0s - loss: 0.2665 - categorical_accuracy: 0.9159\n",
      "Epoch 00007: val_loss did not improve from 0.25762\n",
      "24865/24865 [==============================] - 5s 217us/sample - loss: 0.2665 - categorical_accuracy: 0.9159 - val_loss: 0.2610 - val_categorical_accuracy: 0.9170\n",
      "Epoch 8/100\n",
      "24768/24865 [============================>.] - ETA: 0s - loss: 0.2527 - categorical_accuracy: 0.9214\n",
      "Epoch 00008: val_loss improved from 0.25762 to 0.24281, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 218us/sample - loss: 0.2526 - categorical_accuracy: 0.9212 - val_loss: 0.2428 - val_categorical_accuracy: 0.9195\n",
      "Epoch 9/100\n",
      "24736/24865 [============================>.] - ETA: 0s - loss: 0.2407 - categorical_accuracy: 0.9237\n",
      "Epoch 00009: val_loss improved from 0.24281 to 0.23220, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 220us/sample - loss: 0.2405 - categorical_accuracy: 0.9238 - val_loss: 0.2322 - val_categorical_accuracy: 0.9240\n",
      "Epoch 10/100\n",
      "24864/24865 [============================>.] - ETA: 0s - loss: 0.2295 - categorical_accuracy: 0.9265\n",
      "Epoch 00010: val_loss improved from 0.23220 to 0.22533, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 221us/sample - loss: 0.2295 - categorical_accuracy: 0.9265 - val_loss: 0.2253 - val_categorical_accuracy: 0.9260\n",
      "Epoch 11/100\n",
      "24800/24865 [============================>.] - ETA: 0s - loss: 0.2187 - categorical_accuracy: 0.9308\n",
      "Epoch 00011: val_loss did not improve from 0.22533\n",
      "24865/24865 [==============================] - 5s 217us/sample - loss: 0.2186 - categorical_accuracy: 0.9308 - val_loss: 0.2457 - val_categorical_accuracy: 0.9211\n",
      "Epoch 12/100\n",
      "24736/24865 [============================>.] - ETA: 0s - loss: 0.2127 - categorical_accuracy: 0.9331\n",
      "Epoch 00012: val_loss improved from 0.22533 to 0.22124, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 217us/sample - loss: 0.2129 - categorical_accuracy: 0.9330 - val_loss: 0.2212 - val_categorical_accuracy: 0.9298\n",
      "Epoch 13/100\n",
      "24864/24865 [============================>.] - ETA: 0s - loss: 0.2022 - categorical_accuracy: 0.9379\n",
      "Epoch 00013: val_loss did not improve from 0.22124\n",
      "24865/24865 [==============================] - 5s 211us/sample - loss: 0.2022 - categorical_accuracy: 0.9379 - val_loss: 0.2252 - val_categorical_accuracy: 0.9314\n",
      "Epoch 14/100\n",
      "24800/24865 [============================>.] - ETA: 0s - loss: 0.1994 - categorical_accuracy: 0.9377\n",
      "Epoch 00014: val_loss improved from 0.22124 to 0.20600, saving model to t_weights_0\n",
      "24865/24865 [==============================] - 5s 220us/sample - loss: 0.1992 - categorical_accuracy: 0.9378 - val_loss: 0.2060 - val_categorical_accuracy: 0.9314\n",
      "Epoch 15/100\n",
      "24768/24865 [============================>.] - ETA: 0s - loss: 0.1994 - categorical_accuracy: 0.9384\n",
      "Epoch 00015: val_loss did not improve from 0.20600\n",
      "24865/24865 [==============================] - 5s 218us/sample - loss: 0.1994 - categorical_accuracy: 0.9384 - val_loss: 0.2172 - val_categorical_accuracy: 0.9331\n",
      "Epoch 16/100\n",
      "24640/24865 [============================>.] - ETA: 0s - loss: 0.1885 - categorical_accuracy: 0.9408\n",
      "Epoch 00016: val_loss did not improve from 0.20600\n",
      "24865/24865 [==============================] - 5s 218us/sample - loss: 0.1889 - categorical_accuracy: 0.9406 - val_loss: 0.2285 - val_categorical_accuracy: 0.9327\n",
      "Epoch 17/100\n",
      "24736/24865 [============================>.] - ETA: 0s - loss: 0.1829 - categorical_accuracy: 0.9427\n",
      "Epoch 00017: val_loss did not improve from 0.20600\n",
      "24865/24865 [==============================] - 5s 214us/sample - loss: 0.1832 - categorical_accuracy: 0.9426 - val_loss: 0.2116 - val_categorical_accuracy: 0.9334\n",
      "Epoch 18/100\n",
      "24672/24865 [============================>.] - ETA: 0s - loss: 0.1826 - categorical_accuracy: 0.9439\n",
      "Epoch 00018: val_loss did not improve from 0.20600\n",
      "24865/24865 [==============================] - 5s 215us/sample - loss: 0.1829 - categorical_accuracy: 0.9437 - val_loss: 0.2245 - val_categorical_accuracy: 0.9282\n",
      "Epoch 19/100\n",
      "24736/24865 [============================>.] - ETA: 0s - loss: 0.1765 - categorical_accuracy: 0.9461\n",
      "Epoch 00019: val_loss did not improve from 0.20600\n",
      "24865/24865 [==============================] - 5s 217us/sample - loss: 0.1762 - categorical_accuracy: 0.9462 - val_loss: 0.2208 - val_categorical_accuracy: 0.9308\n",
      "0.9349855 0.8163\n",
      "\n",
      "\n",
      "nrx: 20 - real: 0 \n",
      "Train on 32573 samples, validate on 4071 samples\n",
      "Epoch 1/100\n",
      "32448/32573 [============================>.] - ETA: 0s - loss: 1.2643 - categorical_accuracy: 0.4879\n",
      "Epoch 00001: val_loss improved from inf to 0.79917, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 8s 233us/sample - loss: 1.2626 - categorical_accuracy: 0.4885 - val_loss: 0.7992 - val_categorical_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "32352/32573 [============================>.] - ETA: 0s - loss: 0.7572 - categorical_accuracy: 0.6876\n",
      "Epoch 00002: val_loss improved from 0.79917 to 0.58964, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 220us/sample - loss: 0.7565 - categorical_accuracy: 0.6879 - val_loss: 0.5896 - val_categorical_accuracy: 0.7740\n",
      "Epoch 3/100\n",
      "32480/32573 [============================>.] - ETA: 0s - loss: 0.5600 - categorical_accuracy: 0.7885\n",
      "Epoch 00003: val_loss improved from 0.58964 to 0.42346, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 220us/sample - loss: 0.5601 - categorical_accuracy: 0.7886 - val_loss: 0.4235 - val_categorical_accuracy: 0.8543\n",
      "Epoch 4/100\n",
      "32320/32573 [============================>.] - ETA: 0s - loss: 0.4362 - categorical_accuracy: 0.8477\n",
      "Epoch 00004: val_loss improved from 0.42346 to 0.33782, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 215us/sample - loss: 0.4360 - categorical_accuracy: 0.8478 - val_loss: 0.3378 - val_categorical_accuracy: 0.8828\n",
      "Epoch 5/100\n",
      "32352/32573 [============================>.] - ETA: 0s - loss: 0.3626 - categorical_accuracy: 0.8797\n",
      "Epoch 00005: val_loss improved from 0.33782 to 0.32118, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 220us/sample - loss: 0.3621 - categorical_accuracy: 0.8800 - val_loss: 0.3212 - val_categorical_accuracy: 0.8978\n",
      "Epoch 6/100\n",
      "32416/32573 [============================>.] - ETA: 0s - loss: 0.3223 - categorical_accuracy: 0.8936\n",
      "Epoch 00006: val_loss improved from 0.32118 to 0.29476, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 219us/sample - loss: 0.3223 - categorical_accuracy: 0.8937 - val_loss: 0.2948 - val_categorical_accuracy: 0.8981\n",
      "Epoch 7/100\n",
      "32448/32573 [============================>.] - ETA: 0s - loss: 0.3010 - categorical_accuracy: 0.9017\n",
      "Epoch 00007: val_loss improved from 0.29476 to 0.26002, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 221us/sample - loss: 0.3012 - categorical_accuracy: 0.9015 - val_loss: 0.2600 - val_categorical_accuracy: 0.9150\n",
      "Epoch 8/100\n",
      "32320/32573 [============================>.] - ETA: 0s - loss: 0.2797 - categorical_accuracy: 0.9088\n",
      "Epoch 00008: val_loss did not improve from 0.26002\n",
      "32573/32573 [==============================] - 7s 220us/sample - loss: 0.2800 - categorical_accuracy: 0.9087 - val_loss: 0.2683 - val_categorical_accuracy: 0.9086\n",
      "Epoch 9/100\n",
      "32512/32573 [============================>.] - ETA: 0s - loss: 0.2683 - categorical_accuracy: 0.9138\n",
      "Epoch 00009: val_loss improved from 0.26002 to 0.24813, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 222us/sample - loss: 0.2685 - categorical_accuracy: 0.9138 - val_loss: 0.2481 - val_categorical_accuracy: 0.9175\n",
      "Epoch 10/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.2566 - categorical_accuracy: 0.9181\n",
      "Epoch 00010: val_loss did not improve from 0.24813\n",
      "32573/32573 [==============================] - 7s 219us/sample - loss: 0.2570 - categorical_accuracy: 0.9181 - val_loss: 0.2548 - val_categorical_accuracy: 0.9167\n",
      "Epoch 11/100\n",
      "32352/32573 [============================>.] - ETA: 0s - loss: 0.2477 - categorical_accuracy: 0.9208\n",
      "Epoch 00011: val_loss improved from 0.24813 to 0.23596, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 220us/sample - loss: 0.2476 - categorical_accuracy: 0.9209 - val_loss: 0.2360 - val_categorical_accuracy: 0.9241\n",
      "Epoch 12/100\n",
      "32480/32573 [============================>.] - ETA: 0s - loss: 0.2451 - categorical_accuracy: 0.9216\n",
      "Epoch 00012: val_loss improved from 0.23596 to 0.22256, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 215us/sample - loss: 0.2449 - categorical_accuracy: 0.9216 - val_loss: 0.2226 - val_categorical_accuracy: 0.9270\n",
      "Epoch 13/100\n",
      "32320/32573 [============================>.] - ETA: 0s - loss: 0.2334 - categorical_accuracy: 0.9256\n",
      "Epoch 00013: val_loss did not improve from 0.22256\n",
      "32573/32573 [==============================] - 7s 219us/sample - loss: 0.2331 - categorical_accuracy: 0.9257 - val_loss: 0.2256 - val_categorical_accuracy: 0.9270\n",
      "Epoch 14/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.2266 - categorical_accuracy: 0.9275\n",
      "Epoch 00014: val_loss improved from 0.22256 to 0.21944, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 216us/sample - loss: 0.2268 - categorical_accuracy: 0.9275 - val_loss: 0.2194 - val_categorical_accuracy: 0.9302\n",
      "Epoch 15/100\n",
      "32416/32573 [============================>.] - ETA: 0s - loss: 0.2254 - categorical_accuracy: 0.9287\n",
      "Epoch 00015: val_loss did not improve from 0.21944\n",
      "32573/32573 [==============================] - 7s 208us/sample - loss: 0.2252 - categorical_accuracy: 0.9288 - val_loss: 0.2272 - val_categorical_accuracy: 0.9295\n",
      "Epoch 16/100\n",
      "32544/32573 [============================>.] - ETA: 0s - loss: 0.2195 - categorical_accuracy: 0.9317\n",
      "Epoch 00016: val_loss did not improve from 0.21944\n",
      "32573/32573 [==============================] - 7s 217us/sample - loss: 0.2194 - categorical_accuracy: 0.9317 - val_loss: 0.2262 - val_categorical_accuracy: 0.9278\n",
      "Epoch 17/100\n",
      "32256/32573 [============================>.] - ETA: 0s - loss: 0.2151 - categorical_accuracy: 0.9322\n",
      "Epoch 00017: val_loss did not improve from 0.21944\n",
      "32573/32573 [==============================] - 7s 219us/sample - loss: 0.2144 - categorical_accuracy: 0.9325 - val_loss: 0.2197 - val_categorical_accuracy: 0.9268\n",
      "Epoch 18/100\n",
      "32448/32573 [============================>.] - ETA: 0s - loss: 0.2071 - categorical_accuracy: 0.9350\n",
      "Epoch 00018: val_loss did not improve from 0.21944\n",
      "32573/32573 [==============================] - 7s 217us/sample - loss: 0.2072 - categorical_accuracy: 0.9350 - val_loss: 0.2204 - val_categorical_accuracy: 0.9312\n",
      "Epoch 19/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.2055 - categorical_accuracy: 0.9361\n",
      "Epoch 00019: val_loss improved from 0.21944 to 0.21271, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 220us/sample - loss: 0.2051 - categorical_accuracy: 0.9361 - val_loss: 0.2127 - val_categorical_accuracy: 0.9339\n",
      "Epoch 20/100\n",
      "32448/32573 [============================>.] - ETA: 0s - loss: 0.1990 - categorical_accuracy: 0.9385\n",
      "Epoch 00020: val_loss did not improve from 0.21271\n",
      "32573/32573 [==============================] - 7s 218us/sample - loss: 0.1990 - categorical_accuracy: 0.9384 - val_loss: 0.2158 - val_categorical_accuracy: 0.9339\n",
      "Epoch 21/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.1961 - categorical_accuracy: 0.9387\n",
      "Epoch 00021: val_loss improved from 0.21271 to 0.20902, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 214us/sample - loss: 0.1961 - categorical_accuracy: 0.9386 - val_loss: 0.2090 - val_categorical_accuracy: 0.9339\n",
      "Epoch 22/100\n",
      "32512/32573 [============================>.] - ETA: 0s - loss: 0.1927 - categorical_accuracy: 0.9397\n",
      "Epoch 00022: val_loss did not improve from 0.20902\n",
      "32573/32573 [==============================] - 7s 217us/sample - loss: 0.1927 - categorical_accuracy: 0.9397 - val_loss: 0.2151 - val_categorical_accuracy: 0.9337\n",
      "Epoch 23/100\n",
      "32544/32573 [============================>.] - ETA: 0s - loss: 0.1878 - categorical_accuracy: 0.9412\n",
      "Epoch 00023: val_loss did not improve from 0.20902\n",
      "32573/32573 [==============================] - 7s 218us/sample - loss: 0.1878 - categorical_accuracy: 0.9412 - val_loss: 0.2333 - val_categorical_accuracy: 0.9307\n",
      "Epoch 24/100\n",
      "32352/32573 [============================>.] - ETA: 0s - loss: 0.1867 - categorical_accuracy: 0.9428\n",
      "Epoch 00024: val_loss improved from 0.20902 to 0.20344, saving model to t_weights_0\n",
      "32573/32573 [==============================] - 7s 221us/sample - loss: 0.1867 - categorical_accuracy: 0.9428 - val_loss: 0.2034 - val_categorical_accuracy: 0.9376\n",
      "Epoch 25/100\n",
      "32448/32573 [============================>.] - ETA: 0s - loss: 0.1814 - categorical_accuracy: 0.9439\n",
      "Epoch 00025: val_loss did not improve from 0.20344\n",
      "32573/32573 [==============================] - 7s 219us/sample - loss: 0.1810 - categorical_accuracy: 0.9440 - val_loss: 0.2130 - val_categorical_accuracy: 0.9354\n",
      "Epoch 26/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.1795 - categorical_accuracy: 0.9444\n",
      "Epoch 00026: val_loss did not improve from 0.20344\n",
      "32573/32573 [==============================] - 7s 218us/sample - loss: 0.1795 - categorical_accuracy: 0.9444 - val_loss: 0.2239 - val_categorical_accuracy: 0.9344\n",
      "Epoch 27/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.1774 - categorical_accuracy: 0.9471\n",
      "Epoch 00027: val_loss did not improve from 0.20344\n",
      "32573/32573 [==============================] - 7s 219us/sample - loss: 0.1775 - categorical_accuracy: 0.9471 - val_loss: 0.2199 - val_categorical_accuracy: 0.9349\n",
      "Epoch 28/100\n",
      "32384/32573 [============================>.] - ETA: 0s - loss: 0.1706 - categorical_accuracy: 0.9479\n",
      "Epoch 00028: val_loss did not improve from 0.20344\n",
      "32573/32573 [==============================] - 7s 218us/sample - loss: 0.1703 - categorical_accuracy: 0.9480 - val_loss: 0.2182 - val_categorical_accuracy: 0.9342\n",
      "Epoch 29/100\n",
      "32416/32573 [============================>.] - ETA: 0s - loss: 0.1687 - categorical_accuracy: 0.9482\n",
      "Epoch 00029: val_loss did not improve from 0.20344\n",
      "32573/32573 [==============================] - 7s 214us/sample - loss: 0.1689 - categorical_accuracy: 0.9482 - val_loss: 0.2121 - val_categorical_accuracy: 0.9374\n",
      "0.9390813 0.8418\n",
      "\n",
      "\n",
      "nrx: 25 - real: 0 \n",
      "Train on 39923 samples, validate on 4990 samples\n",
      "Epoch 1/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 1.1676 - categorical_accuracy: 0.5176\n",
      "Epoch 00001: val_loss improved from inf to 0.68589, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 231us/sample - loss: 1.1668 - categorical_accuracy: 0.5178 - val_loss: 0.6859 - val_categorical_accuracy: 0.7008\n",
      "Epoch 2/100\n",
      "39744/39923 [============================>.] - ETA: 0s - loss: 0.6784 - categorical_accuracy: 0.7221\n",
      "Epoch 00002: val_loss improved from 0.68589 to 0.48305, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 215us/sample - loss: 0.6777 - categorical_accuracy: 0.7224 - val_loss: 0.4830 - val_categorical_accuracy: 0.8192\n",
      "Epoch 3/100\n",
      "39680/39923 [============================>.] - ETA: 0s - loss: 0.4856 - categorical_accuracy: 0.8226\n",
      "Epoch 00003: val_loss improved from 0.48305 to 0.36749, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 216us/sample - loss: 0.4852 - categorical_accuracy: 0.8228 - val_loss: 0.3675 - val_categorical_accuracy: 0.8699\n",
      "Epoch 4/100\n",
      "39648/39923 [============================>.] - ETA: 0s - loss: 0.3890 - categorical_accuracy: 0.8662\n",
      "Epoch 00004: val_loss improved from 0.36749 to 0.29612, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 216us/sample - loss: 0.3892 - categorical_accuracy: 0.8661 - val_loss: 0.2961 - val_categorical_accuracy: 0.8960\n",
      "Epoch 5/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.3350 - categorical_accuracy: 0.8886\n",
      "Epoch 00005: val_loss improved from 0.29612 to 0.26882, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 217us/sample - loss: 0.3351 - categorical_accuracy: 0.8886 - val_loss: 0.2688 - val_categorical_accuracy: 0.9136\n",
      "Epoch 6/100\n",
      "39680/39923 [============================>.] - ETA: 0s - loss: 0.2971 - categorical_accuracy: 0.9037\n",
      "Epoch 00006: val_loss improved from 0.26882 to 0.26189, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 215us/sample - loss: 0.2969 - categorical_accuracy: 0.9037 - val_loss: 0.2619 - val_categorical_accuracy: 0.9132\n",
      "Epoch 7/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.2745 - categorical_accuracy: 0.9114\n",
      "Epoch 00007: val_loss improved from 0.26189 to 0.23654, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 8s 212us/sample - loss: 0.2746 - categorical_accuracy: 0.9114 - val_loss: 0.2365 - val_categorical_accuracy: 0.9216\n",
      "Epoch 8/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.2585 - categorical_accuracy: 0.9175\n",
      "Epoch 00008: val_loss improved from 0.23654 to 0.22317, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 214us/sample - loss: 0.2584 - categorical_accuracy: 0.9176 - val_loss: 0.2232 - val_categorical_accuracy: 0.9309\n",
      "Epoch 9/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.2446 - categorical_accuracy: 0.9232\n",
      "Epoch 00009: val_loss did not improve from 0.22317\n",
      "39923/39923 [==============================] - 9s 215us/sample - loss: 0.2446 - categorical_accuracy: 0.9233 - val_loss: 0.2248 - val_categorical_accuracy: 0.9259\n",
      "Epoch 10/100\n",
      "39712/39923 [============================>.] - ETA: 0s - loss: 0.2358 - categorical_accuracy: 0.9249\n",
      "Epoch 00010: val_loss did not improve from 0.22317\n",
      "39923/39923 [==============================] - 8s 213us/sample - loss: 0.2355 - categorical_accuracy: 0.9249 - val_loss: 0.2358 - val_categorical_accuracy: 0.9242\n",
      "Epoch 11/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.2239 - categorical_accuracy: 0.9293\n",
      "Epoch 00011: val_loss improved from 0.22317 to 0.21209, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 220us/sample - loss: 0.2238 - categorical_accuracy: 0.9293 - val_loss: 0.2121 - val_categorical_accuracy: 0.9325\n",
      "Epoch 12/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.2170 - categorical_accuracy: 0.9315\n",
      "Epoch 00012: val_loss improved from 0.21209 to 0.20891, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.2170 - categorical_accuracy: 0.9314 - val_loss: 0.2089 - val_categorical_accuracy: 0.9387\n",
      "Epoch 13/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.2100 - categorical_accuracy: 0.9332\n",
      "Epoch 00013: val_loss did not improve from 0.20891\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.2099 - categorical_accuracy: 0.9333 - val_loss: 0.2118 - val_categorical_accuracy: 0.9373\n",
      "Epoch 14/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.2052 - categorical_accuracy: 0.9352\n",
      "Epoch 00014: val_loss improved from 0.20891 to 0.19428, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 218us/sample - loss: 0.2051 - categorical_accuracy: 0.9352 - val_loss: 0.1943 - val_categorical_accuracy: 0.9413\n",
      "Epoch 15/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.1998 - categorical_accuracy: 0.9376\n",
      "Epoch 00015: val_loss did not improve from 0.19428\n",
      "39923/39923 [==============================] - 8s 211us/sample - loss: 0.1997 - categorical_accuracy: 0.9376 - val_loss: 0.2031 - val_categorical_accuracy: 0.9373\n",
      "Epoch 16/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.1937 - categorical_accuracy: 0.9397\n",
      "Epoch 00016: val_loss improved from 0.19428 to 0.19291, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1937 - categorical_accuracy: 0.9397 - val_loss: 0.1929 - val_categorical_accuracy: 0.9421\n",
      "Epoch 17/100\n",
      "39776/39923 [============================>.] - ETA: 0s - loss: 0.1915 - categorical_accuracy: 0.9407\n",
      "Epoch 00017: val_loss improved from 0.19291 to 0.19253, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 223us/sample - loss: 0.1921 - categorical_accuracy: 0.9405 - val_loss: 0.1925 - val_categorical_accuracy: 0.9417\n",
      "Epoch 18/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.1864 - categorical_accuracy: 0.9429\n",
      "Epoch 00018: val_loss did not improve from 0.19253\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.1865 - categorical_accuracy: 0.9429 - val_loss: 0.2037 - val_categorical_accuracy: 0.9387\n",
      "Epoch 19/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.1854 - categorical_accuracy: 0.9438\n",
      "Epoch 00019: val_loss improved from 0.19253 to 0.19123, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 220us/sample - loss: 0.1853 - categorical_accuracy: 0.9438 - val_loss: 0.1912 - val_categorical_accuracy: 0.9429\n",
      "Epoch 20/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.1801 - categorical_accuracy: 0.9457\n",
      "Epoch 00020: val_loss did not improve from 0.19123\n",
      "39923/39923 [==============================] - 9s 219us/sample - loss: 0.1800 - categorical_accuracy: 0.9457 - val_loss: 0.1974 - val_categorical_accuracy: 0.9439\n",
      "Epoch 21/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.1771 - categorical_accuracy: 0.9461\n",
      "Epoch 00021: val_loss did not improve from 0.19123\n",
      "39923/39923 [==============================] - 9s 217us/sample - loss: 0.1770 - categorical_accuracy: 0.9461 - val_loss: 0.1967 - val_categorical_accuracy: 0.9445\n",
      "Epoch 22/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.1747 - categorical_accuracy: 0.9466\n",
      "Epoch 00022: val_loss did not improve from 0.19123\n",
      "39923/39923 [==============================] - 9s 219us/sample - loss: 0.1746 - categorical_accuracy: 0.9467 - val_loss: 0.1991 - val_categorical_accuracy: 0.9423\n",
      "Epoch 23/100\n",
      "39680/39923 [============================>.] - ETA: 0s - loss: 0.1678 - categorical_accuracy: 0.9484\n",
      "Epoch 00023: val_loss did not improve from 0.19123\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.1680 - categorical_accuracy: 0.9483 - val_loss: 0.2081 - val_categorical_accuracy: 0.9399\n",
      "Epoch 24/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.1651 - categorical_accuracy: 0.9496\n",
      "Epoch 00024: val_loss improved from 0.19123 to 0.18928, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.1652 - categorical_accuracy: 0.9496 - val_loss: 0.1893 - val_categorical_accuracy: 0.9447\n",
      "Epoch 25/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.1649 - categorical_accuracy: 0.9494\n",
      "Epoch 00025: val_loss did not improve from 0.18928\n",
      "39923/39923 [==============================] - 9s 220us/sample - loss: 0.1648 - categorical_accuracy: 0.9494 - val_loss: 0.1946 - val_categorical_accuracy: 0.9467\n",
      "Epoch 26/100\n",
      "39776/39923 [============================>.] - ETA: 0s - loss: 0.1619 - categorical_accuracy: 0.9506\n",
      "Epoch 00026: val_loss did not improve from 0.18928\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.1621 - categorical_accuracy: 0.9505 - val_loss: 0.2145 - val_categorical_accuracy: 0.9411\n",
      "Epoch 27/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.1584 - categorical_accuracy: 0.9521\n",
      "Epoch 00027: val_loss did not improve from 0.18928\n",
      "39923/39923 [==============================] - 9s 220us/sample - loss: 0.1585 - categorical_accuracy: 0.9521 - val_loss: 0.2026 - val_categorical_accuracy: 0.9397\n",
      "Epoch 28/100\n",
      "39712/39923 [============================>.] - ETA: 0s - loss: 0.1552 - categorical_accuracy: 0.9538\n",
      "Epoch 00028: val_loss did not improve from 0.18928\n",
      "39923/39923 [==============================] - 9s 215us/sample - loss: 0.1549 - categorical_accuracy: 0.9540 - val_loss: 0.2153 - val_categorical_accuracy: 0.9407\n",
      "Epoch 29/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.1529 - categorical_accuracy: 0.9531\n",
      "Epoch 00029: val_loss did not improve from 0.18928\n",
      "39923/39923 [==============================] - 9s 220us/sample - loss: 0.1533 - categorical_accuracy: 0.9530 - val_loss: 0.1955 - val_categorical_accuracy: 0.9479\n",
      "0.9486974 0.8252\n",
      "\n",
      "\n",
      "nrx: 0 - real: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/Documents/txid_framework/dataset_scripts/working/data_utilities.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cls_weights = np.max(stat,axis=0)/stat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 2.2664 - categorical_accuracy: 0.1890\n",
      "Epoch 00001: val_loss improved from inf to 2.06093, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 1s 514us/sample - loss: 2.2499 - categorical_accuracy: 0.2050 - val_loss: 2.0609 - val_categorical_accuracy: 0.4850\n",
      "Epoch 2/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 1.7373 - categorical_accuracy: 0.3914\n",
      "Epoch 00002: val_loss improved from 2.06093 to 1.06915, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 264us/sample - loss: 1.6636 - categorical_accuracy: 0.4112 - val_loss: 1.0691 - val_categorical_accuracy: 0.7700\n",
      "Epoch 3/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 1.0713 - categorical_accuracy: 0.5989\n",
      "Epoch 00003: val_loss improved from 1.06915 to 0.73259, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 264us/sample - loss: 1.0688 - categorical_accuracy: 0.5994 - val_loss: 0.7326 - val_categorical_accuracy: 0.7450\n",
      "Epoch 4/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.7943 - categorical_accuracy: 0.6936\n",
      "Epoch 00004: val_loss improved from 0.73259 to 0.50972, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 243us/sample - loss: 0.7840 - categorical_accuracy: 0.6975 - val_loss: 0.5097 - val_categorical_accuracy: 0.8950\n",
      "Epoch 5/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.5788 - categorical_accuracy: 0.7704\n",
      "Epoch 00005: val_loss improved from 0.50972 to 0.36205, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 277us/sample - loss: 0.5769 - categorical_accuracy: 0.7706 - val_loss: 0.3621 - val_categorical_accuracy: 0.9700\n",
      "Epoch 6/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.4619 - categorical_accuracy: 0.8342\n",
      "Epoch 00006: val_loss improved from 0.36205 to 0.28176, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 278us/sample - loss: 0.4575 - categorical_accuracy: 0.8363 - val_loss: 0.2818 - val_categorical_accuracy: 0.9400\n",
      "Epoch 7/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.3490 - categorical_accuracy: 0.8939\n",
      "Epoch 00007: val_loss improved from 0.28176 to 0.18321, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 266us/sample - loss: 0.3521 - categorical_accuracy: 0.8931 - val_loss: 0.1832 - val_categorical_accuracy: 0.9800\n",
      "Epoch 8/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.2719 - categorical_accuracy: 0.9238\n",
      "Epoch 00008: val_loss improved from 0.18321 to 0.11851, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 265us/sample - loss: 0.2668 - categorical_accuracy: 0.9262 - val_loss: 0.1185 - val_categorical_accuracy: 0.9850\n",
      "Epoch 9/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.2006 - categorical_accuracy: 0.9411\n",
      "Epoch 00009: val_loss improved from 0.11851 to 0.08507, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 250us/sample - loss: 0.1997 - categorical_accuracy: 0.9431 - val_loss: 0.0851 - val_categorical_accuracy: 0.9850\n",
      "Epoch 10/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.1619 - categorical_accuracy: 0.9620\n",
      "Epoch 00010: val_loss improved from 0.08507 to 0.06045, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.1562 - categorical_accuracy: 0.9650 - val_loss: 0.0605 - val_categorical_accuracy: 0.9900\n",
      "Epoch 11/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.1432 - categorical_accuracy: 0.9656\n",
      "Epoch 00011: val_loss improved from 0.06045 to 0.05722, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 269us/sample - loss: 0.1435 - categorical_accuracy: 0.9656 - val_loss: 0.0572 - val_categorical_accuracy: 0.9900\n",
      "Epoch 12/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.1205 - categorical_accuracy: 0.9681\n",
      "Epoch 00012: val_loss improved from 0.05722 to 0.05455, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 271us/sample - loss: 0.1206 - categorical_accuracy: 0.9681 - val_loss: 0.0546 - val_categorical_accuracy: 0.9900\n",
      "Epoch 13/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.1088 - categorical_accuracy: 0.9745\n",
      "Epoch 00013: val_loss improved from 0.05455 to 0.04386, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 273us/sample - loss: 0.1072 - categorical_accuracy: 0.9750 - val_loss: 0.0439 - val_categorical_accuracy: 0.9950\n",
      "Epoch 14/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0992 - categorical_accuracy: 0.9785\n",
      "Epoch 00014: val_loss did not improve from 0.04386\n",
      "1600/1600 [==============================] - 0s 234us/sample - loss: 0.1019 - categorical_accuracy: 0.9756 - val_loss: 0.0767 - val_categorical_accuracy: 0.9750\n",
      "Epoch 15/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0986 - categorical_accuracy: 0.9805\n",
      "Epoch 00015: val_loss improved from 0.04386 to 0.02926, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 272us/sample - loss: 0.0973 - categorical_accuracy: 0.9806 - val_loss: 0.0293 - val_categorical_accuracy: 0.9950\n",
      "Epoch 16/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.0736 - categorical_accuracy: 0.9905\n",
      "Epoch 00016: val_loss improved from 0.02926 to 0.02762, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 238us/sample - loss: 0.0728 - categorical_accuracy: 0.9900 - val_loss: 0.0276 - val_categorical_accuracy: 0.9950\n",
      "Epoch 17/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0649 - categorical_accuracy: 0.9866\n",
      "Epoch 00017: val_loss improved from 0.02762 to 0.02505, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 272us/sample - loss: 0.0650 - categorical_accuracy: 0.9869 - val_loss: 0.0250 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0597 - categorical_accuracy: 0.9927\n",
      "Epoch 00018: val_loss improved from 0.02505 to 0.02095, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 280us/sample - loss: 0.0601 - categorical_accuracy: 0.9919 - val_loss: 0.0209 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0499 - categorical_accuracy: 0.9900\n",
      "Epoch 00019: val_loss improved from 0.02095 to 0.01982, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 279us/sample - loss: 0.0501 - categorical_accuracy: 0.9906 - val_loss: 0.0198 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0519 - categorical_accuracy: 0.9889\n",
      "Epoch 00020: val_loss did not improve from 0.01982\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0526 - categorical_accuracy: 0.9887 - val_loss: 0.0203 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0591 - categorical_accuracy: 0.9896\n",
      "Epoch 00021: val_loss did not improve from 0.01982\n",
      "1600/1600 [==============================] - 0s 220us/sample - loss: 0.0578 - categorical_accuracy: 0.9900 - val_loss: 0.0209 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0442 - categorical_accuracy: 0.9937\n",
      "Epoch 00022: val_loss improved from 0.01982 to 0.01860, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 238us/sample - loss: 0.0443 - categorical_accuracy: 0.9931 - val_loss: 0.0186 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0429 - categorical_accuracy: 0.9935\n",
      "Epoch 00023: val_loss did not improve from 0.01860\n",
      "1600/1600 [==============================] - 0s 230us/sample - loss: 0.0429 - categorical_accuracy: 0.9937 - val_loss: 0.0192 - val_categorical_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0390 - categorical_accuracy: 0.9941\n",
      "Epoch 00024: val_loss did not improve from 0.01860\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0394 - categorical_accuracy: 0.9944 - val_loss: 0.0216 - val_categorical_accuracy: 0.9950\n",
      "Epoch 25/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0387 - categorical_accuracy: 0.9962\n",
      "Epoch 00025: val_loss did not improve from 0.01860\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0384 - categorical_accuracy: 0.9962 - val_loss: 0.0190 - val_categorical_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0356 - categorical_accuracy: 0.9953\n",
      "Epoch 00026: val_loss improved from 0.01860 to 0.01785, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 279us/sample - loss: 0.0351 - categorical_accuracy: 0.9956 - val_loss: 0.0178 - val_categorical_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0387 - categorical_accuracy: 0.9933\n",
      "Epoch 00027: val_loss improved from 0.01785 to 0.01762, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 261us/sample - loss: 0.0364 - categorical_accuracy: 0.9944 - val_loss: 0.0176 - val_categorical_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.0394 - categorical_accuracy: 0.9959\n",
      "Epoch 00028: val_loss did not improve from 0.01762\n",
      "1600/1600 [==============================] - 0s 204us/sample - loss: 0.0384 - categorical_accuracy: 0.9962 - val_loss: 0.0189 - val_categorical_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0389 - categorical_accuracy: 0.9926\n",
      "Epoch 00029: val_loss did not improve from 0.01762\n",
      "1600/1600 [==============================] - 0s 229us/sample - loss: 0.0393 - categorical_accuracy: 0.9931 - val_loss: 0.0178 - val_categorical_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0325 - categorical_accuracy: 0.9963\n",
      "Epoch 00030: val_loss did not improve from 0.01762\n",
      "1600/1600 [==============================] - 0s 231us/sample - loss: 0.0312 - categorical_accuracy: 0.9969 - val_loss: 0.0185 - val_categorical_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0331 - categorical_accuracy: 0.9954\n",
      "Epoch 00031: val_loss improved from 0.01762 to 0.01757, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 275us/sample - loss: 0.0334 - categorical_accuracy: 0.9950 - val_loss: 0.0176 - val_categorical_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0321 - categorical_accuracy: 0.9947\n",
      "Epoch 00032: val_loss did not improve from 0.01757\n",
      "1600/1600 [==============================] - 0s 232us/sample - loss: 0.0321 - categorical_accuracy: 0.9950 - val_loss: 0.0180 - val_categorical_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0323 - categorical_accuracy: 0.9968\n",
      "Epoch 00033: val_loss improved from 0.01757 to 0.01745, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 272us/sample - loss: 0.0322 - categorical_accuracy: 0.9969 - val_loss: 0.0175 - val_categorical_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0279 - categorical_accuracy: 0.9979\n",
      "Epoch 00034: val_loss improved from 0.01745 to 0.01743, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 243us/sample - loss: 0.0271 - categorical_accuracy: 0.9981 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0252 - categorical_accuracy: 0.9981\n",
      "Epoch 00035: val_loss improved from 0.01743 to 0.01742, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 267us/sample - loss: 0.0250 - categorical_accuracy: 0.9981 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0274 - categorical_accuracy: 0.9967\n",
      "Epoch 00036: val_loss improved from 0.01742 to 0.01741, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 276us/sample - loss: 0.0272 - categorical_accuracy: 0.9969 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0291 - categorical_accuracy: 0.9974\n",
      "Epoch 00037: val_loss improved from 0.01741 to 0.01738, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 269us/sample - loss: 0.0290 - categorical_accuracy: 0.9975 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0262 - categorical_accuracy: 0.9980\n",
      "Epoch 00038: val_loss did not improve from 0.01738\n",
      "1600/1600 [==============================] - 0s 237us/sample - loss: 0.0262 - categorical_accuracy: 0.9981 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0248 - categorical_accuracy: 0.9980\n",
      "Epoch 00039: val_loss did not improve from 0.01738\n",
      "1600/1600 [==============================] - 0s 233us/sample - loss: 0.0246 - categorical_accuracy: 0.9981 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0261 - categorical_accuracy: 0.9958\n",
      "Epoch 00040: val_loss did not improve from 0.01738\n",
      "1600/1600 [==============================] - 0s 203us/sample - loss: 0.0260 - categorical_accuracy: 0.9962 - val_loss: 0.0176 - val_categorical_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0262 - categorical_accuracy: 0.9985\n",
      "Epoch 00041: val_loss improved from 0.01738 to 0.01730, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 263us/sample - loss: 0.0264 - categorical_accuracy: 0.9981 - val_loss: 0.0173 - val_categorical_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0288 - categorical_accuracy: 0.9949\n",
      "Epoch 00042: val_loss did not improve from 0.01730\n",
      "1600/1600 [==============================] - 0s 232us/sample - loss: 0.0290 - categorical_accuracy: 0.9950 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0222 - categorical_accuracy: 0.9987\n",
      "Epoch 00043: val_loss did not improve from 0.01730\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0222 - categorical_accuracy: 0.9987 - val_loss: 0.0173 - val_categorical_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0229 - categorical_accuracy: 0.9980\n",
      "Epoch 00044: val_loss did not improve from 0.01730\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0231 - categorical_accuracy: 0.9981 - val_loss: 0.0206 - val_categorical_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0265 - categorical_accuracy: 0.9968\n",
      "Epoch 00045: val_loss improved from 0.01730 to 0.01729, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 279us/sample - loss: 0.0263 - categorical_accuracy: 0.9969 - val_loss: 0.0173 - val_categorical_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0228 - categorical_accuracy: 0.9979\n",
      "Epoch 00046: val_loss did not improve from 0.01729\n",
      "1600/1600 [==============================] - 0s 207us/sample - loss: 0.0234 - categorical_accuracy: 0.9975 - val_loss: 0.0197 - val_categorical_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.0212 - categorical_accuracy: 1.0000\n",
      "Epoch 00047: val_loss improved from 0.01729 to 0.01723, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 243us/sample - loss: 0.0212 - categorical_accuracy: 1.0000 - val_loss: 0.0172 - val_categorical_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0209 - categorical_accuracy: 1.0000\n",
      "Epoch 00048: val_loss improved from 0.01723 to 0.01721, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 269us/sample - loss: 0.0208 - categorical_accuracy: 1.0000 - val_loss: 0.0172 - val_categorical_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0243 - categorical_accuracy: 0.9987\n",
      "Epoch 00049: val_loss did not improve from 0.01721\n",
      "1600/1600 [==============================] - 0s 234us/sample - loss: 0.0248 - categorical_accuracy: 0.9981 - val_loss: 0.0172 - val_categorical_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0239 - categorical_accuracy: 0.9981\n",
      "Epoch 00050: val_loss did not improve from 0.01721\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0238 - categorical_accuracy: 0.9981 - val_loss: 0.0172 - val_categorical_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0232 - categorical_accuracy: 0.9980\n",
      "Epoch 00051: val_loss improved from 0.01721 to 0.01719, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 280us/sample - loss: 0.0231 - categorical_accuracy: 0.9981 - val_loss: 0.0172 - val_categorical_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0242 - categorical_accuracy: 0.9981\n",
      "Epoch 00052: val_loss did not improve from 0.01719\n",
      "1600/1600 [==============================] - 0s 193us/sample - loss: 0.0241 - categorical_accuracy: 0.9981 - val_loss: 0.0229 - val_categorical_accuracy: 0.9950\n",
      "Epoch 53/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0243 - categorical_accuracy: 0.9980\n",
      "Epoch 00053: val_loss did not improve from 0.01719\n",
      "1600/1600 [==============================] - 0s 195us/sample - loss: 0.0245 - categorical_accuracy: 0.9981 - val_loss: 0.0172 - val_categorical_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0208 - categorical_accuracy: 0.9993\n",
      "Epoch 00054: val_loss improved from 0.01719 to 0.01712, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 284us/sample - loss: 0.0217 - categorical_accuracy: 0.9987 - val_loss: 0.0171 - val_categorical_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0206 - categorical_accuracy: 0.9987\n",
      "Epoch 00055: val_loss improved from 0.01712 to 0.01710, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 280us/sample - loss: 0.0207 - categorical_accuracy: 0.9987 - val_loss: 0.0171 - val_categorical_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0207 - categorical_accuracy: 1.0000\n",
      "Epoch 00056: val_loss improved from 0.01710 to 0.01708, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 277us/sample - loss: 0.0211 - categorical_accuracy: 1.0000 - val_loss: 0.0171 - val_categorical_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0196 - categorical_accuracy: 1.0000\n",
      "Epoch 00057: val_loss improved from 0.01708 to 0.01708, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 283us/sample - loss: 0.0196 - categorical_accuracy: 1.0000 - val_loss: 0.0171 - val_categorical_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1312/1600 [=======================>......] - ETA: 0s - loss: 0.0222 - categorical_accuracy: 0.9977\n",
      "Epoch 00058: val_loss improved from 0.01708 to 0.01703, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 254us/sample - loss: 0.0215 - categorical_accuracy: 0.9981 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0199 - categorical_accuracy: 0.9993\n",
      "Epoch 00059: val_loss improved from 0.01703 to 0.01700, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 260us/sample - loss: 0.0200 - categorical_accuracy: 0.9994 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0211 - categorical_accuracy: 0.9973\n",
      "Epoch 00060: val_loss improved from 0.01700 to 0.01698, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 279us/sample - loss: 0.0210 - categorical_accuracy: 0.9975 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0193 - categorical_accuracy: 1.0000\n",
      "Epoch 00061: val_loss improved from 0.01698 to 0.01694, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 275us/sample - loss: 0.0193 - categorical_accuracy: 1.0000 - val_loss: 0.0169 - val_categorical_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0313 - categorical_accuracy: 0.9970\n",
      "Epoch 00062: val_loss did not improve from 0.01694\n",
      "1600/1600 [==============================] - 0s 227us/sample - loss: 0.0306 - categorical_accuracy: 0.9975 - val_loss: 0.0171 - val_categorical_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0207 - categorical_accuracy: 0.9993\n",
      "Epoch 00063: val_loss did not improve from 0.01694\n",
      "1600/1600 [==============================] - 0s 234us/sample - loss: 0.0213 - categorical_accuracy: 0.9987 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0256 - categorical_accuracy: 0.9979\n",
      "Epoch 00064: val_loss did not improve from 0.01694\n",
      "1600/1600 [==============================] - 0s 207us/sample - loss: 0.0254 - categorical_accuracy: 0.9981 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0203 - categorical_accuracy: 0.9993\n",
      "Epoch 00065: val_loss improved from 0.01694 to 0.01694, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 252us/sample - loss: 0.0218 - categorical_accuracy: 0.9987 - val_loss: 0.0169 - val_categorical_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0221 - categorical_accuracy: 0.9980\n",
      "Epoch 00066: val_loss did not improve from 0.01694\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0220 - categorical_accuracy: 0.9981 - val_loss: 0.0177 - val_categorical_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0210 - categorical_accuracy: 1.0000\n",
      "Epoch 00067: val_loss improved from 0.01694 to 0.01689, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 290us/sample - loss: 0.0213 - categorical_accuracy: 1.0000 - val_loss: 0.0169 - val_categorical_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0185 - categorical_accuracy: 1.0000\n",
      "Epoch 00068: val_loss improved from 0.01689 to 0.01685, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 284us/sample - loss: 0.0190 - categorical_accuracy: 1.0000 - val_loss: 0.0168 - val_categorical_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0194 - categorical_accuracy: 0.9993\n",
      "Epoch 00069: val_loss improved from 0.01685 to 0.01683, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 282us/sample - loss: 0.0194 - categorical_accuracy: 0.9994 - val_loss: 0.0168 - val_categorical_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0187 - categorical_accuracy: 1.0000\n",
      "Epoch 00070: val_loss improved from 0.01683 to 0.01678, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 245us/sample - loss: 0.0186 - categorical_accuracy: 1.0000 - val_loss: 0.0168 - val_categorical_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 0.0191 - categorical_accuracy: 1.0000\n",
      "Epoch 00071: val_loss improved from 0.01678 to 0.01673, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 256us/sample - loss: 0.0193 - categorical_accuracy: 1.0000 - val_loss: 0.0167 - val_categorical_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0178 - categorical_accuracy: 1.0000\n",
      "Epoch 00072: val_loss improved from 0.01673 to 0.01669, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 274us/sample - loss: 0.0178 - categorical_accuracy: 1.0000 - val_loss: 0.0167 - val_categorical_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0220 - categorical_accuracy: 0.9987\n",
      "Epoch 00073: val_loss improved from 0.01669 to 0.01664, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 271us/sample - loss: 0.0240 - categorical_accuracy: 0.9975 - val_loss: 0.0166 - val_categorical_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0227 - categorical_accuracy: 0.9979\n",
      "Epoch 00074: val_loss did not improve from 0.01664\n",
      "1600/1600 [==============================] - 0s 221us/sample - loss: 0.0221 - categorical_accuracy: 0.9981 - val_loss: 0.0168 - val_categorical_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0425 - categorical_accuracy: 0.9922\n",
      "Epoch 00075: val_loss did not improve from 0.01664\n",
      "1600/1600 [==============================] - 0s 237us/sample - loss: 0.0415 - categorical_accuracy: 0.9925 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.0301 - categorical_accuracy: 0.9964\n",
      "Epoch 00076: val_loss did not improve from 0.01664\n",
      "1600/1600 [==============================] - 0s 214us/sample - loss: 0.0285 - categorical_accuracy: 0.9969 - val_loss: 0.0167 - val_categorical_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.0193 - categorical_accuracy: 0.9993\n",
      "Epoch 00077: val_loss did not improve from 0.01664\n",
      "1600/1600 [==============================] - 0s 209us/sample - loss: 0.0199 - categorical_accuracy: 0.9987 - val_loss: 0.0167 - val_categorical_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0193 - categorical_accuracy: 0.9993\n",
      "Epoch 00078: val_loss did not improve from 0.01664\n",
      "1600/1600 [==============================] - 0s 237us/sample - loss: 0.0192 - categorical_accuracy: 0.9994 - val_loss: 0.0167 - val_categorical_accuracy: 1.0000\n",
      "1.0 0.3539\n",
      "\n",
      "\n",
      "nrx: 5 - real: 1 \n",
      "Train on 9009 samples, validate on 1125 samples\n",
      "Epoch 1/100\n",
      "8960/9009 [============================>.] - ETA: 0s - loss: 1.6362 - categorical_accuracy: 0.3883\n",
      "Epoch 00001: val_loss improved from inf to 0.88399, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 274us/sample - loss: 1.6337 - categorical_accuracy: 0.3893 - val_loss: 0.8840 - val_categorical_accuracy: 0.6764\n",
      "Epoch 2/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.8113 - categorical_accuracy: 0.6790\n",
      "Epoch 00002: val_loss improved from 0.88399 to 0.56511, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 229us/sample - loss: 0.8104 - categorical_accuracy: 0.6794 - val_loss: 0.5651 - val_categorical_accuracy: 0.7698\n",
      "Epoch 3/100\n",
      "8864/9009 [============================>.] - ETA: 0s - loss: 0.5195 - categorical_accuracy: 0.8017\n",
      "Epoch 00003: val_loss improved from 0.56511 to 0.38535, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 228us/sample - loss: 0.5186 - categorical_accuracy: 0.8028 - val_loss: 0.3853 - val_categorical_accuracy: 0.8542\n",
      "Epoch 4/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.3476 - categorical_accuracy: 0.8832\n",
      "Epoch 00004: val_loss improved from 0.38535 to 0.24309, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 231us/sample - loss: 0.3472 - categorical_accuracy: 0.8837 - val_loss: 0.2431 - val_categorical_accuracy: 0.9333\n",
      "Epoch 5/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.2445 - categorical_accuracy: 0.9328\n",
      "Epoch 00005: val_loss improved from 0.24309 to 0.18485, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 230us/sample - loss: 0.2442 - categorical_accuracy: 0.9327 - val_loss: 0.1848 - val_categorical_accuracy: 0.9502\n",
      "Epoch 6/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.1875 - categorical_accuracy: 0.9512\n",
      "Epoch 00006: val_loss did not improve from 0.18485\n",
      "9009/9009 [==============================] - 2s 198us/sample - loss: 0.1883 - categorical_accuracy: 0.9508 - val_loss: 0.1897 - val_categorical_accuracy: 0.9422\n",
      "Epoch 7/100\n",
      "8992/9009 [============================>.] - ETA: 0s - loss: 0.1575 - categorical_accuracy: 0.9616\n",
      "Epoch 00007: val_loss improved from 0.18485 to 0.13537, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 236us/sample - loss: 0.1574 - categorical_accuracy: 0.9617 - val_loss: 0.1354 - val_categorical_accuracy: 0.9636\n",
      "Epoch 8/100\n",
      "8864/9009 [============================>.] - ETA: 0s - loss: 0.1312 - categorical_accuracy: 0.9689\n",
      "Epoch 00008: val_loss improved from 0.13537 to 0.13038, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 233us/sample - loss: 0.1306 - categorical_accuracy: 0.9689 - val_loss: 0.1304 - val_categorical_accuracy: 0.9707\n",
      "Epoch 9/100\n",
      "8864/9009 [============================>.] - ETA: 0s - loss: 0.1137 - categorical_accuracy: 0.9728\n",
      "Epoch 00009: val_loss improved from 0.13038 to 0.10389, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 231us/sample - loss: 0.1135 - categorical_accuracy: 0.9728 - val_loss: 0.1039 - val_categorical_accuracy: 0.9751\n",
      "Epoch 10/100\n",
      "8832/9009 [============================>.] - ETA: 0s - loss: 0.1120 - categorical_accuracy: 0.9742\n",
      "Epoch 00010: val_loss did not improve from 0.10389\n",
      "9009/9009 [==============================] - 2s 222us/sample - loss: 0.1115 - categorical_accuracy: 0.9742 - val_loss: 0.1215 - val_categorical_accuracy: 0.9724\n",
      "Epoch 11/100\n",
      "8960/9009 [============================>.] - ETA: 0s - loss: 0.0977 - categorical_accuracy: 0.9779\n",
      "Epoch 00011: val_loss did not improve from 0.10389\n",
      "9009/9009 [==============================] - 2s 223us/sample - loss: 0.0977 - categorical_accuracy: 0.9779 - val_loss: 0.2172 - val_categorical_accuracy: 0.9244\n",
      "Epoch 12/100\n",
      "8832/9009 [============================>.] - ETA: 0s - loss: 0.0816 - categorical_accuracy: 0.9826\n",
      "Epoch 00012: val_loss did not improve from 0.10389\n",
      "9009/9009 [==============================] - 2s 223us/sample - loss: 0.0822 - categorical_accuracy: 0.9824 - val_loss: 0.1103 - val_categorical_accuracy: 0.9778\n",
      "Epoch 13/100\n",
      "8928/9009 [============================>.] - ETA: 0s - loss: 0.0808 - categorical_accuracy: 0.9830\n",
      "Epoch 00013: val_loss did not improve from 0.10389\n",
      "9009/9009 [==============================] - 2s 230us/sample - loss: 0.0810 - categorical_accuracy: 0.9828 - val_loss: 0.1187 - val_categorical_accuracy: 0.9707\n",
      "Epoch 14/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.0762 - categorical_accuracy: 0.9845\n",
      "Epoch 00014: val_loss improved from 0.10389 to 0.09983, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 232us/sample - loss: 0.0759 - categorical_accuracy: 0.9845 - val_loss: 0.0998 - val_categorical_accuracy: 0.9760\n",
      "Epoch 15/100\n",
      "8768/9009 [============================>.] - ETA: 0s - loss: 0.0720 - categorical_accuracy: 0.9857\n",
      "Epoch 00015: val_loss did not improve from 0.09983\n",
      "9009/9009 [==============================] - 2s 222us/sample - loss: 0.0712 - categorical_accuracy: 0.9860 - val_loss: 0.0999 - val_categorical_accuracy: 0.9822\n",
      "Epoch 16/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.0670 - categorical_accuracy: 0.9862\n",
      "Epoch 00016: val_loss did not improve from 0.09983\n",
      "9009/9009 [==============================] - 2s 222us/sample - loss: 0.0669 - categorical_accuracy: 0.9861 - val_loss: 0.1014 - val_categorical_accuracy: 0.9787\n",
      "Epoch 17/100\n",
      "8960/9009 [============================>.] - ETA: 0s - loss: 0.0657 - categorical_accuracy: 0.9867\n",
      "Epoch 00017: val_loss improved from 0.09983 to 0.09208, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 228us/sample - loss: 0.0655 - categorical_accuracy: 0.9868 - val_loss: 0.0921 - val_categorical_accuracy: 0.9822\n",
      "Epoch 18/100\n",
      "8800/9009 [============================>.] - ETA: 0s - loss: 0.0608 - categorical_accuracy: 0.9884\n",
      "Epoch 00018: val_loss did not improve from 0.09208\n",
      "9009/9009 [==============================] - 2s 222us/sample - loss: 0.0615 - categorical_accuracy: 0.9880 - val_loss: 0.0955 - val_categorical_accuracy: 0.9822\n",
      "Epoch 19/100\n",
      "8960/9009 [============================>.] - ETA: 0s - loss: 0.0584 - categorical_accuracy: 0.9898\n",
      "Epoch 00019: val_loss improved from 0.09208 to 0.08865, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 235us/sample - loss: 0.0583 - categorical_accuracy: 0.9899 - val_loss: 0.0887 - val_categorical_accuracy: 0.9796\n",
      "Epoch 20/100\n",
      "8896/9009 [============================>.] - ETA: 0s - loss: 0.0593 - categorical_accuracy: 0.9890\n",
      "Epoch 00020: val_loss did not improve from 0.08865\n",
      "9009/9009 [==============================] - 2s 221us/sample - loss: 0.0589 - categorical_accuracy: 0.9891 - val_loss: 0.0916 - val_categorical_accuracy: 0.9858\n",
      "Epoch 21/100\n",
      "8864/9009 [============================>.] - ETA: 0s - loss: 0.0591 - categorical_accuracy: 0.9876\n",
      "Epoch 00021: val_loss did not improve from 0.08865\n",
      "9009/9009 [==============================] - 2s 223us/sample - loss: 0.0588 - categorical_accuracy: 0.9878 - val_loss: 0.1046 - val_categorical_accuracy: 0.9813\n",
      "Epoch 22/100\n",
      "8800/9009 [============================>.] - ETA: 0s - loss: 0.0448 - categorical_accuracy: 0.9930\n",
      "Epoch 00022: val_loss improved from 0.08865 to 0.08764, saving model to t_weights_0\n",
      "9009/9009 [==============================] - 2s 230us/sample - loss: 0.0452 - categorical_accuracy: 0.9928 - val_loss: 0.0876 - val_categorical_accuracy: 0.9804\n",
      "Epoch 23/100\n",
      "8960/9009 [============================>.] - ETA: 0s - loss: 0.0506 - categorical_accuracy: 0.9923\n",
      "Epoch 00023: val_loss did not improve from 0.08764\n",
      "9009/9009 [==============================] - 2s 220us/sample - loss: 0.0505 - categorical_accuracy: 0.9923 - val_loss: 0.1061 - val_categorical_accuracy: 0.9813\n",
      "Epoch 24/100\n",
      "8992/9009 [============================>.] - ETA: 0s - loss: 0.0470 - categorical_accuracy: 0.9921\n",
      "Epoch 00024: val_loss did not improve from 0.08764\n",
      "9009/9009 [==============================] - 2s 226us/sample - loss: 0.0470 - categorical_accuracy: 0.9921 - val_loss: 0.0977 - val_categorical_accuracy: 0.9822\n",
      "Epoch 25/100\n",
      "8928/9009 [============================>.] - ETA: 0s - loss: 0.0489 - categorical_accuracy: 0.9918\n",
      "Epoch 00025: val_loss did not improve from 0.08764\n",
      "9009/9009 [==============================] - 2s 228us/sample - loss: 0.0487 - categorical_accuracy: 0.9919 - val_loss: 0.0925 - val_categorical_accuracy: 0.9867\n",
      "Epoch 26/100\n",
      "8864/9009 [============================>.] - ETA: 0s - loss: 0.0500 - categorical_accuracy: 0.9921\n",
      "Epoch 00026: val_loss did not improve from 0.08764\n",
      "9009/9009 [==============================] - 2s 223us/sample - loss: 0.0507 - categorical_accuracy: 0.9919 - val_loss: 0.1007 - val_categorical_accuracy: 0.9822\n",
      "Epoch 27/100\n",
      "8992/9009 [============================>.] - ETA: 0s - loss: 0.0435 - categorical_accuracy: 0.9935\n",
      "Epoch 00027: val_loss did not improve from 0.08764\n",
      "9009/9009 [==============================] - 2s 223us/sample - loss: 0.0435 - categorical_accuracy: 0.9936 - val_loss: 0.0924 - val_categorical_accuracy: 0.9840\n",
      "0.9813333 0.6334\n",
      "\n",
      "\n",
      "nrx: 10 - real: 1 \n",
      "Train on 16377 samples, validate on 2046 samples\n",
      "Epoch 1/100\n",
      "16128/16377 [============================>.] - ETA: 0s - loss: 1.2832 - categorical_accuracy: 0.4854\n",
      "Epoch 00001: val_loss improved from inf to 0.63253, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 251us/sample - loss: 1.2753 - categorical_accuracy: 0.4880 - val_loss: 0.6325 - val_categorical_accuracy: 0.7302\n",
      "Epoch 2/100\n",
      "16288/16377 [============================>.] - ETA: 0s - loss: 0.6047 - categorical_accuracy: 0.7471\n",
      "Epoch 00002: val_loss improved from 0.63253 to 0.42589, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 225us/sample - loss: 0.6040 - categorical_accuracy: 0.7476 - val_loss: 0.4259 - val_categorical_accuracy: 0.8480\n",
      "Epoch 3/100\n",
      "16352/16377 [============================>.] - ETA: 0s - loss: 0.3933 - categorical_accuracy: 0.8550\n",
      "Epoch 00003: val_loss improved from 0.42589 to 0.25367, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 226us/sample - loss: 0.3934 - categorical_accuracy: 0.8550 - val_loss: 0.2537 - val_categorical_accuracy: 0.9242\n",
      "Epoch 4/100\n",
      "16128/16377 [============================>.] - ETA: 0s - loss: 0.2821 - categorical_accuracy: 0.9071\n",
      "Epoch 00004: val_loss improved from 0.25367 to 0.17151, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 219us/sample - loss: 0.2812 - categorical_accuracy: 0.9076 - val_loss: 0.1715 - val_categorical_accuracy: 0.9550\n",
      "Epoch 5/100\n",
      "16288/16377 [============================>.] - ETA: 0s - loss: 0.2042 - categorical_accuracy: 0.9408\n",
      "Epoch 00005: val_loss improved from 0.17151 to 0.13941, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 218us/sample - loss: 0.2046 - categorical_accuracy: 0.9406 - val_loss: 0.1394 - val_categorical_accuracy: 0.9638\n",
      "Epoch 6/100\n",
      "16160/16377 [============================>.] - ETA: 0s - loss: 0.1687 - categorical_accuracy: 0.9522\n",
      "Epoch 00006: val_loss did not improve from 0.13941\n",
      "16377/16377 [==============================] - 4s 220us/sample - loss: 0.1685 - categorical_accuracy: 0.9523 - val_loss: 0.1463 - val_categorical_accuracy: 0.9516\n",
      "Epoch 7/100\n",
      "16128/16377 [============================>.] - ETA: 0s - loss: 0.1442 - categorical_accuracy: 0.9623\n",
      "Epoch 00007: val_loss improved from 0.13941 to 0.11249, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 3s 208us/sample - loss: 0.1439 - categorical_accuracy: 0.9627 - val_loss: 0.1125 - val_categorical_accuracy: 0.9663\n",
      "Epoch 8/100\n",
      "16224/16377 [============================>.] - ETA: 0s - loss: 0.1259 - categorical_accuracy: 0.9670\n",
      "Epoch 00008: val_loss improved from 0.11249 to 0.09344, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 221us/sample - loss: 0.1255 - categorical_accuracy: 0.9670 - val_loss: 0.0934 - val_categorical_accuracy: 0.9726\n",
      "Epoch 9/100\n",
      "16224/16377 [============================>.] - ETA: 0s - loss: 0.1140 - categorical_accuracy: 0.9700\n",
      "Epoch 00009: val_loss improved from 0.09344 to 0.09301, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 224us/sample - loss: 0.1143 - categorical_accuracy: 0.9700 - val_loss: 0.0930 - val_categorical_accuracy: 0.9741\n",
      "Epoch 10/100\n",
      "16032/16377 [============================>.] - ETA: 0s - loss: 0.0968 - categorical_accuracy: 0.9753\n",
      "Epoch 00010: val_loss did not improve from 0.09301\n",
      "16377/16377 [==============================] - 3s 200us/sample - loss: 0.0969 - categorical_accuracy: 0.9753 - val_loss: 0.1000 - val_categorical_accuracy: 0.9741\n",
      "Epoch 11/100\n",
      "16352/16377 [============================>.] - ETA: 0s - loss: 0.0890 - categorical_accuracy: 0.9791\n",
      "Epoch 00011: val_loss improved from 0.09301 to 0.08904, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 3s 213us/sample - loss: 0.0891 - categorical_accuracy: 0.9791 - val_loss: 0.0890 - val_categorical_accuracy: 0.9800\n",
      "Epoch 12/100\n",
      "16128/16377 [============================>.] - ETA: 0s - loss: 0.0866 - categorical_accuracy: 0.9793\n",
      "Epoch 00012: val_loss improved from 0.08904 to 0.08214, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 225us/sample - loss: 0.0860 - categorical_accuracy: 0.9795 - val_loss: 0.0821 - val_categorical_accuracy: 0.9790\n",
      "Epoch 13/100\n",
      "16256/16377 [============================>.] - ETA: 0s - loss: 0.0791 - categorical_accuracy: 0.9815\n",
      "Epoch 00013: val_loss did not improve from 0.08214\n",
      "16377/16377 [==============================] - 4s 220us/sample - loss: 0.0790 - categorical_accuracy: 0.9815 - val_loss: 0.0963 - val_categorical_accuracy: 0.9804\n",
      "Epoch 14/100\n",
      "16256/16377 [============================>.] - ETA: 0s - loss: 0.0760 - categorical_accuracy: 0.9828\n",
      "Epoch 00014: val_loss improved from 0.08214 to 0.07445, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 227us/sample - loss: 0.0757 - categorical_accuracy: 0.9830 - val_loss: 0.0744 - val_categorical_accuracy: 0.9839\n",
      "Epoch 15/100\n",
      "16352/16377 [============================>.] - ETA: 0s - loss: 0.0695 - categorical_accuracy: 0.9850\n",
      "Epoch 00015: val_loss improved from 0.07445 to 0.07171, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 227us/sample - loss: 0.0694 - categorical_accuracy: 0.9850 - val_loss: 0.0717 - val_categorical_accuracy: 0.9868\n",
      "Epoch 16/100\n",
      "16192/16377 [============================>.] - ETA: 0s - loss: 0.0662 - categorical_accuracy: 0.9865\n",
      "Epoch 00016: val_loss did not improve from 0.07171\n",
      "16377/16377 [==============================] - 4s 224us/sample - loss: 0.0663 - categorical_accuracy: 0.9864 - val_loss: 0.0973 - val_categorical_accuracy: 0.9746\n",
      "Epoch 17/100\n",
      "16224/16377 [============================>.] - ETA: 0s - loss: 0.0661 - categorical_accuracy: 0.9866\n",
      "Epoch 00017: val_loss did not improve from 0.07171\n",
      "16377/16377 [==============================] - 4s 224us/sample - loss: 0.0661 - categorical_accuracy: 0.9866 - val_loss: 0.0746 - val_categorical_accuracy: 0.9848\n",
      "Epoch 18/100\n",
      "16224/16377 [============================>.] - ETA: 0s - loss: 0.0623 - categorical_accuracy: 0.9872\n",
      "Epoch 00018: val_loss improved from 0.07171 to 0.06677, saving model to t_weights_0\n",
      "16377/16377 [==============================] - 4s 227us/sample - loss: 0.0622 - categorical_accuracy: 0.9874 - val_loss: 0.0668 - val_categorical_accuracy: 0.9868\n",
      "Epoch 19/100\n",
      "16192/16377 [============================>.] - ETA: 0s - loss: 0.0569 - categorical_accuracy: 0.9898\n",
      "Epoch 00019: val_loss did not improve from 0.06677\n",
      "16377/16377 [==============================] - 4s 222us/sample - loss: 0.0567 - categorical_accuracy: 0.9898 - val_loss: 0.0720 - val_categorical_accuracy: 0.9888\n",
      "Epoch 20/100\n",
      "16256/16377 [============================>.] - ETA: 0s - loss: 0.0543 - categorical_accuracy: 0.9891\n",
      "Epoch 00020: val_loss did not improve from 0.06677\n",
      "16377/16377 [==============================] - 4s 223us/sample - loss: 0.0542 - categorical_accuracy: 0.9891 - val_loss: 0.0705 - val_categorical_accuracy: 0.9844\n",
      "Epoch 21/100\n",
      "16128/16377 [============================>.] - ETA: 0s - loss: 0.0549 - categorical_accuracy: 0.9891\n",
      "Epoch 00021: val_loss did not improve from 0.06677\n",
      "16377/16377 [==============================] - 3s 213us/sample - loss: 0.0550 - categorical_accuracy: 0.9891 - val_loss: 0.0863 - val_categorical_accuracy: 0.9824\n",
      "Epoch 22/100\n",
      "16192/16377 [============================>.] - ETA: 0s - loss: 0.0513 - categorical_accuracy: 0.9913\n",
      "Epoch 00022: val_loss did not improve from 0.06677\n",
      "16377/16377 [==============================] - 4s 224us/sample - loss: 0.0513 - categorical_accuracy: 0.9913 - val_loss: 0.0764 - val_categorical_accuracy: 0.9858\n",
      "Epoch 23/100\n",
      "16224/16377 [============================>.] - ETA: 0s - loss: 0.0460 - categorical_accuracy: 0.9931\n",
      "Epoch 00023: val_loss did not improve from 0.06677\n",
      "16377/16377 [==============================] - 4s 221us/sample - loss: 0.0460 - categorical_accuracy: 0.9931 - val_loss: 0.0695 - val_categorical_accuracy: 0.9878\n",
      "0.98435974 0.6731\n",
      "\n",
      "\n",
      "nrx: 15 - real: 1 \n",
      "Train on 24377 samples, validate on 3046 samples\n",
      "Epoch 1/100\n",
      "24224/24377 [============================>.] - ETA: 0s - loss: 1.1619 - categorical_accuracy: 0.5177\n",
      "Epoch 00001: val_loss improved from inf to 0.67190, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 6s 242us/sample - loss: 1.1592 - categorical_accuracy: 0.5185 - val_loss: 0.6719 - val_categorical_accuracy: 0.7127\n",
      "Epoch 2/100\n",
      "24256/24377 [============================>.] - ETA: 0s - loss: 0.6069 - categorical_accuracy: 0.7458\n",
      "Epoch 00002: val_loss improved from 0.67190 to 0.40793, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 225us/sample - loss: 0.6063 - categorical_accuracy: 0.7461 - val_loss: 0.4079 - val_categorical_accuracy: 0.8726\n",
      "Epoch 3/100\n",
      "24352/24377 [============================>.] - ETA: 0s - loss: 0.3832 - categorical_accuracy: 0.8710\n",
      "Epoch 00003: val_loss improved from 0.40793 to 0.23092, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 223us/sample - loss: 0.3833 - categorical_accuracy: 0.8710 - val_loss: 0.2309 - val_categorical_accuracy: 0.9360\n",
      "Epoch 4/100\n",
      "24320/24377 [============================>.] - ETA: 0s - loss: 0.2599 - categorical_accuracy: 0.9219\n",
      "Epoch 00004: val_loss improved from 0.23092 to 0.19473, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 211us/sample - loss: 0.2600 - categorical_accuracy: 0.9218 - val_loss: 0.1947 - val_categorical_accuracy: 0.9435\n",
      "Epoch 5/100\n",
      "24288/24377 [============================>.] - ETA: 0s - loss: 0.2040 - categorical_accuracy: 0.9419\n",
      "Epoch 00005: val_loss improved from 0.19473 to 0.16300, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 224us/sample - loss: 0.2040 - categorical_accuracy: 0.9418 - val_loss: 0.1630 - val_categorical_accuracy: 0.9547\n",
      "Epoch 6/100\n",
      "24288/24377 [============================>.] - ETA: 0s - loss: 0.1737 - categorical_accuracy: 0.9517\n",
      "Epoch 00006: val_loss improved from 0.16300 to 0.16287, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 225us/sample - loss: 0.1736 - categorical_accuracy: 0.9517 - val_loss: 0.1629 - val_categorical_accuracy: 0.9567\n",
      "Epoch 7/100\n",
      "24160/24377 [============================>.] - ETA: 0s - loss: 0.1536 - categorical_accuracy: 0.9581\n",
      "Epoch 00007: val_loss improved from 0.16287 to 0.13918, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 224us/sample - loss: 0.1540 - categorical_accuracy: 0.9578 - val_loss: 0.1392 - val_categorical_accuracy: 0.9665\n",
      "Epoch 8/100\n",
      "24224/24377 [============================>.] - ETA: 0s - loss: 0.1391 - categorical_accuracy: 0.9629\n",
      "Epoch 00008: val_loss improved from 0.13918 to 0.13177, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 222us/sample - loss: 0.1396 - categorical_accuracy: 0.9628 - val_loss: 0.1318 - val_categorical_accuracy: 0.9685\n",
      "Epoch 9/100\n",
      "24288/24377 [============================>.] - ETA: 0s - loss: 0.1269 - categorical_accuracy: 0.9670\n",
      "Epoch 00009: val_loss improved from 0.13177 to 0.12148, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 6s 226us/sample - loss: 0.1268 - categorical_accuracy: 0.9670 - val_loss: 0.1215 - val_categorical_accuracy: 0.9714\n",
      "Epoch 10/100\n",
      "24128/24377 [============================>.] - ETA: 0s - loss: 0.1248 - categorical_accuracy: 0.9668\n",
      "Epoch 00010: val_loss did not improve from 0.12148\n",
      "24377/24377 [==============================] - 5s 216us/sample - loss: 0.1245 - categorical_accuracy: 0.9669 - val_loss: 0.1235 - val_categorical_accuracy: 0.9701\n",
      "Epoch 11/100\n",
      "24320/24377 [============================>.] - ETA: 0s - loss: 0.1122 - categorical_accuracy: 0.9717\n",
      "Epoch 00011: val_loss improved from 0.12148 to 0.11861, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 226us/sample - loss: 0.1128 - categorical_accuracy: 0.9716 - val_loss: 0.1186 - val_categorical_accuracy: 0.9724\n",
      "Epoch 12/100\n",
      "24192/24377 [============================>.] - ETA: 0s - loss: 0.1045 - categorical_accuracy: 0.9741\n",
      "Epoch 00012: val_loss improved from 0.11861 to 0.10575, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 223us/sample - loss: 0.1043 - categorical_accuracy: 0.9742 - val_loss: 0.1057 - val_categorical_accuracy: 0.9750\n",
      "Epoch 13/100\n",
      "24288/24377 [============================>.] - ETA: 0s - loss: 0.0996 - categorical_accuracy: 0.9748\n",
      "Epoch 00013: val_loss did not improve from 0.10575\n",
      "24377/24377 [==============================] - 5s 222us/sample - loss: 0.0995 - categorical_accuracy: 0.9749 - val_loss: 0.1148 - val_categorical_accuracy: 0.9728\n",
      "Epoch 14/100\n",
      "24192/24377 [============================>.] - ETA: 0s - loss: 0.0938 - categorical_accuracy: 0.9781\n",
      "Epoch 00014: val_loss improved from 0.10575 to 0.10202, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 225us/sample - loss: 0.0937 - categorical_accuracy: 0.9781 - val_loss: 0.1020 - val_categorical_accuracy: 0.9796\n",
      "Epoch 15/100\n",
      "24192/24377 [============================>.] - ETA: 0s - loss: 0.0911 - categorical_accuracy: 0.9785\n",
      "Epoch 00015: val_loss did not improve from 0.10202\n",
      "24377/24377 [==============================] - 5s 223us/sample - loss: 0.0911 - categorical_accuracy: 0.9784 - val_loss: 0.1198 - val_categorical_accuracy: 0.9767\n",
      "Epoch 16/100\n",
      "24288/24377 [============================>.] - ETA: 0s - loss: 0.0867 - categorical_accuracy: 0.9792\n",
      "Epoch 00016: val_loss did not improve from 0.10202\n",
      "24377/24377 [==============================] - 5s 221us/sample - loss: 0.0869 - categorical_accuracy: 0.9792 - val_loss: 0.1055 - val_categorical_accuracy: 0.9783\n",
      "Epoch 17/100\n",
      "24256/24377 [============================>.] - ETA: 0s - loss: 0.0851 - categorical_accuracy: 0.9797\n",
      "Epoch 00017: val_loss did not improve from 0.10202\n",
      "24377/24377 [==============================] - 5s 220us/sample - loss: 0.0850 - categorical_accuracy: 0.9798 - val_loss: 0.1048 - val_categorical_accuracy: 0.9813\n",
      "Epoch 18/100\n",
      "24224/24377 [============================>.] - ETA: 0s - loss: 0.0766 - categorical_accuracy: 0.9830\n",
      "Epoch 00018: val_loss improved from 0.10202 to 0.09776, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 225us/sample - loss: 0.0768 - categorical_accuracy: 0.9830 - val_loss: 0.0978 - val_categorical_accuracy: 0.9839\n",
      "Epoch 19/100\n",
      "24224/24377 [============================>.] - ETA: 0s - loss: 0.0771 - categorical_accuracy: 0.9829\n",
      "Epoch 00019: val_loss did not improve from 0.09776\n",
      "24377/24377 [==============================] - 5s 223us/sample - loss: 0.0770 - categorical_accuracy: 0.9829 - val_loss: 0.0978 - val_categorical_accuracy: 0.9806\n",
      "Epoch 20/100\n",
      "24160/24377 [============================>.] - ETA: 0s - loss: 0.0687 - categorical_accuracy: 0.9857\n",
      "Epoch 00020: val_loss did not improve from 0.09776\n",
      "24377/24377 [==============================] - 5s 224us/sample - loss: 0.0691 - categorical_accuracy: 0.9855 - val_loss: 0.1146 - val_categorical_accuracy: 0.9793\n",
      "Epoch 21/100\n",
      "24256/24377 [============================>.] - ETA: 0s - loss: 0.0649 - categorical_accuracy: 0.9871\n",
      "Epoch 00021: val_loss did not improve from 0.09776\n",
      "24377/24377 [==============================] - 5s 212us/sample - loss: 0.0652 - categorical_accuracy: 0.9870 - val_loss: 0.1011 - val_categorical_accuracy: 0.9826\n",
      "Epoch 22/100\n",
      "24224/24377 [============================>.] - ETA: 0s - loss: 0.0644 - categorical_accuracy: 0.9862\n",
      "Epoch 00022: val_loss improved from 0.09776 to 0.09242, saving model to t_weights_0\n",
      "24377/24377 [==============================] - 5s 224us/sample - loss: 0.0643 - categorical_accuracy: 0.9863 - val_loss: 0.0924 - val_categorical_accuracy: 0.9862\n",
      "Epoch 23/100\n",
      "24096/24377 [============================>.] - ETA: 0s - loss: 0.0644 - categorical_accuracy: 0.9865\n",
      "Epoch 00023: val_loss did not improve from 0.09242\n",
      "24377/24377 [==============================] - 5s 205us/sample - loss: 0.0645 - categorical_accuracy: 0.9864 - val_loss: 0.1011 - val_categorical_accuracy: 0.9819\n",
      "Epoch 24/100\n",
      "24288/24377 [============================>.] - ETA: 0s - loss: 0.0605 - categorical_accuracy: 0.9886\n",
      "Epoch 00024: val_loss did not improve from 0.09242\n",
      "24377/24377 [==============================] - 5s 221us/sample - loss: 0.0604 - categorical_accuracy: 0.9886 - val_loss: 0.1043 - val_categorical_accuracy: 0.9829\n",
      "Epoch 25/100\n",
      "24160/24377 [============================>.] - ETA: 0s - loss: 0.0622 - categorical_accuracy: 0.9871\n",
      "Epoch 00025: val_loss did not improve from 0.09242\n",
      "24377/24377 [==============================] - 5s 222us/sample - loss: 0.0624 - categorical_accuracy: 0.9871 - val_loss: 0.1068 - val_categorical_accuracy: 0.9829\n",
      "Epoch 26/100\n",
      "24256/24377 [============================>.] - ETA: 0s - loss: 0.0525 - categorical_accuracy: 0.9916\n",
      "Epoch 00026: val_loss did not improve from 0.09242\n",
      "24377/24377 [==============================] - 5s 223us/sample - loss: 0.0527 - categorical_accuracy: 0.9915 - val_loss: 0.1224 - val_categorical_accuracy: 0.9780\n",
      "Epoch 27/100\n",
      "24256/24377 [============================>.] - ETA: 0s - loss: 0.0555 - categorical_accuracy: 0.9902\n",
      "Epoch 00027: val_loss did not improve from 0.09242\n",
      "24377/24377 [==============================] - 5s 220us/sample - loss: 0.0557 - categorical_accuracy: 0.9900 - val_loss: 0.1140 - val_categorical_accuracy: 0.9813\n",
      "0.97603416 0.8027\n",
      "\n",
      "\n",
      "nrx: 20 - real: 1 \n",
      "Train on 31911 samples, validate on 3988 samples\n",
      "Epoch 1/100\n",
      "31744/31911 [============================>.] - ETA: 0s - loss: 1.1698 - categorical_accuracy: 0.5193\n",
      "Epoch 00001: val_loss improved from inf to 0.69705, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 8s 240us/sample - loss: 1.1681 - categorical_accuracy: 0.5202 - val_loss: 0.6971 - val_categorical_accuracy: 0.7019\n",
      "Epoch 2/100\n",
      "31808/31911 [============================>.] - ETA: 0s - loss: 0.6363 - categorical_accuracy: 0.7480\n",
      "Epoch 00002: val_loss improved from 0.69705 to 0.47479, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 225us/sample - loss: 0.6358 - categorical_accuracy: 0.7481 - val_loss: 0.4748 - val_categorical_accuracy: 0.8310\n",
      "Epoch 3/100\n",
      "31776/31911 [============================>.] - ETA: 0s - loss: 0.4482 - categorical_accuracy: 0.8406\n",
      "Epoch 00003: val_loss improved from 0.47479 to 0.33407, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 224us/sample - loss: 0.4482 - categorical_accuracy: 0.8405 - val_loss: 0.3341 - val_categorical_accuracy: 0.8899\n",
      "Epoch 4/100\n",
      "31840/31911 [============================>.] - ETA: 0s - loss: 0.3395 - categorical_accuracy: 0.8871\n",
      "Epoch 00004: val_loss improved from 0.33407 to 0.29945, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 219us/sample - loss: 0.3395 - categorical_accuracy: 0.8871 - val_loss: 0.2994 - val_categorical_accuracy: 0.8974\n",
      "Epoch 5/100\n",
      "31840/31911 [============================>.] - ETA: 0s - loss: 0.2801 - categorical_accuracy: 0.9100\n",
      "Epoch 00005: val_loss improved from 0.29945 to 0.25646, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 225us/sample - loss: 0.2801 - categorical_accuracy: 0.9101 - val_loss: 0.2565 - val_categorical_accuracy: 0.9178\n",
      "Epoch 6/100\n",
      "31840/31911 [============================>.] - ETA: 0s - loss: 0.2547 - categorical_accuracy: 0.9197\n",
      "Epoch 00006: val_loss improved from 0.25646 to 0.23629, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 225us/sample - loss: 0.2545 - categorical_accuracy: 0.9197 - val_loss: 0.2363 - val_categorical_accuracy: 0.9258\n",
      "Epoch 7/100\n",
      "31744/31911 [============================>.] - ETA: 0s - loss: 0.2324 - categorical_accuracy: 0.9286\n",
      "Epoch 00007: val_loss improved from 0.23629 to 0.22043, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 225us/sample - loss: 0.2325 - categorical_accuracy: 0.9285 - val_loss: 0.2204 - val_categorical_accuracy: 0.9318\n",
      "Epoch 8/100\n",
      "31840/31911 [============================>.] - ETA: 0s - loss: 0.2173 - categorical_accuracy: 0.9326\n",
      "Epoch 00008: val_loss improved from 0.22043 to 0.21281, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 223us/sample - loss: 0.2173 - categorical_accuracy: 0.9327 - val_loss: 0.2128 - val_categorical_accuracy: 0.9356\n",
      "Epoch 9/100\n",
      "31808/31911 [============================>.] - ETA: 0s - loss: 0.2077 - categorical_accuracy: 0.9368\n",
      "Epoch 00009: val_loss improved from 0.21281 to 0.20817, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 226us/sample - loss: 0.2077 - categorical_accuracy: 0.9368 - val_loss: 0.2082 - val_categorical_accuracy: 0.9356\n",
      "Epoch 10/100\n",
      "31776/31911 [============================>.] - ETA: 0s - loss: 0.2012 - categorical_accuracy: 0.9380\n",
      "Epoch 00010: val_loss did not improve from 0.20817\n",
      "31911/31911 [==============================] - 7s 223us/sample - loss: 0.2009 - categorical_accuracy: 0.9382 - val_loss: 0.2187 - val_categorical_accuracy: 0.9325\n",
      "Epoch 11/100\n",
      "31680/31911 [============================>.] - ETA: 0s - loss: 0.1878 - categorical_accuracy: 0.9435\n",
      "Epoch 00011: val_loss improved from 0.20817 to 0.20416, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 226us/sample - loss: 0.1878 - categorical_accuracy: 0.9435 - val_loss: 0.2042 - val_categorical_accuracy: 0.9383\n",
      "Epoch 12/100\n",
      "31744/31911 [============================>.] - ETA: 0s - loss: 0.1857 - categorical_accuracy: 0.9432\n",
      "Epoch 00012: val_loss did not improve from 0.20416\n",
      "31911/31911 [==============================] - 7s 213us/sample - loss: 0.1862 - categorical_accuracy: 0.9431 - val_loss: 0.2102 - val_categorical_accuracy: 0.9351\n",
      "Epoch 13/100\n",
      "31872/31911 [============================>.] - ETA: 0s - loss: 0.1819 - categorical_accuracy: 0.9450\n",
      "Epoch 00013: val_loss improved from 0.20416 to 0.20009, saving model to t_weights_0\n",
      "31911/31911 [==============================] - 7s 227us/sample - loss: 0.1818 - categorical_accuracy: 0.9451 - val_loss: 0.2001 - val_categorical_accuracy: 0.9388\n",
      "Epoch 14/100\n",
      "31712/31911 [============================>.] - ETA: 0s - loss: 0.1742 - categorical_accuracy: 0.9477\n",
      "Epoch 00014: val_loss did not improve from 0.20009\n",
      "31911/31911 [==============================] - 7s 223us/sample - loss: 0.1741 - categorical_accuracy: 0.9478 - val_loss: 0.2058 - val_categorical_accuracy: 0.9391\n",
      "Epoch 15/100\n",
      "31904/31911 [============================>.] - ETA: 0s - loss: 0.1699 - categorical_accuracy: 0.9498\n",
      "Epoch 00015: val_loss did not improve from 0.20009\n",
      "31911/31911 [==============================] - 7s 225us/sample - loss: 0.1698 - categorical_accuracy: 0.9498 - val_loss: 0.2154 - val_categorical_accuracy: 0.9383\n",
      "Epoch 16/100\n",
      "31808/31911 [============================>.] - ETA: 0s - loss: 0.1629 - categorical_accuracy: 0.9514\n",
      "Epoch 00016: val_loss did not improve from 0.20009\n",
      "31911/31911 [==============================] - 7s 222us/sample - loss: 0.1628 - categorical_accuracy: 0.9514 - val_loss: 0.2149 - val_categorical_accuracy: 0.9366\n",
      "Epoch 17/100\n",
      "31744/31911 [============================>.] - ETA: 0s - loss: 0.1570 - categorical_accuracy: 0.9531\n",
      "Epoch 00017: val_loss did not improve from 0.20009\n",
      "31911/31911 [==============================] - 7s 221us/sample - loss: 0.1573 - categorical_accuracy: 0.9530 - val_loss: 0.2039 - val_categorical_accuracy: 0.9398\n",
      "Epoch 18/100\n",
      "31744/31911 [============================>.] - ETA: 0s - loss: 0.1597 - categorical_accuracy: 0.9533\n",
      "Epoch 00018: val_loss did not improve from 0.20009\n",
      "31911/31911 [==============================] - 7s 222us/sample - loss: 0.1599 - categorical_accuracy: 0.9531 - val_loss: 0.2323 - val_categorical_accuracy: 0.9341\n",
      "0.95185554 0.8417\n",
      "\n",
      "\n",
      "nrx: 25 - real: 1 \n",
      "Train on 39887 samples, validate on 4985 samples\n",
      "Epoch 1/100\n",
      "39776/39887 [============================>.] - ETA: 0s - loss: 1.1458 - categorical_accuracy: 0.5294\n",
      "Epoch 00001: val_loss improved from inf to 0.65440, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 220us/sample - loss: 1.1446 - categorical_accuracy: 0.5299 - val_loss: 0.6544 - val_categorical_accuracy: 0.7519\n",
      "Epoch 2/100\n",
      "39648/39887 [============================>.] - ETA: 0s - loss: 0.6073 - categorical_accuracy: 0.7744\n",
      "Epoch 00002: val_loss improved from 0.65440 to 0.43493, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 217us/sample - loss: 0.6070 - categorical_accuracy: 0.7744 - val_loss: 0.4349 - val_categorical_accuracy: 0.8544\n",
      "Epoch 3/100\n",
      "39840/39887 [============================>.] - ETA: 0s - loss: 0.4397 - categorical_accuracy: 0.8496\n",
      "Epoch 00003: val_loss improved from 0.43493 to 0.34408, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.4397 - categorical_accuracy: 0.8496 - val_loss: 0.3441 - val_categorical_accuracy: 0.8826\n",
      "Epoch 4/100\n",
      "39712/39887 [============================>.] - ETA: 0s - loss: 0.3545 - categorical_accuracy: 0.8845\n",
      "Epoch 00004: val_loss improved from 0.34408 to 0.30056, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 225us/sample - loss: 0.3543 - categorical_accuracy: 0.8846 - val_loss: 0.3006 - val_categorical_accuracy: 0.8979\n",
      "Epoch 5/100\n",
      "39744/39887 [============================>.] - ETA: 0s - loss: 0.3156 - categorical_accuracy: 0.8983\n",
      "Epoch 00005: val_loss did not improve from 0.30056\n",
      "39887/39887 [==============================] - 9s 220us/sample - loss: 0.3153 - categorical_accuracy: 0.8985 - val_loss: 0.3203 - val_categorical_accuracy: 0.8913\n",
      "Epoch 6/100\n",
      "39680/39887 [============================>.] - ETA: 0s - loss: 0.2882 - categorical_accuracy: 0.9072\n",
      "Epoch 00006: val_loss improved from 0.30056 to 0.25113, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.2884 - categorical_accuracy: 0.9071 - val_loss: 0.2511 - val_categorical_accuracy: 0.9186\n",
      "Epoch 7/100\n",
      "39712/39887 [============================>.] - ETA: 0s - loss: 0.2637 - categorical_accuracy: 0.9158\n",
      "Epoch 00007: val_loss improved from 0.25113 to 0.24766, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 224us/sample - loss: 0.2639 - categorical_accuracy: 0.9158 - val_loss: 0.2477 - val_categorical_accuracy: 0.9186\n",
      "Epoch 8/100\n",
      "39744/39887 [============================>.] - ETA: 0s - loss: 0.2515 - categorical_accuracy: 0.9215\n",
      "Epoch 00008: val_loss improved from 0.24766 to 0.24416, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 223us/sample - loss: 0.2513 - categorical_accuracy: 0.9215 - val_loss: 0.2442 - val_categorical_accuracy: 0.9220\n",
      "Epoch 9/100\n",
      "39744/39887 [============================>.] - ETA: 0s - loss: 0.2373 - categorical_accuracy: 0.9261\n",
      "Epoch 00009: val_loss did not improve from 0.24416\n",
      "39887/39887 [==============================] - 9s 218us/sample - loss: 0.2370 - categorical_accuracy: 0.9262 - val_loss: 0.2463 - val_categorical_accuracy: 0.9230\n",
      "Epoch 10/100\n",
      "39808/39887 [============================>.] - ETA: 0s - loss: 0.2316 - categorical_accuracy: 0.9270\n",
      "Epoch 00010: val_loss improved from 0.24416 to 0.22740, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 224us/sample - loss: 0.2319 - categorical_accuracy: 0.9269 - val_loss: 0.2274 - val_categorical_accuracy: 0.9296\n",
      "Epoch 11/100\n",
      "39648/39887 [============================>.] - ETA: 0s - loss: 0.2256 - categorical_accuracy: 0.9285\n",
      "Epoch 00011: val_loss did not improve from 0.22740\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.2255 - categorical_accuracy: 0.9286 - val_loss: 0.2324 - val_categorical_accuracy: 0.9286\n",
      "Epoch 12/100\n",
      "39776/39887 [============================>.] - ETA: 0s - loss: 0.2162 - categorical_accuracy: 0.9323\n",
      "Epoch 00012: val_loss did not improve from 0.22740\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.2161 - categorical_accuracy: 0.9324 - val_loss: 0.2287 - val_categorical_accuracy: 0.9274\n",
      "Epoch 13/100\n",
      "39872/39887 [============================>.] - ETA: 0s - loss: 0.2098 - categorical_accuracy: 0.9329\n",
      "Epoch 00013: val_loss improved from 0.22740 to 0.22661, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 223us/sample - loss: 0.2098 - categorical_accuracy: 0.9329 - val_loss: 0.2266 - val_categorical_accuracy: 0.9306\n",
      "Epoch 14/100\n",
      "39712/39887 [============================>.] - ETA: 0s - loss: 0.2034 - categorical_accuracy: 0.9373\n",
      "Epoch 00014: val_loss improved from 0.22661 to 0.21479, saving model to t_weights_0\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.2034 - categorical_accuracy: 0.9374 - val_loss: 0.2148 - val_categorical_accuracy: 0.9340\n",
      "Epoch 15/100\n",
      "39712/39887 [============================>.] - ETA: 0s - loss: 0.1983 - categorical_accuracy: 0.9381\n",
      "Epoch 00015: val_loss did not improve from 0.21479\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.1983 - categorical_accuracy: 0.9380 - val_loss: 0.2215 - val_categorical_accuracy: 0.9320\n",
      "Epoch 16/100\n",
      "39648/39887 [============================>.] - ETA: 0s - loss: 0.1917 - categorical_accuracy: 0.9395\n",
      "Epoch 00016: val_loss did not improve from 0.21479\n",
      "39887/39887 [==============================] - 9s 217us/sample - loss: 0.1915 - categorical_accuracy: 0.9397 - val_loss: 0.2209 - val_categorical_accuracy: 0.9326\n",
      "Epoch 17/100\n",
      "39712/39887 [============================>.] - ETA: 0s - loss: 0.1883 - categorical_accuracy: 0.9415\n",
      "Epoch 00017: val_loss did not improve from 0.21479\n",
      "39887/39887 [==============================] - 9s 223us/sample - loss: 0.1884 - categorical_accuracy: 0.9415 - val_loss: 0.2474 - val_categorical_accuracy: 0.9252\n",
      "Epoch 18/100\n",
      "39744/39887 [============================>.] - ETA: 0s - loss: 0.1845 - categorical_accuracy: 0.9430\n",
      "Epoch 00018: val_loss did not improve from 0.21479\n",
      "39887/39887 [==============================] - 9s 220us/sample - loss: 0.1843 - categorical_accuracy: 0.9431 - val_loss: 0.2450 - val_categorical_accuracy: 0.9294\n",
      "Epoch 19/100\n",
      "39776/39887 [============================>.] - ETA: 0s - loss: 0.1803 - categorical_accuracy: 0.9439\n",
      "Epoch 00019: val_loss did not improve from 0.21479\n",
      "39887/39887 [==============================] - 9s 222us/sample - loss: 0.1803 - categorical_accuracy: 0.9439 - val_loss: 0.2154 - val_categorical_accuracy: 0.9360\n",
      "0.9362086 0.8553\n",
      "\n",
      "\n",
      "nrx: 0 - real: 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/Documents/txid_framework/dataset_scripts/working/data_utilities.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cls_weights = np.max(stat,axis=0)/stat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 2.2804 - categorical_accuracy: 0.1875\n",
      "Epoch 00001: val_loss improved from inf to 2.14947, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 1s 485us/sample - loss: 2.2741 - categorical_accuracy: 0.1988 - val_loss: 2.1495 - val_categorical_accuracy: 0.4900\n",
      "Epoch 2/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 1.8347 - categorical_accuracy: 0.4906\n",
      "Epoch 00002: val_loss improved from 2.14947 to 1.11775, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 252us/sample - loss: 1.7612 - categorical_accuracy: 0.5056 - val_loss: 1.1177 - val_categorical_accuracy: 0.7200\n",
      "Epoch 3/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.9990 - categorical_accuracy: 0.6577\n",
      "Epoch 00003: val_loss improved from 1.11775 to 0.49500, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 267us/sample - loss: 0.9564 - categorical_accuracy: 0.6744 - val_loss: 0.4950 - val_categorical_accuracy: 0.9150\n",
      "Epoch 4/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.5645 - categorical_accuracy: 0.8087\n",
      "Epoch 00004: val_loss improved from 0.49500 to 0.27688, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 271us/sample - loss: 0.5619 - categorical_accuracy: 0.8100 - val_loss: 0.2769 - val_categorical_accuracy: 0.9200\n",
      "Epoch 5/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.3736 - categorical_accuracy: 0.8833\n",
      "Epoch 00005: val_loss improved from 0.27688 to 0.21449, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 258us/sample - loss: 0.3772 - categorical_accuracy: 0.8794 - val_loss: 0.2145 - val_categorical_accuracy: 0.9550\n",
      "Epoch 6/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.2885 - categorical_accuracy: 0.9042\n",
      "Epoch 00006: val_loss improved from 0.21449 to 0.12137, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 227us/sample - loss: 0.2833 - categorical_accuracy: 0.9056 - val_loss: 0.1214 - val_categorical_accuracy: 0.9950\n",
      "Epoch 7/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.2362 - categorical_accuracy: 0.9286\n",
      "Epoch 00007: val_loss improved from 0.12137 to 0.09761, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 273us/sample - loss: 0.2361 - categorical_accuracy: 0.9294 - val_loss: 0.0976 - val_categorical_accuracy: 0.9900\n",
      "Epoch 8/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.1834 - categorical_accuracy: 0.9479\n",
      "Epoch 00008: val_loss did not improve from 0.09761\n",
      "1600/1600 [==============================] - 0s 233us/sample - loss: 0.1861 - categorical_accuracy: 0.9481 - val_loss: 0.1216 - val_categorical_accuracy: 0.9700\n",
      "Epoch 9/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.1627 - categorical_accuracy: 0.9611\n",
      "Epoch 00009: val_loss improved from 0.09761 to 0.05860, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 267us/sample - loss: 0.1610 - categorical_accuracy: 0.9619 - val_loss: 0.0586 - val_categorical_accuracy: 0.9950\n",
      "Epoch 10/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.1225 - categorical_accuracy: 0.9719\n",
      "Epoch 00010: val_loss improved from 0.05860 to 0.04732, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 278us/sample - loss: 0.1230 - categorical_accuracy: 0.9712 - val_loss: 0.0473 - val_categorical_accuracy: 0.9950\n",
      "Epoch 11/100\n",
      "1312/1600 [=======================>......] - ETA: 0s - loss: 0.1297 - categorical_accuracy: 0.9688\n",
      "Epoch 00011: val_loss improved from 0.04732 to 0.04057, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 261us/sample - loss: 0.1248 - categorical_accuracy: 0.9694 - val_loss: 0.0406 - val_categorical_accuracy: 0.9950\n",
      "Epoch 12/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.1046 - categorical_accuracy: 0.9751\n",
      "Epoch 00012: val_loss improved from 0.04057 to 0.03244, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 239us/sample - loss: 0.1025 - categorical_accuracy: 0.9762 - val_loss: 0.0324 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0937 - categorical_accuracy: 0.9770\n",
      "Epoch 00013: val_loss did not improve from 0.03244\n",
      "1600/1600 [==============================] - 0s 231us/sample - loss: 0.0930 - categorical_accuracy: 0.9769 - val_loss: 0.0367 - val_categorical_accuracy: 0.9950\n",
      "Epoch 14/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0892 - categorical_accuracy: 0.9834\n",
      "Epoch 00014: val_loss improved from 0.03244 to 0.02895, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 271us/sample - loss: 0.0902 - categorical_accuracy: 0.9831 - val_loss: 0.0289 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0787 - categorical_accuracy: 0.9818\n",
      "Epoch 00015: val_loss improved from 0.02895 to 0.02805, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 279us/sample - loss: 0.0814 - categorical_accuracy: 0.9806 - val_loss: 0.0280 - val_categorical_accuracy: 0.9950\n",
      "Epoch 16/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0809 - categorical_accuracy: 0.9831\n",
      "Epoch 00016: val_loss did not improve from 0.02805\n",
      "1600/1600 [==============================] - 0s 232us/sample - loss: 0.0795 - categorical_accuracy: 0.9837 - val_loss: 0.0321 - val_categorical_accuracy: 0.9950\n",
      "Epoch 17/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0734 - categorical_accuracy: 0.9831\n",
      "Epoch 00017: val_loss improved from 0.02805 to 0.02471, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 267us/sample - loss: 0.0722 - categorical_accuracy: 0.9837 - val_loss: 0.0247 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0569 - categorical_accuracy: 0.9880\n",
      "Epoch 00018: val_loss did not improve from 0.02471\n",
      "1600/1600 [==============================] - 0s 200us/sample - loss: 0.0565 - categorical_accuracy: 0.9887 - val_loss: 0.0312 - val_categorical_accuracy: 0.9950\n",
      "Epoch 19/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.0559 - categorical_accuracy: 0.9884\n",
      "Epoch 00019: val_loss did not improve from 0.02471\n",
      "1600/1600 [==============================] - 0s 225us/sample - loss: 0.0572 - categorical_accuracy: 0.9869 - val_loss: 0.0276 - val_categorical_accuracy: 0.9950\n",
      "Epoch 20/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0552 - categorical_accuracy: 0.9870\n",
      "Epoch 00020: val_loss improved from 0.02471 to 0.02237, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 266us/sample - loss: 0.0545 - categorical_accuracy: 0.9875 - val_loss: 0.0224 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0513 - categorical_accuracy: 0.9898\n",
      "Epoch 00021: val_loss improved from 0.02237 to 0.02225, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 275us/sample - loss: 0.0519 - categorical_accuracy: 0.9894 - val_loss: 0.0222 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0458 - categorical_accuracy: 0.9940\n",
      "Epoch 00022: val_loss improved from 0.02225 to 0.02053, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 265us/sample - loss: 0.0458 - categorical_accuracy: 0.9937 - val_loss: 0.0205 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0514 - categorical_accuracy: 0.9885\n",
      "Epoch 00023: val_loss did not improve from 0.02053\n",
      "1600/1600 [==============================] - 0s 228us/sample - loss: 0.0511 - categorical_accuracy: 0.9887 - val_loss: 0.0368 - val_categorical_accuracy: 0.9950\n",
      "Epoch 24/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0443 - categorical_accuracy: 0.9933\n",
      "Epoch 00024: val_loss improved from 0.02053 to 0.02016, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 250us/sample - loss: 0.0434 - categorical_accuracy: 0.9931 - val_loss: 0.0202 - val_categorical_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0430 - categorical_accuracy: 0.9929\n",
      "Epoch 00025: val_loss did not improve from 0.02016\n",
      "1600/1600 [==============================] - 0s 219us/sample - loss: 0.0405 - categorical_accuracy: 0.9937 - val_loss: 0.0268 - val_categorical_accuracy: 0.9950\n",
      "Epoch 26/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 0.0361 - categorical_accuracy: 0.9948\n",
      "Epoch 00026: val_loss did not improve from 0.02016\n",
      "1600/1600 [==============================] - 0s 225us/sample - loss: 0.0375 - categorical_accuracy: 0.9950 - val_loss: 0.0227 - val_categorical_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0376 - categorical_accuracy: 0.9948\n",
      "Epoch 00027: val_loss did not improve from 0.02016\n",
      "1600/1600 [==============================] - 0s 238us/sample - loss: 0.0371 - categorical_accuracy: 0.9950 - val_loss: 0.0223 - val_categorical_accuracy: 0.9950\n",
      "Epoch 28/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0370 - categorical_accuracy: 0.9967\n",
      "Epoch 00028: val_loss improved from 0.02016 to 0.01884, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 284us/sample - loss: 0.0370 - categorical_accuracy: 0.9962 - val_loss: 0.0188 - val_categorical_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0346 - categorical_accuracy: 0.9962\n",
      "Epoch 00029: val_loss improved from 0.01884 to 0.01852, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 277us/sample - loss: 0.0352 - categorical_accuracy: 0.9956 - val_loss: 0.0185 - val_categorical_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.0339 - categorical_accuracy: 0.9925\n",
      "Epoch 00030: val_loss did not improve from 0.01852\n",
      "1600/1600 [==============================] - 0s 200us/sample - loss: 0.0340 - categorical_accuracy: 0.9925 - val_loss: 0.0191 - val_categorical_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.0295 - categorical_accuracy: 0.9964\n",
      "Epoch 00031: val_loss did not improve from 0.01852\n",
      "1600/1600 [==============================] - 0s 215us/sample - loss: 0.0294 - categorical_accuracy: 0.9969 - val_loss: 0.0290 - val_categorical_accuracy: 0.9950\n",
      "Epoch 32/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0302 - categorical_accuracy: 0.9967\n",
      "Epoch 00032: val_loss did not improve from 0.01852\n",
      "1600/1600 [==============================] - 0s 236us/sample - loss: 0.0313 - categorical_accuracy: 0.9962 - val_loss: 0.0224 - val_categorical_accuracy: 0.9950\n",
      "Epoch 33/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0293 - categorical_accuracy: 0.9967\n",
      "Epoch 00033: val_loss did not improve from 0.01852\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0291 - categorical_accuracy: 0.9969 - val_loss: 0.0437 - val_categorical_accuracy: 0.9950\n",
      "Epoch 34/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0340 - categorical_accuracy: 0.9955\n",
      "Epoch 00034: val_loss did not improve from 0.01852\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0337 - categorical_accuracy: 0.9956 - val_loss: 0.0202 - val_categorical_accuracy: 1.0000\n",
      "0.995 0.3097758405977584\n",
      "\n",
      "\n",
      "nrx: 5 - real: 2 \n",
      "Train on 9153 samples, validate on 1144 samples\n",
      "Epoch 1/100\n",
      "9024/9153 [============================>.] - ETA: 0s - loss: 1.6195 - categorical_accuracy: 0.3715\n",
      "Epoch 00001: val_loss improved from inf to 0.76189, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 272us/sample - loss: 1.6100 - categorical_accuracy: 0.3745 - val_loss: 0.7619 - val_categorical_accuracy: 0.7395\n",
      "Epoch 2/100\n",
      "9120/9153 [============================>.] - ETA: 0s - loss: 0.7488 - categorical_accuracy: 0.6766\n",
      "Epoch 00002: val_loss improved from 0.76189 to 0.51579, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 231us/sample - loss: 0.7490 - categorical_accuracy: 0.6768 - val_loss: 0.5158 - val_categorical_accuracy: 0.7998\n",
      "Epoch 3/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.5448 - categorical_accuracy: 0.7815\n",
      "Epoch 00003: val_loss improved from 0.51579 to 0.37620, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 233us/sample - loss: 0.5416 - categorical_accuracy: 0.7835 - val_loss: 0.3762 - val_categorical_accuracy: 0.8636\n",
      "Epoch 4/100\n",
      "9088/9153 [============================>.] - ETA: 0s - loss: 0.3932 - categorical_accuracy: 0.8564\n",
      "Epoch 00004: val_loss improved from 0.37620 to 0.28561, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 231us/sample - loss: 0.3922 - categorical_accuracy: 0.8569 - val_loss: 0.2856 - val_categorical_accuracy: 0.8960\n",
      "Epoch 5/100\n",
      "9120/9153 [============================>.] - ETA: 0s - loss: 0.3135 - categorical_accuracy: 0.8941\n",
      "Epoch 00005: val_loss improved from 0.28561 to 0.21624, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 212us/sample - loss: 0.3139 - categorical_accuracy: 0.8935 - val_loss: 0.2162 - val_categorical_accuracy: 0.9318\n",
      "Epoch 6/100\n",
      "9024/9153 [============================>.] - ETA: 0s - loss: 0.2371 - categorical_accuracy: 0.9284\n",
      "Epoch 00006: val_loss improved from 0.21624 to 0.17481, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 219us/sample - loss: 0.2368 - categorical_accuracy: 0.9284 - val_loss: 0.1748 - val_categorical_accuracy: 0.9528\n",
      "Epoch 7/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.1893 - categorical_accuracy: 0.9444\n",
      "Epoch 00007: val_loss improved from 0.17481 to 0.14366, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 229us/sample - loss: 0.1879 - categorical_accuracy: 0.9453 - val_loss: 0.1437 - val_categorical_accuracy: 0.9589\n",
      "Epoch 8/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.1488 - categorical_accuracy: 0.9625\n",
      "Epoch 00008: val_loss improved from 0.14366 to 0.11947, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 229us/sample - loss: 0.1500 - categorical_accuracy: 0.9626 - val_loss: 0.1195 - val_categorical_accuracy: 0.9694\n",
      "Epoch 9/100\n",
      "8960/9153 [============================>.] - ETA: 0s - loss: 0.1386 - categorical_accuracy: 0.9674\n",
      "Epoch 00009: val_loss did not improve from 0.11947\n",
      "9153/9153 [==============================] - 2s 226us/sample - loss: 0.1377 - categorical_accuracy: 0.9678 - val_loss: 0.1333 - val_categorical_accuracy: 0.9642\n",
      "Epoch 10/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.1116 - categorical_accuracy: 0.9745\n",
      "Epoch 00010: val_loss improved from 0.11947 to 0.10023, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 237us/sample - loss: 0.1124 - categorical_accuracy: 0.9744 - val_loss: 0.1002 - val_categorical_accuracy: 0.9790\n",
      "Epoch 11/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.1032 - categorical_accuracy: 0.9765\n",
      "Epoch 00011: val_loss did not improve from 0.10023\n",
      "9153/9153 [==============================] - 2s 223us/sample - loss: 0.1026 - categorical_accuracy: 0.9766 - val_loss: 0.1097 - val_categorical_accuracy: 0.9738\n",
      "Epoch 12/100\n",
      "8960/9153 [============================>.] - ETA: 0s - loss: 0.0934 - categorical_accuracy: 0.9780\n",
      "Epoch 00012: val_loss did not improve from 0.10023\n",
      "9153/9153 [==============================] - 2s 222us/sample - loss: 0.0935 - categorical_accuracy: 0.9779 - val_loss: 0.1050 - val_categorical_accuracy: 0.9781\n",
      "Epoch 13/100\n",
      "8992/9153 [============================>.] - ETA: 0s - loss: 0.0862 - categorical_accuracy: 0.9810\n",
      "Epoch 00013: val_loss improved from 0.10023 to 0.09513, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 230us/sample - loss: 0.0863 - categorical_accuracy: 0.9808 - val_loss: 0.0951 - val_categorical_accuracy: 0.9781\n",
      "Epoch 14/100\n",
      "9088/9153 [============================>.] - ETA: 0s - loss: 0.0765 - categorical_accuracy: 0.9835\n",
      "Epoch 00014: val_loss improved from 0.09513 to 0.09183, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 230us/sample - loss: 0.0762 - categorical_accuracy: 0.9836 - val_loss: 0.0918 - val_categorical_accuracy: 0.9816\n",
      "Epoch 15/100\n",
      "9088/9153 [============================>.] - ETA: 0s - loss: 0.0728 - categorical_accuracy: 0.9827\n",
      "Epoch 00015: val_loss did not improve from 0.09183\n",
      "9153/9153 [==============================] - 2s 223us/sample - loss: 0.0725 - categorical_accuracy: 0.9828 - val_loss: 0.1019 - val_categorical_accuracy: 0.9816\n",
      "Epoch 16/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.0683 - categorical_accuracy: 0.9857\n",
      "Epoch 00016: val_loss did not improve from 0.09183\n",
      "9153/9153 [==============================] - 2s 227us/sample - loss: 0.0678 - categorical_accuracy: 0.9859 - val_loss: 0.0956 - val_categorical_accuracy: 0.9843\n",
      "Epoch 17/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.0623 - categorical_accuracy: 0.9884\n",
      "Epoch 00017: val_loss improved from 0.09183 to 0.08120, saving model to t_weights_0\n",
      "9153/9153 [==============================] - 2s 235us/sample - loss: 0.0630 - categorical_accuracy: 0.9882 - val_loss: 0.0812 - val_categorical_accuracy: 0.9895\n",
      "Epoch 18/100\n",
      "9120/9153 [============================>.] - ETA: 0s - loss: 0.0529 - categorical_accuracy: 0.9899\n",
      "Epoch 00018: val_loss did not improve from 0.08120\n",
      "9153/9153 [==============================] - 2s 224us/sample - loss: 0.0528 - categorical_accuracy: 0.9899 - val_loss: 0.0847 - val_categorical_accuracy: 0.9860\n",
      "Epoch 19/100\n",
      "8992/9153 [============================>.] - ETA: 0s - loss: 0.0608 - categorical_accuracy: 0.9872\n",
      "Epoch 00019: val_loss did not improve from 0.08120\n",
      "9153/9153 [==============================] - 2s 222us/sample - loss: 0.0610 - categorical_accuracy: 0.9870 - val_loss: 0.0823 - val_categorical_accuracy: 0.9878\n",
      "Epoch 20/100\n",
      "9056/9153 [============================>.] - ETA: 0s - loss: 0.0570 - categorical_accuracy: 0.9876\n",
      "Epoch 00020: val_loss did not improve from 0.08120\n",
      "9153/9153 [==============================] - 2s 221us/sample - loss: 0.0568 - categorical_accuracy: 0.9878 - val_loss: 0.1242 - val_categorical_accuracy: 0.9694\n",
      "Epoch 21/100\n",
      "9088/9153 [============================>.] - ETA: 0s - loss: 0.0539 - categorical_accuracy: 0.9903\n",
      "Epoch 00021: val_loss did not improve from 0.08120\n",
      "9153/9153 [==============================] - 2s 224us/sample - loss: 0.0540 - categorical_accuracy: 0.9903 - val_loss: 0.0961 - val_categorical_accuracy: 0.9878\n",
      "Epoch 22/100\n",
      "8928/9153 [============================>.] - ETA: 0s - loss: 0.0473 - categorical_accuracy: 0.9923\n",
      "Epoch 00022: val_loss did not improve from 0.08120\n",
      "9153/9153 [==============================] - 2s 225us/sample - loss: 0.0479 - categorical_accuracy: 0.9922 - val_loss: 0.0976 - val_categorical_accuracy: 0.9895\n",
      "0.9895105 0.5153590701535907\n",
      "\n",
      "\n",
      "nrx: 10 - real: 2 \n",
      "Train on 16969 samples, validate on 2121 samples\n",
      "Epoch 1/100\n",
      "16960/16969 [============================>.] - ETA: 0s - loss: 1.4575 - categorical_accuracy: 0.4452\n",
      "Epoch 00001: val_loss improved from inf to 0.85215, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 245us/sample - loss: 1.4571 - categorical_accuracy: 0.4453 - val_loss: 0.8522 - val_categorical_accuracy: 0.6803\n",
      "Epoch 2/100\n",
      "16736/16969 [============================>.] - ETA: 0s - loss: 0.8272 - categorical_accuracy: 0.6729\n",
      "Epoch 00002: val_loss improved from 0.85215 to 0.60356, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 226us/sample - loss: 0.8251 - categorical_accuracy: 0.6742 - val_loss: 0.6036 - val_categorical_accuracy: 0.7897\n",
      "Epoch 3/100\n",
      "16928/16969 [============================>.] - ETA: 0s - loss: 0.6321 - categorical_accuracy: 0.7684\n",
      "Epoch 00003: val_loss improved from 0.60356 to 0.48903, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 231us/sample - loss: 0.6321 - categorical_accuracy: 0.7685 - val_loss: 0.4890 - val_categorical_accuracy: 0.8322\n",
      "Epoch 4/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.4887 - categorical_accuracy: 0.8323\n",
      "Epoch 00004: val_loss improved from 0.48903 to 0.39589, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 226us/sample - loss: 0.4887 - categorical_accuracy: 0.8321 - val_loss: 0.3959 - val_categorical_accuracy: 0.8755\n",
      "Epoch 5/100\n",
      "16960/16969 [============================>.] - ETA: 0s - loss: 0.4039 - categorical_accuracy: 0.8703\n",
      "Epoch 00005: val_loss improved from 0.39589 to 0.36319, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 231us/sample - loss: 0.4038 - categorical_accuracy: 0.8704 - val_loss: 0.3632 - val_categorical_accuracy: 0.8812\n",
      "Epoch 6/100\n",
      "16768/16969 [============================>.] - ETA: 0s - loss: 0.3587 - categorical_accuracy: 0.8853\n",
      "Epoch 00006: val_loss improved from 0.36319 to 0.35987, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 223us/sample - loss: 0.3586 - categorical_accuracy: 0.8851 - val_loss: 0.3599 - val_categorical_accuracy: 0.8652\n",
      "Epoch 7/100\n",
      "16768/16969 [============================>.] - ETA: 0s - loss: 0.3335 - categorical_accuracy: 0.8930\n",
      "Epoch 00007: val_loss improved from 0.35987 to 0.32020, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 3s 197us/sample - loss: 0.3329 - categorical_accuracy: 0.8930 - val_loss: 0.3202 - val_categorical_accuracy: 0.8920\n",
      "Epoch 8/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.3065 - categorical_accuracy: 0.9015\n",
      "Epoch 00008: val_loss improved from 0.32020 to 0.28973, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 231us/sample - loss: 0.3067 - categorical_accuracy: 0.9015 - val_loss: 0.2897 - val_categorical_accuracy: 0.9038\n",
      "Epoch 9/100\n",
      "16960/16969 [============================>.] - ETA: 0s - loss: 0.2907 - categorical_accuracy: 0.9052\n",
      "Epoch 00009: val_loss did not improve from 0.28973\n",
      "16969/16969 [==============================] - 4s 226us/sample - loss: 0.2906 - categorical_accuracy: 0.9052 - val_loss: 0.2919 - val_categorical_accuracy: 0.9019\n",
      "Epoch 10/100\n",
      "16928/16969 [============================>.] - ETA: 0s - loss: 0.2811 - categorical_accuracy: 0.9117\n",
      "Epoch 00010: val_loss improved from 0.28973 to 0.27777, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 225us/sample - loss: 0.2810 - categorical_accuracy: 0.9118 - val_loss: 0.2778 - val_categorical_accuracy: 0.9048\n",
      "Epoch 11/100\n",
      "16704/16969 [============================>.] - ETA: 0s - loss: 0.2766 - categorical_accuracy: 0.9120\n",
      "Epoch 00011: val_loss improved from 0.27777 to 0.27492, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 227us/sample - loss: 0.2773 - categorical_accuracy: 0.9119 - val_loss: 0.2749 - val_categorical_accuracy: 0.9114\n",
      "Epoch 12/100\n",
      "16768/16969 [============================>.] - ETA: 0s - loss: 0.2634 - categorical_accuracy: 0.9162\n",
      "Epoch 00012: val_loss did not improve from 0.27492\n",
      "16969/16969 [==============================] - 4s 230us/sample - loss: 0.2647 - categorical_accuracy: 0.9159 - val_loss: 0.3009 - val_categorical_accuracy: 0.8996\n",
      "Epoch 13/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.2507 - categorical_accuracy: 0.9193\n",
      "Epoch 00013: val_loss improved from 0.27492 to 0.27127, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 228us/sample - loss: 0.2505 - categorical_accuracy: 0.9196 - val_loss: 0.2713 - val_categorical_accuracy: 0.9123\n",
      "Epoch 14/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.2456 - categorical_accuracy: 0.9208\n",
      "Epoch 00014: val_loss did not improve from 0.27127\n",
      "16969/16969 [==============================] - 4s 229us/sample - loss: 0.2453 - categorical_accuracy: 0.9209 - val_loss: 0.2881 - val_categorical_accuracy: 0.9052\n",
      "Epoch 15/100\n",
      "16960/16969 [============================>.] - ETA: 0s - loss: 0.2357 - categorical_accuracy: 0.9249\n",
      "Epoch 00015: val_loss improved from 0.27127 to 0.26545, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 224us/sample - loss: 0.2357 - categorical_accuracy: 0.9250 - val_loss: 0.2655 - val_categorical_accuracy: 0.9161\n",
      "Epoch 16/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.2369 - categorical_accuracy: 0.9248\n",
      "Epoch 00016: val_loss did not improve from 0.26545\n",
      "16969/16969 [==============================] - 4s 227us/sample - loss: 0.2376 - categorical_accuracy: 0.9245 - val_loss: 0.2677 - val_categorical_accuracy: 0.9213\n",
      "Epoch 17/100\n",
      "16736/16969 [============================>.] - ETA: 0s - loss: 0.2258 - categorical_accuracy: 0.9281\n",
      "Epoch 00017: val_loss improved from 0.26545 to 0.25052, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 227us/sample - loss: 0.2255 - categorical_accuracy: 0.9283 - val_loss: 0.2505 - val_categorical_accuracy: 0.9213\n",
      "Epoch 18/100\n",
      "16960/16969 [============================>.] - ETA: 0s - loss: 0.2229 - categorical_accuracy: 0.9297\n",
      "Epoch 00018: val_loss did not improve from 0.25052\n",
      "16969/16969 [==============================] - 4s 224us/sample - loss: 0.2229 - categorical_accuracy: 0.9297 - val_loss: 0.2693 - val_categorical_accuracy: 0.9198\n",
      "Epoch 19/100\n",
      "16832/16969 [============================>.] - ETA: 0s - loss: 0.2168 - categorical_accuracy: 0.9310\n",
      "Epoch 00019: val_loss did not improve from 0.25052\n",
      "16969/16969 [==============================] - 4s 227us/sample - loss: 0.2170 - categorical_accuracy: 0.9309 - val_loss: 0.2845 - val_categorical_accuracy: 0.9128\n",
      "Epoch 20/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.2145 - categorical_accuracy: 0.9344\n",
      "Epoch 00020: val_loss did not improve from 0.25052\n",
      "16969/16969 [==============================] - 4s 222us/sample - loss: 0.2141 - categorical_accuracy: 0.9344 - val_loss: 0.2700 - val_categorical_accuracy: 0.9180\n",
      "Epoch 21/100\n",
      "16768/16969 [============================>.] - ETA: 0s - loss: 0.2066 - categorical_accuracy: 0.9364\n",
      "Epoch 00021: val_loss improved from 0.25052 to 0.24588, saving model to t_weights_0\n",
      "16969/16969 [==============================] - 4s 233us/sample - loss: 0.2063 - categorical_accuracy: 0.9366 - val_loss: 0.2459 - val_categorical_accuracy: 0.9269\n",
      "Epoch 22/100\n",
      "16800/16969 [============================>.] - ETA: 0s - loss: 0.2054 - categorical_accuracy: 0.9360\n",
      "Epoch 00022: val_loss did not improve from 0.24588\n",
      "16969/16969 [==============================] - 4s 217us/sample - loss: 0.2050 - categorical_accuracy: 0.9362 - val_loss: 0.2565 - val_categorical_accuracy: 0.9203\n",
      "Epoch 23/100\n",
      "16960/16969 [============================>.] - ETA: 0s - loss: 0.1978 - categorical_accuracy: 0.9398\n",
      "Epoch 00023: val_loss did not improve from 0.24588\n",
      "16969/16969 [==============================] - 4s 216us/sample - loss: 0.1981 - categorical_accuracy: 0.9397 - val_loss: 0.2542 - val_categorical_accuracy: 0.9241\n",
      "Epoch 24/100\n",
      "16736/16969 [============================>.] - ETA: 0s - loss: 0.1997 - categorical_accuracy: 0.9397\n",
      "Epoch 00024: val_loss did not improve from 0.24588\n",
      "16969/16969 [==============================] - 4s 223us/sample - loss: 0.1997 - categorical_accuracy: 0.9398 - val_loss: 0.2667 - val_categorical_accuracy: 0.9241\n",
      "Epoch 25/100\n",
      "16832/16969 [============================>.] - ETA: 0s - loss: 0.1882 - categorical_accuracy: 0.9422\n",
      "Epoch 00025: val_loss did not improve from 0.24588\n",
      "16969/16969 [==============================] - 4s 230us/sample - loss: 0.1879 - categorical_accuracy: 0.9424 - val_loss: 0.2643 - val_categorical_accuracy: 0.9255\n",
      "Epoch 26/100\n",
      "16832/16969 [============================>.] - ETA: 0s - loss: 0.1825 - categorical_accuracy: 0.9452\n",
      "Epoch 00026: val_loss did not improve from 0.24588\n",
      "16969/16969 [==============================] - 4s 224us/sample - loss: 0.1828 - categorical_accuracy: 0.9452 - val_loss: 0.2468 - val_categorical_accuracy: 0.9227\n",
      "0.9354078 0.7284142797841427\n",
      "\n",
      "\n",
      "nrx: 15 - real: 2 \n",
      "Train on 24527 samples, validate on 3065 samples\n",
      "Epoch 1/100\n",
      "24288/24527 [============================>.] - ETA: 0s - loss: 1.3228 - categorical_accuracy: 0.4734\n",
      "Epoch 00001: val_loss improved from inf to 0.76225, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 6s 237us/sample - loss: 1.3183 - categorical_accuracy: 0.4747 - val_loss: 0.7623 - val_categorical_accuracy: 0.6858\n",
      "Epoch 2/100\n",
      "24320/24527 [============================>.] - ETA: 0s - loss: 0.7422 - categorical_accuracy: 0.6950\n",
      "Epoch 00002: val_loss improved from 0.76225 to 0.55635, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 224us/sample - loss: 0.7408 - categorical_accuracy: 0.6959 - val_loss: 0.5563 - val_categorical_accuracy: 0.7853\n",
      "Epoch 3/100\n",
      "24448/24527 [============================>.] - ETA: 0s - loss: 0.5462 - categorical_accuracy: 0.7970\n",
      "Epoch 00003: val_loss improved from 0.55635 to 0.40804, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 223us/sample - loss: 0.5457 - categorical_accuracy: 0.7972 - val_loss: 0.4080 - val_categorical_accuracy: 0.8574\n",
      "Epoch 4/100\n",
      "24480/24527 [============================>.] - ETA: 0s - loss: 0.4217 - categorical_accuracy: 0.8552\n",
      "Epoch 00004: val_loss improved from 0.40804 to 0.35661, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 222us/sample - loss: 0.4215 - categorical_accuracy: 0.8553 - val_loss: 0.3566 - val_categorical_accuracy: 0.8741\n",
      "Epoch 5/100\n",
      "24416/24527 [============================>.] - ETA: 0s - loss: 0.3554 - categorical_accuracy: 0.8818\n",
      "Epoch 00005: val_loss improved from 0.35661 to 0.28652, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 223us/sample - loss: 0.3548 - categorical_accuracy: 0.8820 - val_loss: 0.2865 - val_categorical_accuracy: 0.9038\n",
      "Epoch 6/100\n",
      "24512/24527 [============================>.] - ETA: 0s - loss: 0.3072 - categorical_accuracy: 0.8994\n",
      "Epoch 00006: val_loss improved from 0.28652 to 0.24605, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 6s 226us/sample - loss: 0.3075 - categorical_accuracy: 0.8993 - val_loss: 0.2460 - val_categorical_accuracy: 0.9165\n",
      "Epoch 7/100\n",
      "24320/24527 [============================>.] - ETA: 0s - loss: 0.2842 - categorical_accuracy: 0.9076\n",
      "Epoch 00007: val_loss did not improve from 0.24605\n",
      "24527/24527 [==============================] - 5s 220us/sample - loss: 0.2841 - categorical_accuracy: 0.9077 - val_loss: 0.2694 - val_categorical_accuracy: 0.9086\n",
      "Epoch 8/100\n",
      "24256/24527 [============================>.] - ETA: 0s - loss: 0.2607 - categorical_accuracy: 0.9150\n",
      "Epoch 00008: val_loss improved from 0.24605 to 0.22868, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 219us/sample - loss: 0.2608 - categorical_accuracy: 0.9150 - val_loss: 0.2287 - val_categorical_accuracy: 0.9230\n",
      "Epoch 9/100\n",
      "24448/24527 [============================>.] - ETA: 0s - loss: 0.2489 - categorical_accuracy: 0.9180\n",
      "Epoch 00009: val_loss improved from 0.22868 to 0.21492, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 220us/sample - loss: 0.2490 - categorical_accuracy: 0.9180 - val_loss: 0.2149 - val_categorical_accuracy: 0.9305\n",
      "Epoch 10/100\n",
      "24288/24527 [============================>.] - ETA: 0s - loss: 0.2309 - categorical_accuracy: 0.9254\n",
      "Epoch 00010: val_loss did not improve from 0.21492\n",
      "24527/24527 [==============================] - 5s 221us/sample - loss: 0.2305 - categorical_accuracy: 0.9255 - val_loss: 0.2231 - val_categorical_accuracy: 0.9282\n",
      "Epoch 11/100\n",
      "24288/24527 [============================>.] - ETA: 0s - loss: 0.2249 - categorical_accuracy: 0.9271\n",
      "Epoch 00011: val_loss improved from 0.21492 to 0.21476, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 224us/sample - loss: 0.2245 - categorical_accuracy: 0.9272 - val_loss: 0.2148 - val_categorical_accuracy: 0.9302\n",
      "Epoch 12/100\n",
      "24288/24527 [============================>.] - ETA: 0s - loss: 0.2133 - categorical_accuracy: 0.9321\n",
      "Epoch 00012: val_loss improved from 0.21476 to 0.21074, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 5s 221us/sample - loss: 0.2136 - categorical_accuracy: 0.9320 - val_loss: 0.2107 - val_categorical_accuracy: 0.9318\n",
      "Epoch 13/100\n",
      "24416/24527 [============================>.] - ETA: 0s - loss: 0.2054 - categorical_accuracy: 0.9357\n",
      "Epoch 00013: val_loss improved from 0.21074 to 0.19920, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 6s 226us/sample - loss: 0.2054 - categorical_accuracy: 0.9356 - val_loss: 0.1992 - val_categorical_accuracy: 0.9328\n",
      "Epoch 14/100\n",
      "24416/24527 [============================>.] - ETA: 0s - loss: 0.1964 - categorical_accuracy: 0.9368\n",
      "Epoch 00014: val_loss did not improve from 0.19920\n",
      "24527/24527 [==============================] - 5s 222us/sample - loss: 0.1966 - categorical_accuracy: 0.9367 - val_loss: 0.2194 - val_categorical_accuracy: 0.9308\n",
      "Epoch 15/100\n",
      "24480/24527 [============================>.] - ETA: 0s - loss: 0.1952 - categorical_accuracy: 0.9376\n",
      "Epoch 00015: val_loss improved from 0.19920 to 0.19713, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 6s 226us/sample - loss: 0.1953 - categorical_accuracy: 0.9376 - val_loss: 0.1971 - val_categorical_accuracy: 0.9390\n",
      "Epoch 16/100\n",
      "24448/24527 [============================>.] - ETA: 0s - loss: 0.1851 - categorical_accuracy: 0.9412\n",
      "Epoch 00016: val_loss did not improve from 0.19713\n",
      "24527/24527 [==============================] - 5s 221us/sample - loss: 0.1850 - categorical_accuracy: 0.9412 - val_loss: 0.2016 - val_categorical_accuracy: 0.9367\n",
      "Epoch 17/100\n",
      "24416/24527 [============================>.] - ETA: 0s - loss: 0.1796 - categorical_accuracy: 0.9435\n",
      "Epoch 00017: val_loss improved from 0.19713 to 0.19291, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 6s 225us/sample - loss: 0.1797 - categorical_accuracy: 0.9434 - val_loss: 0.1929 - val_categorical_accuracy: 0.9409\n",
      "Epoch 18/100\n",
      "24480/24527 [============================>.] - ETA: 0s - loss: 0.1787 - categorical_accuracy: 0.9444\n",
      "Epoch 00018: val_loss did not improve from 0.19291\n",
      "24527/24527 [==============================] - 5s 221us/sample - loss: 0.1786 - categorical_accuracy: 0.9444 - val_loss: 0.2141 - val_categorical_accuracy: 0.9321\n",
      "Epoch 19/100\n",
      "24320/24527 [============================>.] - ETA: 0s - loss: 0.1710 - categorical_accuracy: 0.9472\n",
      "Epoch 00019: val_loss did not improve from 0.19291\n",
      "24527/24527 [==============================] - 5s 218us/sample - loss: 0.1714 - categorical_accuracy: 0.9471 - val_loss: 0.1933 - val_categorical_accuracy: 0.9416\n",
      "Epoch 20/100\n",
      "24480/24527 [============================>.] - ETA: 0s - loss: 0.1692 - categorical_accuracy: 0.9467\n",
      "Epoch 00020: val_loss did not improve from 0.19291\n",
      "24527/24527 [==============================] - 5s 218us/sample - loss: 0.1694 - categorical_accuracy: 0.9466 - val_loss: 0.1942 - val_categorical_accuracy: 0.9419\n",
      "Epoch 21/100\n",
      "24480/24527 [============================>.] - ETA: 0s - loss: 0.1636 - categorical_accuracy: 0.9508\n",
      "Epoch 00021: val_loss did not improve from 0.19291\n",
      "24527/24527 [==============================] - 5s 220us/sample - loss: 0.1635 - categorical_accuracy: 0.9509 - val_loss: 0.1995 - val_categorical_accuracy: 0.9396\n",
      "Epoch 22/100\n",
      "24416/24527 [============================>.] - ETA: 0s - loss: 0.1585 - categorical_accuracy: 0.9523\n",
      "Epoch 00022: val_loss improved from 0.19291 to 0.18126, saving model to t_weights_0\n",
      "24527/24527 [==============================] - 6s 228us/sample - loss: 0.1585 - categorical_accuracy: 0.9523 - val_loss: 0.1813 - val_categorical_accuracy: 0.9475\n",
      "Epoch 23/100\n",
      "24352/24527 [============================>.] - ETA: 0s - loss: 0.1589 - categorical_accuracy: 0.9532\n",
      "Epoch 00023: val_loss did not improve from 0.18126\n",
      "24527/24527 [==============================] - 5s 222us/sample - loss: 0.1588 - categorical_accuracy: 0.9533 - val_loss: 0.1939 - val_categorical_accuracy: 0.9442\n",
      "Epoch 24/100\n",
      "24512/24527 [============================>.] - ETA: 0s - loss: 0.1542 - categorical_accuracy: 0.9545\n",
      "Epoch 00024: val_loss did not improve from 0.18126\n",
      "24527/24527 [==============================] - 5s 222us/sample - loss: 0.1541 - categorical_accuracy: 0.9545 - val_loss: 0.2082 - val_categorical_accuracy: 0.9436\n",
      "Epoch 25/100\n",
      "24480/24527 [============================>.] - ETA: 0s - loss: 0.1480 - categorical_accuracy: 0.9561\n",
      "Epoch 00025: val_loss did not improve from 0.18126\n",
      "24527/24527 [==============================] - 5s 222us/sample - loss: 0.1480 - categorical_accuracy: 0.9561 - val_loss: 0.2014 - val_categorical_accuracy: 0.9432\n",
      "Epoch 26/100\n",
      "24288/24527 [============================>.] - ETA: 0s - loss: 0.1504 - categorical_accuracy: 0.9549\n",
      "Epoch 00026: val_loss did not improve from 0.18126\n",
      "24527/24527 [==============================] - 5s 222us/sample - loss: 0.1498 - categorical_accuracy: 0.9551 - val_loss: 0.1913 - val_categorical_accuracy: 0.9511\n",
      "Epoch 27/100\n",
      "24416/24527 [============================>.] - ETA: 0s - loss: 0.1459 - categorical_accuracy: 0.9573\n",
      "Epoch 00027: val_loss did not improve from 0.18126\n",
      "24527/24527 [==============================] - 5s 223us/sample - loss: 0.1460 - categorical_accuracy: 0.9572 - val_loss: 0.1942 - val_categorical_accuracy: 0.9452\n",
      "0.9435563 0.7468866749688667\n",
      "\n",
      "\n",
      "nrx: 20 - real: 2 \n",
      "Train on 32375 samples, validate on 4046 samples\n",
      "Epoch 1/100\n",
      "32192/32375 [============================>.] - ETA: 0s - loss: 1.1840 - categorical_accuracy: 0.5100\n",
      "Epoch 00001: val_loss improved from inf to 0.70900, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 8s 233us/sample - loss: 1.1815 - categorical_accuracy: 0.5110 - val_loss: 0.7090 - val_categorical_accuracy: 0.7076\n",
      "Epoch 2/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.6985 - categorical_accuracy: 0.7135\n",
      "Epoch 00002: val_loss improved from 0.70900 to 0.52980, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 223us/sample - loss: 0.6979 - categorical_accuracy: 0.7138 - val_loss: 0.5298 - val_categorical_accuracy: 0.8028\n",
      "Epoch 3/100\n",
      "32256/32375 [============================>.] - ETA: 0s - loss: 0.5501 - categorical_accuracy: 0.7928\n",
      "Epoch 00003: val_loss improved from 0.52980 to 0.43877, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 218us/sample - loss: 0.5496 - categorical_accuracy: 0.7929 - val_loss: 0.4388 - val_categorical_accuracy: 0.8337\n",
      "Epoch 4/100\n",
      "32128/32375 [============================>.] - ETA: 0s - loss: 0.4323 - categorical_accuracy: 0.8491\n",
      "Epoch 00004: val_loss improved from 0.43877 to 0.31910, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 222us/sample - loss: 0.4324 - categorical_accuracy: 0.8493 - val_loss: 0.3191 - val_categorical_accuracy: 0.8930\n",
      "Epoch 5/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.3553 - categorical_accuracy: 0.8791\n",
      "Epoch 00005: val_loss improved from 0.31910 to 0.26440, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 223us/sample - loss: 0.3551 - categorical_accuracy: 0.8793 - val_loss: 0.2644 - val_categorical_accuracy: 0.9162\n",
      "Epoch 6/100\n",
      "32352/32375 [============================>.] - ETA: 0s - loss: 0.3054 - categorical_accuracy: 0.9011\n",
      "Epoch 00006: val_loss improved from 0.26440 to 0.23234, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 213us/sample - loss: 0.3054 - categorical_accuracy: 0.9011 - val_loss: 0.2323 - val_categorical_accuracy: 0.9214\n",
      "Epoch 7/100\n",
      "32256/32375 [============================>.] - ETA: 0s - loss: 0.2837 - categorical_accuracy: 0.9085\n",
      "Epoch 00007: val_loss did not improve from 0.23234\n",
      "32375/32375 [==============================] - 7s 222us/sample - loss: 0.2836 - categorical_accuracy: 0.9086 - val_loss: 0.2332 - val_categorical_accuracy: 0.9202\n",
      "Epoch 8/100\n",
      "32224/32375 [============================>.] - ETA: 0s - loss: 0.2625 - categorical_accuracy: 0.9149\n",
      "Epoch 00008: val_loss improved from 0.23234 to 0.22824, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 207us/sample - loss: 0.2622 - categorical_accuracy: 0.9151 - val_loss: 0.2282 - val_categorical_accuracy: 0.9286\n",
      "Epoch 9/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.2473 - categorical_accuracy: 0.9203\n",
      "Epoch 00009: val_loss improved from 0.22824 to 0.21524, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.2471 - categorical_accuracy: 0.9204 - val_loss: 0.2152 - val_categorical_accuracy: 0.9298\n",
      "Epoch 10/100\n",
      "32288/32375 [============================>.] - ETA: 0s - loss: 0.2373 - categorical_accuracy: 0.9223\n",
      "Epoch 00010: val_loss improved from 0.21524 to 0.21108, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.2375 - categorical_accuracy: 0.9223 - val_loss: 0.2111 - val_categorical_accuracy: 0.9325\n",
      "Epoch 11/100\n",
      "32160/32375 [============================>.] - ETA: 0s - loss: 0.2272 - categorical_accuracy: 0.9272\n",
      "Epoch 00011: val_loss improved from 0.21108 to 0.19578, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 215us/sample - loss: 0.2273 - categorical_accuracy: 0.9271 - val_loss: 0.1958 - val_categorical_accuracy: 0.9360\n",
      "Epoch 12/100\n",
      "32160/32375 [============================>.] - ETA: 0s - loss: 0.2212 - categorical_accuracy: 0.9286\n",
      "Epoch 00012: val_loss improved from 0.19578 to 0.19321, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 222us/sample - loss: 0.2208 - categorical_accuracy: 0.9287 - val_loss: 0.1932 - val_categorical_accuracy: 0.9380\n",
      "Epoch 13/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.2146 - categorical_accuracy: 0.9330\n",
      "Epoch 00013: val_loss improved from 0.19321 to 0.19055, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 220us/sample - loss: 0.2146 - categorical_accuracy: 0.9330 - val_loss: 0.1905 - val_categorical_accuracy: 0.9399\n",
      "Epoch 14/100\n",
      "32160/32375 [============================>.] - ETA: 0s - loss: 0.2097 - categorical_accuracy: 0.9339\n",
      "Epoch 00014: val_loss did not improve from 0.19055\n",
      "32375/32375 [==============================] - 7s 220us/sample - loss: 0.2098 - categorical_accuracy: 0.9337 - val_loss: 0.1909 - val_categorical_accuracy: 0.9394\n",
      "Epoch 15/100\n",
      "32192/32375 [============================>.] - ETA: 0s - loss: 0.2023 - categorical_accuracy: 0.9359\n",
      "Epoch 00015: val_loss did not improve from 0.19055\n",
      "32375/32375 [==============================] - 7s 213us/sample - loss: 0.2019 - categorical_accuracy: 0.9361 - val_loss: 0.1971 - val_categorical_accuracy: 0.9380\n",
      "Epoch 16/100\n",
      "32224/32375 [============================>.] - ETA: 0s - loss: 0.1952 - categorical_accuracy: 0.9392\n",
      "Epoch 00016: val_loss improved from 0.19055 to 0.18856, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1950 - categorical_accuracy: 0.9392 - val_loss: 0.1886 - val_categorical_accuracy: 0.9432\n",
      "Epoch 17/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.1920 - categorical_accuracy: 0.9399\n",
      "Epoch 00017: val_loss did not improve from 0.18856\n",
      "32375/32375 [==============================] - 7s 220us/sample - loss: 0.1921 - categorical_accuracy: 0.9399 - val_loss: 0.1929 - val_categorical_accuracy: 0.9449\n",
      "Epoch 18/100\n",
      "32192/32375 [============================>.] - ETA: 0s - loss: 0.1847 - categorical_accuracy: 0.9427\n",
      "Epoch 00018: val_loss did not improve from 0.18856\n",
      "32375/32375 [==============================] - 7s 220us/sample - loss: 0.1852 - categorical_accuracy: 0.9425 - val_loss: 0.1978 - val_categorical_accuracy: 0.9409\n",
      "Epoch 19/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.1829 - categorical_accuracy: 0.9429\n",
      "Epoch 00019: val_loss improved from 0.18856 to 0.18689, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 222us/sample - loss: 0.1829 - categorical_accuracy: 0.9430 - val_loss: 0.1869 - val_categorical_accuracy: 0.9417\n",
      "Epoch 20/100\n",
      "32256/32375 [============================>.] - ETA: 0s - loss: 0.1769 - categorical_accuracy: 0.9459\n",
      "Epoch 00020: val_loss improved from 0.18689 to 0.18318, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 216us/sample - loss: 0.1770 - categorical_accuracy: 0.9459 - val_loss: 0.1832 - val_categorical_accuracy: 0.9454\n",
      "Epoch 21/100\n",
      "32352/32375 [============================>.] - ETA: 0s - loss: 0.1744 - categorical_accuracy: 0.9469\n",
      "Epoch 00021: val_loss did not improve from 0.18318\n",
      "32375/32375 [==============================] - 7s 219us/sample - loss: 0.1743 - categorical_accuracy: 0.9469 - val_loss: 0.1844 - val_categorical_accuracy: 0.9481\n",
      "Epoch 22/100\n",
      "32256/32375 [============================>.] - ETA: 0s - loss: 0.1710 - categorical_accuracy: 0.9477\n",
      "Epoch 00022: val_loss improved from 0.18318 to 0.18200, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 223us/sample - loss: 0.1709 - categorical_accuracy: 0.9478 - val_loss: 0.1820 - val_categorical_accuracy: 0.9478\n",
      "Epoch 23/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.1689 - categorical_accuracy: 0.9482\n",
      "Epoch 00023: val_loss did not improve from 0.18200\n",
      "32375/32375 [==============================] - 7s 222us/sample - loss: 0.1691 - categorical_accuracy: 0.9483 - val_loss: 0.1846 - val_categorical_accuracy: 0.9461\n",
      "Epoch 24/100\n",
      "32224/32375 [============================>.] - ETA: 0s - loss: 0.1648 - categorical_accuracy: 0.9507\n",
      "Epoch 00024: val_loss improved from 0.18200 to 0.18199, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 223us/sample - loss: 0.1652 - categorical_accuracy: 0.9505 - val_loss: 0.1820 - val_categorical_accuracy: 0.9456\n",
      "Epoch 25/100\n",
      "32256/32375 [============================>.] - ETA: 0s - loss: 0.1632 - categorical_accuracy: 0.9507\n",
      "Epoch 00025: val_loss did not improve from 0.18199\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1635 - categorical_accuracy: 0.9506 - val_loss: 0.1904 - val_categorical_accuracy: 0.9456\n",
      "Epoch 26/100\n",
      "32192/32375 [============================>.] - ETA: 0s - loss: 0.1589 - categorical_accuracy: 0.9510\n",
      "Epoch 00026: val_loss did not improve from 0.18199\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1590 - categorical_accuracy: 0.9509 - val_loss: 0.1874 - val_categorical_accuracy: 0.9486\n",
      "Epoch 27/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.1563 - categorical_accuracy: 0.9530\n",
      "Epoch 00027: val_loss improved from 0.18199 to 0.17872, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 214us/sample - loss: 0.1563 - categorical_accuracy: 0.9530 - val_loss: 0.1787 - val_categorical_accuracy: 0.9483\n",
      "Epoch 28/100\n",
      "32256/32375 [============================>.] - ETA: 0s - loss: 0.1516 - categorical_accuracy: 0.9543\n",
      "Epoch 00028: val_loss did not improve from 0.17872\n",
      "32375/32375 [==============================] - 7s 212us/sample - loss: 0.1517 - categorical_accuracy: 0.9543 - val_loss: 0.2067 - val_categorical_accuracy: 0.9427\n",
      "Epoch 29/100\n",
      "32160/32375 [============================>.] - ETA: 0s - loss: 0.1495 - categorical_accuracy: 0.9553\n",
      "Epoch 00029: val_loss improved from 0.17872 to 0.17521, saving model to t_weights_0\n",
      "32375/32375 [==============================] - 7s 223us/sample - loss: 0.1491 - categorical_accuracy: 0.9555 - val_loss: 0.1752 - val_categorical_accuracy: 0.9516\n",
      "Epoch 30/100\n",
      "32352/32375 [============================>.] - ETA: 0s - loss: 0.1501 - categorical_accuracy: 0.9563\n",
      "Epoch 00030: val_loss did not improve from 0.17521\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1501 - categorical_accuracy: 0.9563 - val_loss: 0.2136 - val_categorical_accuracy: 0.9424\n",
      "Epoch 31/100\n",
      "32320/32375 [============================>.] - ETA: 0s - loss: 0.1449 - categorical_accuracy: 0.9565\n",
      "Epoch 00031: val_loss did not improve from 0.17521\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1451 - categorical_accuracy: 0.9564 - val_loss: 0.1854 - val_categorical_accuracy: 0.9483\n",
      "Epoch 32/100\n",
      "32224/32375 [============================>.] - ETA: 0s - loss: 0.1422 - categorical_accuracy: 0.9590\n",
      "Epoch 00032: val_loss did not improve from 0.17521\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1425 - categorical_accuracy: 0.9589 - val_loss: 0.1782 - val_categorical_accuracy: 0.9511\n",
      "Epoch 33/100\n",
      "32352/32375 [============================>.] - ETA: 0s - loss: 0.1402 - categorical_accuracy: 0.9593\n",
      "Epoch 00033: val_loss did not improve from 0.17521\n",
      "32375/32375 [==============================] - 7s 221us/sample - loss: 0.1402 - categorical_accuracy: 0.9593 - val_loss: 0.1765 - val_categorical_accuracy: 0.9501\n",
      "Epoch 34/100\n",
      "32096/32375 [============================>.] - ETA: 0s - loss: 0.1406 - categorical_accuracy: 0.9596\n",
      "Epoch 00034: val_loss did not improve from 0.17521\n",
      "32375/32375 [==============================] - 7s 222us/sample - loss: 0.1401 - categorical_accuracy: 0.9598 - val_loss: 0.1912 - val_categorical_accuracy: 0.9481\n",
      "0.94364804 0.7787463677874636\n",
      "\n",
      "\n",
      "nrx: 25 - real: 2 \n",
      "Train on 40215 samples, validate on 5026 samples\n",
      "Epoch 1/100\n",
      "40096/40215 [============================>.] - ETA: 0s - loss: 1.1341 - categorical_accuracy: 0.5369\n",
      "Epoch 00001: val_loss improved from inf to 0.65772, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 234us/sample - loss: 1.1330 - categorical_accuracy: 0.5373 - val_loss: 0.6577 - val_categorical_accuracy: 0.7527\n",
      "Epoch 2/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.6030 - categorical_accuracy: 0.7779\n",
      "Epoch 00002: val_loss improved from 0.65772 to 0.41607, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 217us/sample - loss: 0.6027 - categorical_accuracy: 0.7779 - val_loss: 0.4161 - val_categorical_accuracy: 0.8597\n",
      "Epoch 3/100\n",
      "40128/40215 [============================>.] - ETA: 0s - loss: 0.4176 - categorical_accuracy: 0.8620\n",
      "Epoch 00003: val_loss improved from 0.41607 to 0.30562, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 223us/sample - loss: 0.4173 - categorical_accuracy: 0.8621 - val_loss: 0.3056 - val_categorical_accuracy: 0.8991\n",
      "Epoch 4/100\n",
      "40160/40215 [============================>.] - ETA: 0s - loss: 0.3228 - categorical_accuracy: 0.8962\n",
      "Epoch 00004: val_loss improved from 0.30562 to 0.27329, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 225us/sample - loss: 0.3229 - categorical_accuracy: 0.8962 - val_loss: 0.2733 - val_categorical_accuracy: 0.9107\n",
      "Epoch 5/100\n",
      "40128/40215 [============================>.] - ETA: 0s - loss: 0.2844 - categorical_accuracy: 0.9116\n",
      "Epoch 00005: val_loss improved from 0.27329 to 0.27289, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 223us/sample - loss: 0.2844 - categorical_accuracy: 0.9116 - val_loss: 0.2729 - val_categorical_accuracy: 0.9103\n",
      "Epoch 6/100\n",
      "40192/40215 [============================>.] - ETA: 0s - loss: 0.2616 - categorical_accuracy: 0.9200\n",
      "Epoch 00006: val_loss improved from 0.27289 to 0.23978, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 224us/sample - loss: 0.2617 - categorical_accuracy: 0.9200 - val_loss: 0.2398 - val_categorical_accuracy: 0.9230\n",
      "Epoch 7/100\n",
      "39968/40215 [============================>.] - ETA: 0s - loss: 0.2441 - categorical_accuracy: 0.9245\n",
      "Epoch 00007: val_loss improved from 0.23978 to 0.23955, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 225us/sample - loss: 0.2442 - categorical_accuracy: 0.9244 - val_loss: 0.2395 - val_categorical_accuracy: 0.9248\n",
      "Epoch 8/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.2286 - categorical_accuracy: 0.9291\n",
      "Epoch 00008: val_loss did not improve from 0.23955\n",
      "40215/40215 [==============================] - 9s 221us/sample - loss: 0.2287 - categorical_accuracy: 0.9291 - val_loss: 0.2812 - val_categorical_accuracy: 0.9095\n",
      "Epoch 9/100\n",
      "40096/40215 [============================>.] - ETA: 0s - loss: 0.2183 - categorical_accuracy: 0.9332\n",
      "Epoch 00009: val_loss improved from 0.23955 to 0.23936, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 218us/sample - loss: 0.2183 - categorical_accuracy: 0.9331 - val_loss: 0.2394 - val_categorical_accuracy: 0.9284\n",
      "Epoch 10/100\n",
      "39968/40215 [============================>.] - ETA: 0s - loss: 0.2082 - categorical_accuracy: 0.9369\n",
      "Epoch 00010: val_loss improved from 0.23936 to 0.22049, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 224us/sample - loss: 0.2078 - categorical_accuracy: 0.9370 - val_loss: 0.2205 - val_categorical_accuracy: 0.9304\n",
      "Epoch 11/100\n",
      "40096/40215 [============================>.] - ETA: 0s - loss: 0.2023 - categorical_accuracy: 0.9371\n",
      "Epoch 00011: val_loss improved from 0.22049 to 0.21948, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 224us/sample - loss: 0.2025 - categorical_accuracy: 0.9371 - val_loss: 0.2195 - val_categorical_accuracy: 0.9322\n",
      "Epoch 12/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.1953 - categorical_accuracy: 0.9418\n",
      "Epoch 00012: val_loss improved from 0.21948 to 0.21899, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 224us/sample - loss: 0.1954 - categorical_accuracy: 0.9418 - val_loss: 0.2190 - val_categorical_accuracy: 0.9337\n",
      "Epoch 13/100\n",
      "40000/40215 [============================>.] - ETA: 0s - loss: 0.1872 - categorical_accuracy: 0.9439\n",
      "Epoch 00013: val_loss improved from 0.21899 to 0.21311, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 213us/sample - loss: 0.1872 - categorical_accuracy: 0.9439 - val_loss: 0.2131 - val_categorical_accuracy: 0.9347\n",
      "Epoch 14/100\n",
      "40096/40215 [============================>.] - ETA: 0s - loss: 0.1821 - categorical_accuracy: 0.9439\n",
      "Epoch 00014: val_loss did not improve from 0.21311\n",
      "40215/40215 [==============================] - 9s 221us/sample - loss: 0.1819 - categorical_accuracy: 0.9440 - val_loss: 0.2251 - val_categorical_accuracy: 0.9333\n",
      "Epoch 15/100\n",
      "40000/40215 [============================>.] - ETA: 0s - loss: 0.1766 - categorical_accuracy: 0.9470\n",
      "Epoch 00015: val_loss did not improve from 0.21311\n",
      "40215/40215 [==============================] - 9s 222us/sample - loss: 0.1770 - categorical_accuracy: 0.9469 - val_loss: 0.2203 - val_categorical_accuracy: 0.9329\n",
      "Epoch 16/100\n",
      "40160/40215 [============================>.] - ETA: 0s - loss: 0.1725 - categorical_accuracy: 0.9471\n",
      "Epoch 00016: val_loss improved from 0.21311 to 0.20760, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 216us/sample - loss: 0.1724 - categorical_accuracy: 0.9471 - val_loss: 0.2076 - val_categorical_accuracy: 0.9409\n",
      "Epoch 17/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.1704 - categorical_accuracy: 0.9496\n",
      "Epoch 00017: val_loss improved from 0.20760 to 0.20402, saving model to t_weights_0\n",
      "40215/40215 [==============================] - 9s 224us/sample - loss: 0.1705 - categorical_accuracy: 0.9496 - val_loss: 0.2040 - val_categorical_accuracy: 0.9421\n",
      "Epoch 18/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.1648 - categorical_accuracy: 0.9511\n",
      "Epoch 00018: val_loss did not improve from 0.20402\n",
      "40215/40215 [==============================] - 9s 223us/sample - loss: 0.1647 - categorical_accuracy: 0.9511 - val_loss: 0.2165 - val_categorical_accuracy: 0.9381\n",
      "Epoch 19/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.1613 - categorical_accuracy: 0.9525\n",
      "Epoch 00019: val_loss did not improve from 0.20402\n",
      "40215/40215 [==============================] - 9s 219us/sample - loss: 0.1613 - categorical_accuracy: 0.9525 - val_loss: 0.2126 - val_categorical_accuracy: 0.9419\n",
      "Epoch 20/100\n",
      "40032/40215 [============================>.] - ETA: 0s - loss: 0.1584 - categorical_accuracy: 0.9536\n",
      "Epoch 00020: val_loss did not improve from 0.20402\n",
      "40215/40215 [==============================] - 9s 222us/sample - loss: 0.1582 - categorical_accuracy: 0.9537 - val_loss: 0.2212 - val_categorical_accuracy: 0.9421\n",
      "Epoch 21/100\n",
      "40160/40215 [============================>.] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9544\n",
      "Epoch 00021: val_loss did not improve from 0.20402\n",
      "40215/40215 [==============================] - 9s 221us/sample - loss: 0.1552 - categorical_accuracy: 0.9543 - val_loss: 0.2138 - val_categorical_accuracy: 0.9419\n",
      "Epoch 22/100\n",
      "40128/40215 [============================>.] - ETA: 0s - loss: 0.1509 - categorical_accuracy: 0.9561\n",
      "Epoch 00022: val_loss did not improve from 0.20402\n",
      "40215/40215 [==============================] - 9s 216us/sample - loss: 0.1507 - categorical_accuracy: 0.9561 - val_loss: 0.2160 - val_categorical_accuracy: 0.9407\n",
      "0.94269794 0.8270029057700291\n",
      "\n",
      "\n",
      "nrx: 0 - real: 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/Documents/txid_framework/dataset_scripts/working/data_utilities.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cls_weights = np.max(stat,axis=0)/stat\n",
      "/home/samer/Documents/txid_framework/dataset_scripts/working/data_utilities.py:119: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  cls_weights = np.max(stat,axis=0)/stat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1440 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "1376/1440 [===========================>..] - ETA: 0s - loss: 2.2088 - categorical_accuracy: 0.2493\n",
      "Epoch 00001: val_loss improved from inf to 1.96212, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 1s 536us/sample - loss: 2.1999 - categorical_accuracy: 0.2535 - val_loss: 1.9621 - val_categorical_accuracy: 0.5333\n",
      "Epoch 2/100\n",
      "1376/1440 [===========================>..] - ETA: 0s - loss: 1.5695 - categorical_accuracy: 0.4891\n",
      "Epoch 00002: val_loss improved from 1.96212 to 0.97583, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 272us/sample - loss: 1.5510 - categorical_accuracy: 0.4917 - val_loss: 0.9758 - val_categorical_accuracy: 0.5444\n",
      "Epoch 3/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.9540 - categorical_accuracy: 0.6280\n",
      "Epoch 00003: val_loss improved from 0.97583 to 0.65258, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 272us/sample - loss: 0.9409 - categorical_accuracy: 0.6326 - val_loss: 0.6526 - val_categorical_accuracy: 0.7944\n",
      "Epoch 4/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.6660 - categorical_accuracy: 0.7454\n",
      "Epoch 00004: val_loss improved from 0.65258 to 0.44933, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 265us/sample - loss: 0.6582 - categorical_accuracy: 0.7507 - val_loss: 0.4493 - val_categorical_accuracy: 0.8889\n",
      "Epoch 5/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.4871 - categorical_accuracy: 0.8331\n",
      "Epoch 00005: val_loss improved from 0.44933 to 0.29225, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 250us/sample - loss: 0.4767 - categorical_accuracy: 0.8361 - val_loss: 0.2923 - val_categorical_accuracy: 0.9722\n",
      "Epoch 6/100\n",
      "1184/1440 [=======================>......] - ETA: 0s - loss: 0.3487 - categorical_accuracy: 0.8826\n",
      "Epoch 00006: val_loss improved from 0.29225 to 0.22074, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 248us/sample - loss: 0.3383 - categorical_accuracy: 0.8889 - val_loss: 0.2207 - val_categorical_accuracy: 0.9667\n",
      "Epoch 7/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.2261 - categorical_accuracy: 0.9430\n",
      "Epoch 00007: val_loss improved from 0.22074 to 0.13589, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 284us/sample - loss: 0.2235 - categorical_accuracy: 0.9431 - val_loss: 0.1359 - val_categorical_accuracy: 0.9889\n",
      "Epoch 8/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.1596 - categorical_accuracy: 0.9611\n",
      "Epoch 00008: val_loss improved from 0.13589 to 0.11601, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 270us/sample - loss: 0.1624 - categorical_accuracy: 0.9611 - val_loss: 0.1160 - val_categorical_accuracy: 0.9778\n",
      "Epoch 9/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.1342 - categorical_accuracy: 0.9703\n",
      "Epoch 00009: val_loss improved from 0.11601 to 0.09298, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 277us/sample - loss: 0.1310 - categorical_accuracy: 0.9708 - val_loss: 0.0930 - val_categorical_accuracy: 0.9778\n",
      "Epoch 10/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.1172 - categorical_accuracy: 0.9741\n",
      "Epoch 00010: val_loss improved from 0.09298 to 0.07946, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 282us/sample - loss: 0.1156 - categorical_accuracy: 0.9743 - val_loss: 0.0795 - val_categorical_accuracy: 0.9889\n",
      "Epoch 11/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.1060 - categorical_accuracy: 0.9766\n",
      "Epoch 00011: val_loss improved from 0.07946 to 0.07833, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 271us/sample - loss: 0.1023 - categorical_accuracy: 0.9778 - val_loss: 0.0783 - val_categorical_accuracy: 0.9889\n",
      "Epoch 12/100\n",
      "1184/1440 [=======================>......] - ETA: 0s - loss: 0.0770 - categorical_accuracy: 0.9848\n",
      "Epoch 00012: val_loss improved from 0.07833 to 0.05801, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 242us/sample - loss: 0.0801 - categorical_accuracy: 0.9840 - val_loss: 0.0580 - val_categorical_accuracy: 0.9722\n",
      "Epoch 13/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0758 - categorical_accuracy: 0.9848\n",
      "Epoch 00013: val_loss did not improve from 0.05801\n",
      "1440/1440 [==============================] - 0s 234us/sample - loss: 0.0759 - categorical_accuracy: 0.9840 - val_loss: 0.0723 - val_categorical_accuracy: 0.9889\n",
      "Epoch 14/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0645 - categorical_accuracy: 0.9893\n",
      "Epoch 00014: val_loss improved from 0.05801 to 0.04618, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 281us/sample - loss: 0.0643 - categorical_accuracy: 0.9896 - val_loss: 0.0462 - val_categorical_accuracy: 0.9889\n",
      "Epoch 15/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0633 - categorical_accuracy: 0.9878\n",
      "Epoch 00015: val_loss did not improve from 0.04618\n",
      "1440/1440 [==============================] - 0s 231us/sample - loss: 0.0639 - categorical_accuracy: 0.9875 - val_loss: 0.0535 - val_categorical_accuracy: 0.9833\n",
      "Epoch 16/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0502 - categorical_accuracy: 0.9916\n",
      "Epoch 00016: val_loss did not improve from 0.04618\n",
      "1440/1440 [==============================] - 0s 238us/sample - loss: 0.0504 - categorical_accuracy: 0.9917 - val_loss: 0.0499 - val_categorical_accuracy: 0.9889\n",
      "Epoch 17/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.0550 - categorical_accuracy: 0.9906\n",
      "Epoch 00017: val_loss improved from 0.04618 to 0.03766, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 285us/sample - loss: 0.0570 - categorical_accuracy: 0.9889 - val_loss: 0.0377 - val_categorical_accuracy: 0.9944\n",
      "Epoch 18/100\n",
      "1120/1440 [======================>.......] - ETA: 0s - loss: 0.0516 - categorical_accuracy: 0.9893\n",
      "Epoch 00018: val_loss improved from 0.03766 to 0.03581, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 245us/sample - loss: 0.0490 - categorical_accuracy: 0.9896 - val_loss: 0.0358 - val_categorical_accuracy: 0.9944\n",
      "Epoch 19/100\n",
      "1152/1440 [=======================>......] - ETA: 0s - loss: 0.0376 - categorical_accuracy: 0.9948\n",
      "Epoch 00019: val_loss did not improve from 0.03581\n",
      "1440/1440 [==============================] - 0s 210us/sample - loss: 0.0418 - categorical_accuracy: 0.9937 - val_loss: 0.0405 - val_categorical_accuracy: 0.9944\n",
      "Epoch 20/100\n",
      "1248/1440 [=========================>....] - ETA: 0s - loss: 0.0477 - categorical_accuracy: 0.9904\n",
      "Epoch 00020: val_loss improved from 0.03581 to 0.03554, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 290us/sample - loss: 0.0464 - categorical_accuracy: 0.9917 - val_loss: 0.0355 - val_categorical_accuracy: 0.9833\n",
      "Epoch 21/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.0332 - categorical_accuracy: 0.9969\n",
      "Epoch 00021: val_loss improved from 0.03554 to 0.03476, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 287us/sample - loss: 0.0353 - categorical_accuracy: 0.9958 - val_loss: 0.0348 - val_categorical_accuracy: 0.9944\n",
      "Epoch 22/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0405 - categorical_accuracy: 0.9924\n",
      "Epoch 00022: val_loss did not improve from 0.03476\n",
      "1440/1440 [==============================] - 0s 235us/sample - loss: 0.0389 - categorical_accuracy: 0.9931 - val_loss: 0.0376 - val_categorical_accuracy: 0.9889\n",
      "Epoch 23/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0349 - categorical_accuracy: 0.9931\n",
      "Epoch 00023: val_loss improved from 0.03476 to 0.02923, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 280us/sample - loss: 0.0343 - categorical_accuracy: 0.9937 - val_loss: 0.0292 - val_categorical_accuracy: 0.9944\n",
      "Epoch 24/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.0358 - categorical_accuracy: 0.9914\n",
      "Epoch 00024: val_loss did not improve from 0.02923\n",
      "1440/1440 [==============================] - 0s 234us/sample - loss: 0.0351 - categorical_accuracy: 0.9924 - val_loss: 0.0319 - val_categorical_accuracy: 0.9944\n",
      "Epoch 25/100\n",
      "1120/1440 [======================>.......] - ETA: 0s - loss: 0.0308 - categorical_accuracy: 0.9973\n",
      "Epoch 00025: val_loss improved from 0.02923 to 0.02919, saving model to t_weights_0\n",
      "1440/1440 [==============================] - 0s 241us/sample - loss: 0.0304 - categorical_accuracy: 0.9979 - val_loss: 0.0292 - val_categorical_accuracy: 0.9944\n",
      "Epoch 26/100\n",
      "1344/1440 [===========================>..] - ETA: 0s - loss: 0.0271 - categorical_accuracy: 0.9978\n",
      "Epoch 00026: val_loss did not improve from 0.02919\n",
      "1440/1440 [==============================] - 0s 226us/sample - loss: 0.0268 - categorical_accuracy: 0.9979 - val_loss: 0.0365 - val_categorical_accuracy: 0.9944\n",
      "Epoch 27/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0249 - categorical_accuracy: 0.9985\n",
      "Epoch 00027: val_loss did not improve from 0.02919\n",
      "1440/1440 [==============================] - 0s 236us/sample - loss: 0.0249 - categorical_accuracy: 0.9986 - val_loss: 0.0308 - val_categorical_accuracy: 0.9944\n",
      "Epoch 28/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0252 - categorical_accuracy: 0.9985\n",
      "Epoch 00028: val_loss did not improve from 0.02919\n",
      "1440/1440 [==============================] - 0s 234us/sample - loss: 0.0259 - categorical_accuracy: 0.9979 - val_loss: 0.0509 - val_categorical_accuracy: 0.9944\n",
      "Epoch 29/100\n",
      "1280/1440 [=========================>....] - ETA: 0s - loss: 0.0263 - categorical_accuracy: 0.9992\n",
      "Epoch 00029: val_loss did not improve from 0.02919\n",
      "1440/1440 [==============================] - 0s 238us/sample - loss: 0.0255 - categorical_accuracy: 0.9993 - val_loss: 0.0292 - val_categorical_accuracy: 0.9944\n",
      "Epoch 30/100\n",
      "1312/1440 [==========================>...] - ETA: 0s - loss: 0.0237 - categorical_accuracy: 0.9985\n",
      "Epoch 00030: val_loss did not improve from 0.02919\n",
      "1440/1440 [==============================] - 0s 234us/sample - loss: 0.0255 - categorical_accuracy: 0.9972 - val_loss: 0.0350 - val_categorical_accuracy: 0.9944\n",
      "0.98888886 0.28977980889073535\n",
      "\n",
      "\n",
      "nrx: 5 - real: 3 \n",
      "Train on 9280 samples, validate on 1160 samples\n",
      "Epoch 1/100\n",
      "9152/9280 [============================>.] - ETA: 0s - loss: 1.5042 - categorical_accuracy: 0.4455\n",
      "Epoch 00001: val_loss improved from inf to 0.70178, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 266us/sample - loss: 1.4965 - categorical_accuracy: 0.4477 - val_loss: 0.7018 - val_categorical_accuracy: 0.7043\n",
      "Epoch 2/100\n",
      "9216/9280 [============================>.] - ETA: 0s - loss: 0.7079 - categorical_accuracy: 0.7043\n",
      "Epoch 00002: val_loss improved from 0.70178 to 0.48378, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 230us/sample - loss: 0.7065 - categorical_accuracy: 0.7052 - val_loss: 0.4838 - val_categorical_accuracy: 0.8362\n",
      "Epoch 3/100\n",
      "9120/9280 [============================>.] - ETA: 0s - loss: 0.4972 - categorical_accuracy: 0.8009\n",
      "Epoch 00003: val_loss improved from 0.48378 to 0.33332, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 227us/sample - loss: 0.4972 - categorical_accuracy: 0.8012 - val_loss: 0.3333 - val_categorical_accuracy: 0.8776\n",
      "Epoch 4/100\n",
      "9248/9280 [============================>.] - ETA: 0s - loss: 0.3586 - categorical_accuracy: 0.8728\n",
      "Epoch 00004: val_loss improved from 0.33332 to 0.23305, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 218us/sample - loss: 0.3585 - categorical_accuracy: 0.8730 - val_loss: 0.2330 - val_categorical_accuracy: 0.9345\n",
      "Epoch 5/100\n",
      "9088/9280 [============================>.] - ETA: 0s - loss: 0.2755 - categorical_accuracy: 0.9093\n",
      "Epoch 00005: val_loss improved from 0.23305 to 0.20823, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 235us/sample - loss: 0.2752 - categorical_accuracy: 0.9092 - val_loss: 0.2082 - val_categorical_accuracy: 0.9190\n",
      "Epoch 6/100\n",
      "9120/9280 [============================>.] - ETA: 0s - loss: 0.2097 - categorical_accuracy: 0.9396\n",
      "Epoch 00006: val_loss improved from 0.20823 to 0.12831, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 229us/sample - loss: 0.2099 - categorical_accuracy: 0.9395 - val_loss: 0.1283 - val_categorical_accuracy: 0.9672\n",
      "Epoch 7/100\n",
      "9088/9280 [============================>.] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9602\n",
      "Epoch 00007: val_loss improved from 0.12831 to 0.11104, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 231us/sample - loss: 0.1550 - categorical_accuracy: 0.9602 - val_loss: 0.1110 - val_categorical_accuracy: 0.9793\n",
      "Epoch 8/100\n",
      "9120/9280 [============================>.] - ETA: 0s - loss: 0.1321 - categorical_accuracy: 0.9677\n",
      "Epoch 00008: val_loss improved from 0.11104 to 0.09196, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 230us/sample - loss: 0.1324 - categorical_accuracy: 0.9673 - val_loss: 0.0920 - val_categorical_accuracy: 0.9802\n",
      "Epoch 9/100\n",
      "9248/9280 [============================>.] - ETA: 0s - loss: 0.1145 - categorical_accuracy: 0.9737\n",
      "Epoch 00009: val_loss improved from 0.09196 to 0.07958, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 233us/sample - loss: 0.1149 - categorical_accuracy: 0.9737 - val_loss: 0.0796 - val_categorical_accuracy: 0.9836\n",
      "Epoch 10/100\n",
      "9152/9280 [============================>.] - ETA: 0s - loss: 0.1052 - categorical_accuracy: 0.9741\n",
      "Epoch 00010: val_loss improved from 0.07958 to 0.07544, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 232us/sample - loss: 0.1053 - categorical_accuracy: 0.9740 - val_loss: 0.0754 - val_categorical_accuracy: 0.9810\n",
      "Epoch 11/100\n",
      "9056/9280 [============================>.] - ETA: 0s - loss: 0.0911 - categorical_accuracy: 0.9797\n",
      "Epoch 00011: val_loss improved from 0.07544 to 0.07020, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 229us/sample - loss: 0.0916 - categorical_accuracy: 0.9798 - val_loss: 0.0702 - val_categorical_accuracy: 0.9879\n",
      "Epoch 12/100\n",
      "9248/9280 [============================>.] - ETA: 0s - loss: 0.0815 - categorical_accuracy: 0.9826\n",
      "Epoch 00012: val_loss improved from 0.07020 to 0.06059, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 233us/sample - loss: 0.0815 - categorical_accuracy: 0.9825 - val_loss: 0.0606 - val_categorical_accuracy: 0.9897\n",
      "Epoch 13/100\n",
      "9056/9280 [============================>.] - ETA: 0s - loss: 0.0811 - categorical_accuracy: 0.9827\n",
      "Epoch 00013: val_loss did not improve from 0.06059\n",
      "9280/9280 [==============================] - 2s 227us/sample - loss: 0.0817 - categorical_accuracy: 0.9827 - val_loss: 0.0616 - val_categorical_accuracy: 0.9897\n",
      "Epoch 14/100\n",
      "9152/9280 [============================>.] - ETA: 0s - loss: 0.0746 - categorical_accuracy: 0.9848\n",
      "Epoch 00014: val_loss improved from 0.06059 to 0.05886, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 231us/sample - loss: 0.0744 - categorical_accuracy: 0.9849 - val_loss: 0.0589 - val_categorical_accuracy: 0.9905\n",
      "Epoch 15/100\n",
      "9120/9280 [============================>.] - ETA: 0s - loss: 0.0664 - categorical_accuracy: 0.9875\n",
      "Epoch 00015: val_loss improved from 0.05886 to 0.05824, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 229us/sample - loss: 0.0668 - categorical_accuracy: 0.9874 - val_loss: 0.0582 - val_categorical_accuracy: 0.9922\n",
      "Epoch 16/100\n",
      "9024/9280 [============================>.] - ETA: 0s - loss: 0.0655 - categorical_accuracy: 0.9869\n",
      "Epoch 00016: val_loss did not improve from 0.05824\n",
      "9280/9280 [==============================] - 2s 221us/sample - loss: 0.0652 - categorical_accuracy: 0.9870 - val_loss: 0.0589 - val_categorical_accuracy: 0.9897\n",
      "Epoch 17/100\n",
      "9184/9280 [============================>.] - ETA: 0s - loss: 0.0669 - categorical_accuracy: 0.9872\n",
      "Epoch 00017: val_loss improved from 0.05824 to 0.04853, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 231us/sample - loss: 0.0667 - categorical_accuracy: 0.9872 - val_loss: 0.0485 - val_categorical_accuracy: 0.9922\n",
      "Epoch 18/100\n",
      "9088/9280 [============================>.] - ETA: 0s - loss: 0.0656 - categorical_accuracy: 0.9860\n",
      "Epoch 00018: val_loss did not improve from 0.04853\n",
      "9280/9280 [==============================] - 2s 223us/sample - loss: 0.0655 - categorical_accuracy: 0.9860 - val_loss: 0.0525 - val_categorical_accuracy: 0.9931\n",
      "Epoch 19/100\n",
      "9216/9280 [============================>.] - ETA: 0s - loss: 0.0568 - categorical_accuracy: 0.9908\n",
      "Epoch 00019: val_loss improved from 0.04853 to 0.04171, saving model to t_weights_0\n",
      "9280/9280 [==============================] - 2s 232us/sample - loss: 0.0567 - categorical_accuracy: 0.9908 - val_loss: 0.0417 - val_categorical_accuracy: 0.9957\n",
      "Epoch 20/100\n",
      "9056/9280 [============================>.] - ETA: 0s - loss: 0.0496 - categorical_accuracy: 0.9924\n",
      "Epoch 00020: val_loss did not improve from 0.04171\n",
      "9280/9280 [==============================] - 2s 227us/sample - loss: 0.0491 - categorical_accuracy: 0.9926 - val_loss: 0.0486 - val_categorical_accuracy: 0.9940\n",
      "Epoch 21/100\n",
      "9024/9280 [============================>.] - ETA: 0s - loss: 0.0460 - categorical_accuracy: 0.9930\n",
      "Epoch 00021: val_loss did not improve from 0.04171\n",
      "9280/9280 [==============================] - 2s 216us/sample - loss: 0.0460 - categorical_accuracy: 0.9930 - val_loss: 0.0530 - val_categorical_accuracy: 0.9914\n",
      "Epoch 22/100\n",
      "9056/9280 [============================>.] - ETA: 0s - loss: 0.0496 - categorical_accuracy: 0.9906\n",
      "Epoch 00022: val_loss did not improve from 0.04171\n",
      "9280/9280 [==============================] - 2s 205us/sample - loss: 0.0496 - categorical_accuracy: 0.9907 - val_loss: 0.0544 - val_categorical_accuracy: 0.9905\n",
      "Epoch 23/100\n",
      "9184/9280 [============================>.] - ETA: 0s - loss: 0.0458 - categorical_accuracy: 0.9938\n",
      "Epoch 00023: val_loss did not improve from 0.04171\n",
      "9280/9280 [==============================] - 2s 222us/sample - loss: 0.0458 - categorical_accuracy: 0.9939 - val_loss: 0.0557 - val_categorical_accuracy: 0.9914\n",
      "Epoch 24/100\n",
      "9152/9280 [============================>.] - ETA: 0s - loss: 0.0484 - categorical_accuracy: 0.9930\n",
      "Epoch 00024: val_loss did not improve from 0.04171\n",
      "9280/9280 [==============================] - 2s 223us/sample - loss: 0.0488 - categorical_accuracy: 0.9929 - val_loss: 0.0443 - val_categorical_accuracy: 0.9940\n",
      "0.98706895 0.44869131699210635\n",
      "\n",
      "\n",
      "nrx: 10 - real: 3 \n",
      "Train on 16706 samples, validate on 2088 samples\n",
      "Epoch 1/100\n",
      "16576/16706 [============================>.] - ETA: 0s - loss: 1.3241 - categorical_accuracy: 0.4751\n",
      "Epoch 00001: val_loss improved from inf to 0.68948, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 252us/sample - loss: 1.3189 - categorical_accuracy: 0.4773 - val_loss: 0.6895 - val_categorical_accuracy: 0.7189\n",
      "Epoch 2/100\n",
      "16608/16706 [============================>.] - ETA: 0s - loss: 0.6761 - categorical_accuracy: 0.7264\n",
      "Epoch 00002: val_loss improved from 0.68948 to 0.51835, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 225us/sample - loss: 0.6757 - categorical_accuracy: 0.7266 - val_loss: 0.5184 - val_categorical_accuracy: 0.8132\n",
      "Epoch 3/100\n",
      "16608/16706 [============================>.] - ETA: 0s - loss: 0.5003 - categorical_accuracy: 0.8215\n",
      "Epoch 00003: val_loss improved from 0.51835 to 0.38530, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 231us/sample - loss: 0.5003 - categorical_accuracy: 0.8216 - val_loss: 0.3853 - val_categorical_accuracy: 0.8846\n",
      "Epoch 4/100\n",
      "16512/16706 [============================>.] - ETA: 0s - loss: 0.3633 - categorical_accuracy: 0.8861\n",
      "Epoch 00004: val_loss improved from 0.38530 to 0.26650, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 221us/sample - loss: 0.3622 - categorical_accuracy: 0.8866 - val_loss: 0.2665 - val_categorical_accuracy: 0.9282\n",
      "Epoch 5/100\n",
      "16608/16706 [============================>.] - ETA: 0s - loss: 0.2738 - categorical_accuracy: 0.9174\n",
      "Epoch 00005: val_loss improved from 0.26650 to 0.21669, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 232us/sample - loss: 0.2735 - categorical_accuracy: 0.9176 - val_loss: 0.2167 - val_categorical_accuracy: 0.9401\n",
      "Epoch 6/100\n",
      "16544/16706 [============================>.] - ETA: 0s - loss: 0.2241 - categorical_accuracy: 0.9356\n",
      "Epoch 00006: val_loss improved from 0.21669 to 0.18251, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 223us/sample - loss: 0.2236 - categorical_accuracy: 0.9358 - val_loss: 0.1825 - val_categorical_accuracy: 0.9473\n",
      "Epoch 7/100\n",
      "16544/16706 [============================>.] - ETA: 0s - loss: 0.2016 - categorical_accuracy: 0.9414\n",
      "Epoch 00007: val_loss improved from 0.18251 to 0.17122, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 229us/sample - loss: 0.2017 - categorical_accuracy: 0.9413 - val_loss: 0.1712 - val_categorical_accuracy: 0.9492\n",
      "Epoch 8/100\n",
      "16544/16706 [============================>.] - ETA: 0s - loss: 0.1777 - categorical_accuracy: 0.9510\n",
      "Epoch 00008: val_loss improved from 0.17122 to 0.16504, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 227us/sample - loss: 0.1776 - categorical_accuracy: 0.9510 - val_loss: 0.1650 - val_categorical_accuracy: 0.9511\n",
      "Epoch 9/100\n",
      "16544/16706 [============================>.] - ETA: 0s - loss: 0.1625 - categorical_accuracy: 0.9548\n",
      "Epoch 00009: val_loss improved from 0.16504 to 0.16039, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 225us/sample - loss: 0.1629 - categorical_accuracy: 0.9547 - val_loss: 0.1604 - val_categorical_accuracy: 0.9535\n",
      "Epoch 10/100\n",
      "16512/16706 [============================>.] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9571\n",
      "Epoch 00010: val_loss improved from 0.16039 to 0.13706, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 228us/sample - loss: 0.1546 - categorical_accuracy: 0.9573 - val_loss: 0.1371 - val_categorical_accuracy: 0.9598\n",
      "Epoch 11/100\n",
      "16640/16706 [============================>.] - ETA: 0s - loss: 0.1410 - categorical_accuracy: 0.9631\n",
      "Epoch 00011: val_loss improved from 0.13706 to 0.13522, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 226us/sample - loss: 0.1412 - categorical_accuracy: 0.9630 - val_loss: 0.1352 - val_categorical_accuracy: 0.9631\n",
      "Epoch 12/100\n",
      "16576/16706 [============================>.] - ETA: 0s - loss: 0.1325 - categorical_accuracy: 0.9657\n",
      "Epoch 00012: val_loss did not improve from 0.13522\n",
      "16706/16706 [==============================] - 4s 224us/sample - loss: 0.1324 - categorical_accuracy: 0.9657 - val_loss: 0.1357 - val_categorical_accuracy: 0.9617\n",
      "Epoch 13/100\n",
      "16576/16706 [============================>.] - ETA: 0s - loss: 0.1313 - categorical_accuracy: 0.9644\n",
      "Epoch 00013: val_loss did not improve from 0.13522\n",
      "16706/16706 [==============================] - 4s 221us/sample - loss: 0.1307 - categorical_accuracy: 0.9647 - val_loss: 0.1874 - val_categorical_accuracy: 0.9373\n",
      "Epoch 14/100\n",
      "16480/16706 [============================>.] - ETA: 0s - loss: 0.1262 - categorical_accuracy: 0.9667\n",
      "Epoch 00014: val_loss did not improve from 0.13522\n",
      "16706/16706 [==============================] - 3s 192us/sample - loss: 0.1261 - categorical_accuracy: 0.9667 - val_loss: 0.1436 - val_categorical_accuracy: 0.9588\n",
      "Epoch 15/100\n",
      "16608/16706 [============================>.] - ETA: 0s - loss: 0.1237 - categorical_accuracy: 0.9671\n",
      "Epoch 00015: val_loss improved from 0.13522 to 0.13182, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 219us/sample - loss: 0.1235 - categorical_accuracy: 0.9671 - val_loss: 0.1318 - val_categorical_accuracy: 0.9655\n",
      "Epoch 16/100\n",
      "16704/16706 [============================>.] - ETA: 0s - loss: 0.1162 - categorical_accuracy: 0.9692\n",
      "Epoch 00016: val_loss improved from 0.13182 to 0.12473, saving model to t_weights_0\n",
      "16706/16706 [==============================] - 4s 229us/sample - loss: 0.1163 - categorical_accuracy: 0.9691 - val_loss: 0.1247 - val_categorical_accuracy: 0.9670\n",
      "Epoch 17/100\n",
      "16672/16706 [============================>.] - ETA: 0s - loss: 0.1199 - categorical_accuracy: 0.9677\n",
      "Epoch 00017: val_loss did not improve from 0.12473\n",
      "16706/16706 [==============================] - 4s 217us/sample - loss: 0.1198 - categorical_accuracy: 0.9678 - val_loss: 0.1324 - val_categorical_accuracy: 0.9636\n",
      "Epoch 18/100\n",
      "16704/16706 [============================>.] - ETA: 0s - loss: 0.1019 - categorical_accuracy: 0.9736\n",
      "Epoch 00018: val_loss did not improve from 0.12473\n",
      "16706/16706 [==============================] - 4s 226us/sample - loss: 0.1018 - categorical_accuracy: 0.9736 - val_loss: 0.1395 - val_categorical_accuracy: 0.9622\n",
      "Epoch 19/100\n",
      "16512/16706 [============================>.] - ETA: 0s - loss: 0.1022 - categorical_accuracy: 0.9743\n",
      "Epoch 00019: val_loss did not improve from 0.12473\n",
      "16706/16706 [==============================] - 4s 220us/sample - loss: 0.1023 - categorical_accuracy: 0.9742 - val_loss: 0.1324 - val_categorical_accuracy: 0.9670\n",
      "Epoch 20/100\n",
      "16576/16706 [============================>.] - ETA: 0s - loss: 0.1034 - categorical_accuracy: 0.9739\n",
      "Epoch 00020: val_loss did not improve from 0.12473\n",
      "16706/16706 [==============================] - 4s 225us/sample - loss: 0.1030 - categorical_accuracy: 0.9740 - val_loss: 0.1352 - val_categorical_accuracy: 0.9660\n",
      "Epoch 21/100\n",
      "16640/16706 [============================>.] - ETA: 0s - loss: 0.0962 - categorical_accuracy: 0.9758\n",
      "Epoch 00021: val_loss did not improve from 0.12473\n",
      "16706/16706 [==============================] - 4s 221us/sample - loss: 0.0964 - categorical_accuracy: 0.9758 - val_loss: 0.1587 - val_categorical_accuracy: 0.9593\n",
      "0.9722222 0.5234732031574574\n",
      "\n",
      "\n",
      "nrx: 15 - real: 3 \n",
      "Train on 24682 samples, validate on 3085 samples\n",
      "Epoch 1/100\n",
      "24480/24682 [============================>.] - ETA: 0s - loss: 1.2950 - categorical_accuracy: 0.4839\n",
      "Epoch 00001: val_loss improved from inf to 0.73585, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 242us/sample - loss: 1.2912 - categorical_accuracy: 0.4855 - val_loss: 0.7359 - val_categorical_accuracy: 0.7135\n",
      "Epoch 2/100\n",
      "24480/24682 [============================>.] - ETA: 0s - loss: 0.7155 - categorical_accuracy: 0.7134\n",
      "Epoch 00002: val_loss improved from 0.73585 to 0.46438, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 223us/sample - loss: 0.7146 - categorical_accuracy: 0.7140 - val_loss: 0.4644 - val_categorical_accuracy: 0.8486\n",
      "Epoch 3/100\n",
      "24640/24682 [============================>.] - ETA: 0s - loss: 0.4990 - categorical_accuracy: 0.8271\n",
      "Epoch 00003: val_loss improved from 0.46438 to 0.33809, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 224us/sample - loss: 0.4989 - categorical_accuracy: 0.8271 - val_loss: 0.3381 - val_categorical_accuracy: 0.8959\n",
      "Epoch 4/100\n",
      "24672/24682 [============================>.] - ETA: 0s - loss: 0.3550 - categorical_accuracy: 0.8868\n",
      "Epoch 00004: val_loss improved from 0.33809 to 0.24498, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 223us/sample - loss: 0.3549 - categorical_accuracy: 0.8868 - val_loss: 0.2450 - val_categorical_accuracy: 0.9293\n",
      "Epoch 5/100\n",
      "24672/24682 [============================>.] - ETA: 0s - loss: 0.2911 - categorical_accuracy: 0.9084\n",
      "Epoch 00005: val_loss improved from 0.24498 to 0.23208, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 225us/sample - loss: 0.2917 - categorical_accuracy: 0.9083 - val_loss: 0.2321 - val_categorical_accuracy: 0.9313\n",
      "Epoch 6/100\n",
      "24544/24682 [============================>.] - ETA: 0s - loss: 0.2581 - categorical_accuracy: 0.9212\n",
      "Epoch 00006: val_loss improved from 0.23208 to 0.21751, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 5s 218us/sample - loss: 0.2582 - categorical_accuracy: 0.9211 - val_loss: 0.2175 - val_categorical_accuracy: 0.9352\n",
      "Epoch 7/100\n",
      "24512/24682 [============================>.] - ETA: 0s - loss: 0.2296 - categorical_accuracy: 0.9306\n",
      "Epoch 00007: val_loss improved from 0.21751 to 0.20712, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 5s 219us/sample - loss: 0.2292 - categorical_accuracy: 0.9307 - val_loss: 0.2071 - val_categorical_accuracy: 0.9355\n",
      "Epoch 8/100\n",
      "24416/24682 [============================>.] - ETA: 0s - loss: 0.2199 - categorical_accuracy: 0.9336\n",
      "Epoch 00008: val_loss improved from 0.20712 to 0.18016, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 223us/sample - loss: 0.2197 - categorical_accuracy: 0.9337 - val_loss: 0.1802 - val_categorical_accuracy: 0.9459\n",
      "Epoch 9/100\n",
      "24672/24682 [============================>.] - ETA: 0s - loss: 0.2112 - categorical_accuracy: 0.9357\n",
      "Epoch 00009: val_loss did not improve from 0.18016\n",
      "24682/24682 [==============================] - 5s 221us/sample - loss: 0.2112 - categorical_accuracy: 0.9357 - val_loss: 0.1903 - val_categorical_accuracy: 0.9452\n",
      "Epoch 10/100\n",
      "24608/24682 [============================>.] - ETA: 0s - loss: 0.1942 - categorical_accuracy: 0.9421\n",
      "Epoch 00010: val_loss did not improve from 0.18016\n",
      "24682/24682 [==============================] - 5s 222us/sample - loss: 0.1942 - categorical_accuracy: 0.9421 - val_loss: 0.2010 - val_categorical_accuracy: 0.9400\n",
      "Epoch 11/100\n",
      "24544/24682 [============================>.] - ETA: 0s - loss: 0.1913 - categorical_accuracy: 0.9421\n",
      "Epoch 00011: val_loss did not improve from 0.18016\n",
      "24682/24682 [==============================] - 5s 222us/sample - loss: 0.1909 - categorical_accuracy: 0.9423 - val_loss: 0.1836 - val_categorical_accuracy: 0.9491\n",
      "Epoch 12/100\n",
      "24512/24682 [============================>.] - ETA: 0s - loss: 0.1804 - categorical_accuracy: 0.9470\n",
      "Epoch 00012: val_loss improved from 0.18016 to 0.17939, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 5s 223us/sample - loss: 0.1809 - categorical_accuracy: 0.9468 - val_loss: 0.1794 - val_categorical_accuracy: 0.9514\n",
      "Epoch 13/100\n",
      "24576/24682 [============================>.] - ETA: 0s - loss: 0.1721 - categorical_accuracy: 0.9486\n",
      "Epoch 00013: val_loss improved from 0.17939 to 0.17186, saving model to t_weights_0\n",
      "24682/24682 [==============================] - 6s 227us/sample - loss: 0.1721 - categorical_accuracy: 0.9486 - val_loss: 0.1719 - val_categorical_accuracy: 0.9536\n",
      "Epoch 14/100\n",
      "24640/24682 [============================>.] - ETA: 0s - loss: 0.1748 - categorical_accuracy: 0.9476\n",
      "Epoch 00014: val_loss did not improve from 0.17186\n",
      "24682/24682 [==============================] - 5s 222us/sample - loss: 0.1747 - categorical_accuracy: 0.9476 - val_loss: 0.1748 - val_categorical_accuracy: 0.9540\n",
      "Epoch 15/100\n",
      "24544/24682 [============================>.] - ETA: 0s - loss: 0.1642 - categorical_accuracy: 0.9517\n",
      "Epoch 00015: val_loss did not improve from 0.17186\n",
      "24682/24682 [==============================] - 5s 221us/sample - loss: 0.1642 - categorical_accuracy: 0.9517 - val_loss: 0.1865 - val_categorical_accuracy: 0.9517\n",
      "Epoch 16/100\n",
      "24480/24682 [============================>.] - ETA: 0s - loss: 0.1610 - categorical_accuracy: 0.9529\n",
      "Epoch 00016: val_loss did not improve from 0.17186\n",
      "24682/24682 [==============================] - 5s 218us/sample - loss: 0.1611 - categorical_accuracy: 0.9528 - val_loss: 0.1955 - val_categorical_accuracy: 0.9452\n",
      "Epoch 17/100\n",
      "24576/24682 [============================>.] - ETA: 0s - loss: 0.1574 - categorical_accuracy: 0.9535\n",
      "Epoch 00017: val_loss did not improve from 0.17186\n",
      "24682/24682 [==============================] - 5s 217us/sample - loss: 0.1573 - categorical_accuracy: 0.9536 - val_loss: 0.1932 - val_categorical_accuracy: 0.9494\n",
      "Epoch 18/100\n",
      "24640/24682 [============================>.] - ETA: 0s - loss: 0.1523 - categorical_accuracy: 0.9558\n",
      "Epoch 00018: val_loss did not improve from 0.17186\n",
      "24682/24682 [==============================] - 5s 214us/sample - loss: 0.1523 - categorical_accuracy: 0.9558 - val_loss: 0.1774 - val_categorical_accuracy: 0.9556\n",
      "0.95170176 0.6456169505608641\n",
      "\n",
      "\n",
      "nrx: 20 - real: 3 \n",
      "Train on 32221 samples, validate on 4027 samples\n",
      "Epoch 1/100\n",
      "32096/32221 [============================>.] - ETA: 0s - loss: 1.1568 - categorical_accuracy: 0.5272\n",
      "Epoch 00001: val_loss improved from inf to 0.64784, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 8s 236us/sample - loss: 1.1554 - categorical_accuracy: 0.5276 - val_loss: 0.6478 - val_categorical_accuracy: 0.7358\n",
      "Epoch 2/100\n",
      "32128/32221 [============================>.] - ETA: 0s - loss: 0.6117 - categorical_accuracy: 0.7576\n",
      "Epoch 00002: val_loss improved from 0.64784 to 0.42925, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 225us/sample - loss: 0.6115 - categorical_accuracy: 0.7578 - val_loss: 0.4293 - val_categorical_accuracy: 0.8339\n",
      "Epoch 3/100\n",
      "32032/32221 [============================>.] - ETA: 0s - loss: 0.4457 - categorical_accuracy: 0.8438\n",
      "Epoch 00003: val_loss improved from 0.42925 to 0.31584, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 225us/sample - loss: 0.4462 - categorical_accuracy: 0.8435 - val_loss: 0.3158 - val_categorical_accuracy: 0.8994\n",
      "Epoch 4/100\n",
      "32128/32221 [============================>.] - ETA: 0s - loss: 0.3583 - categorical_accuracy: 0.8839\n",
      "Epoch 00004: val_loss improved from 0.31584 to 0.26051, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 226us/sample - loss: 0.3581 - categorical_accuracy: 0.8840 - val_loss: 0.2605 - val_categorical_accuracy: 0.9163\n",
      "Epoch 5/100\n",
      "31904/32221 [============================>.] - ETA: 0s - loss: 0.3057 - categorical_accuracy: 0.9052\n",
      "Epoch 00005: val_loss improved from 0.26051 to 0.23046, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 223us/sample - loss: 0.3057 - categorical_accuracy: 0.9053 - val_loss: 0.2305 - val_categorical_accuracy: 0.9280\n",
      "Epoch 6/100\n",
      "32032/32221 [============================>.] - ETA: 0s - loss: 0.2620 - categorical_accuracy: 0.9190\n",
      "Epoch 00006: val_loss improved from 0.23046 to 0.22717, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 224us/sample - loss: 0.2623 - categorical_accuracy: 0.9190 - val_loss: 0.2272 - val_categorical_accuracy: 0.9262\n",
      "Epoch 7/100\n",
      "32032/32221 [============================>.] - ETA: 0s - loss: 0.2381 - categorical_accuracy: 0.9274\n",
      "Epoch 00007: val_loss improved from 0.22717 to 0.19020, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 222us/sample - loss: 0.2377 - categorical_accuracy: 0.9276 - val_loss: 0.1902 - val_categorical_accuracy: 0.9419\n",
      "Epoch 8/100\n",
      "32192/32221 [============================>.] - ETA: 0s - loss: 0.2216 - categorical_accuracy: 0.9324\n",
      "Epoch 00008: val_loss did not improve from 0.19020\n",
      "32221/32221 [==============================] - 7s 218us/sample - loss: 0.2216 - categorical_accuracy: 0.9324 - val_loss: 0.2022 - val_categorical_accuracy: 0.9374\n",
      "Epoch 9/100\n",
      "32032/32221 [============================>.] - ETA: 0s - loss: 0.2068 - categorical_accuracy: 0.9380\n",
      "Epoch 00009: val_loss improved from 0.19020 to 0.17961, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 226us/sample - loss: 0.2072 - categorical_accuracy: 0.9379 - val_loss: 0.1796 - val_categorical_accuracy: 0.9444\n",
      "Epoch 10/100\n",
      "32032/32221 [============================>.] - ETA: 0s - loss: 0.1987 - categorical_accuracy: 0.9406\n",
      "Epoch 00010: val_loss improved from 0.17961 to 0.17690, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 226us/sample - loss: 0.1986 - categorical_accuracy: 0.9406 - val_loss: 0.1769 - val_categorical_accuracy: 0.9481\n",
      "Epoch 11/100\n",
      "32064/32221 [============================>.] - ETA: 0s - loss: 0.1877 - categorical_accuracy: 0.9448\n",
      "Epoch 00011: val_loss improved from 0.17690 to 0.17660, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 226us/sample - loss: 0.1876 - categorical_accuracy: 0.9448 - val_loss: 0.1766 - val_categorical_accuracy: 0.9488\n",
      "Epoch 12/100\n",
      "32000/32221 [============================>.] - ETA: 0s - loss: 0.1755 - categorical_accuracy: 0.9486\n",
      "Epoch 00012: val_loss did not improve from 0.17660\n",
      "32221/32221 [==============================] - 7s 222us/sample - loss: 0.1755 - categorical_accuracy: 0.9486 - val_loss: 0.2139 - val_categorical_accuracy: 0.9392\n",
      "Epoch 13/100\n",
      "31968/32221 [============================>.] - ETA: 0s - loss: 0.1711 - categorical_accuracy: 0.9510\n",
      "Epoch 00013: val_loss improved from 0.17660 to 0.16706, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 225us/sample - loss: 0.1710 - categorical_accuracy: 0.9510 - val_loss: 0.1671 - val_categorical_accuracy: 0.9536\n",
      "Epoch 14/100\n",
      "32192/32221 [============================>.] - ETA: 0s - loss: 0.1634 - categorical_accuracy: 0.9522\n",
      "Epoch 00014: val_loss did not improve from 0.16706\n",
      "32221/32221 [==============================] - 7s 223us/sample - loss: 0.1633 - categorical_accuracy: 0.9522 - val_loss: 0.1924 - val_categorical_accuracy: 0.9399\n",
      "Epoch 15/100\n",
      "32000/32221 [============================>.] - ETA: 0s - loss: 0.1621 - categorical_accuracy: 0.9538\n",
      "Epoch 00015: val_loss did not improve from 0.16706\n",
      "32221/32221 [==============================] - 7s 224us/sample - loss: 0.1624 - categorical_accuracy: 0.9537 - val_loss: 0.2021 - val_categorical_accuracy: 0.9392\n",
      "Epoch 16/100\n",
      "32096/32221 [============================>.] - ETA: 0s - loss: 0.1571 - categorical_accuracy: 0.9556\n",
      "Epoch 00016: val_loss improved from 0.16706 to 0.16635, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 219us/sample - loss: 0.1568 - categorical_accuracy: 0.9557 - val_loss: 0.1663 - val_categorical_accuracy: 0.9551\n",
      "Epoch 17/100\n",
      "32160/32221 [============================>.] - ETA: 0s - loss: 0.1507 - categorical_accuracy: 0.9567\n",
      "Epoch 00017: val_loss improved from 0.16635 to 0.16086, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 226us/sample - loss: 0.1506 - categorical_accuracy: 0.9567 - val_loss: 0.1609 - val_categorical_accuracy: 0.9546\n",
      "Epoch 18/100\n",
      "31968/32221 [============================>.] - ETA: 0s - loss: 0.1467 - categorical_accuracy: 0.9581\n",
      "Epoch 00018: val_loss improved from 0.16086 to 0.15311, saving model to t_weights_0\n",
      "32221/32221 [==============================] - 7s 225us/sample - loss: 0.1468 - categorical_accuracy: 0.9581 - val_loss: 0.1531 - val_categorical_accuracy: 0.9560\n",
      "Epoch 19/100\n",
      "32000/32221 [============================>.] - ETA: 0s - loss: 0.1458 - categorical_accuracy: 0.9587\n",
      "Epoch 00019: val_loss did not improve from 0.15311\n",
      "32221/32221 [==============================] - 7s 224us/sample - loss: 0.1459 - categorical_accuracy: 0.9587 - val_loss: 0.1791 - val_categorical_accuracy: 0.9508\n",
      "Epoch 20/100\n",
      "32128/32221 [============================>.] - ETA: 0s - loss: 0.1387 - categorical_accuracy: 0.9610\n",
      "Epoch 00020: val_loss did not improve from 0.15311\n",
      "32221/32221 [==============================] - 7s 223us/sample - loss: 0.1387 - categorical_accuracy: 0.9610 - val_loss: 0.1593 - val_categorical_accuracy: 0.9563\n",
      "Epoch 21/100\n",
      "32128/32221 [============================>.] - ETA: 0s - loss: 0.1355 - categorical_accuracy: 0.9625\n",
      "Epoch 00021: val_loss did not improve from 0.15311\n",
      "32221/32221 [==============================] - 7s 215us/sample - loss: 0.1357 - categorical_accuracy: 0.9624 - val_loss: 0.1614 - val_categorical_accuracy: 0.9578\n",
      "Epoch 22/100\n",
      "32064/32221 [============================>.] - ETA: 0s - loss: 0.1321 - categorical_accuracy: 0.9634\n",
      "Epoch 00022: val_loss did not improve from 0.15311\n",
      "32221/32221 [==============================] - 7s 218us/sample - loss: 0.1328 - categorical_accuracy: 0.9633 - val_loss: 0.1784 - val_categorical_accuracy: 0.9483\n",
      "Epoch 23/100\n",
      "32000/32221 [============================>.] - ETA: 0s - loss: 0.1304 - categorical_accuracy: 0.9646\n",
      "Epoch 00023: val_loss did not improve from 0.15311\n",
      "32221/32221 [==============================] - 7s 214us/sample - loss: 0.1301 - categorical_accuracy: 0.9647 - val_loss: 0.1617 - val_categorical_accuracy: 0.9578\n",
      "0.9557984 0.7226838388034899\n",
      "\n",
      "\n",
      "nrx: 25 - real: 3 \n",
      "Train on 40221 samples, validate on 5027 samples\n",
      "Epoch 1/100\n",
      "40096/40221 [============================>.] - ETA: 0s - loss: 1.0419 - categorical_accuracy: 0.5639\n",
      "Epoch 00001: val_loss improved from inf to 0.59346, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 230us/sample - loss: 1.0409 - categorical_accuracy: 0.5644 - val_loss: 0.5935 - val_categorical_accuracy: 0.7639\n",
      "Epoch 2/100\n",
      "40096/40221 [============================>.] - ETA: 0s - loss: 0.5389 - categorical_accuracy: 0.7985\n",
      "Epoch 00002: val_loss improved from 0.59346 to 0.35197, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 226us/sample - loss: 0.5384 - categorical_accuracy: 0.7988 - val_loss: 0.3520 - val_categorical_accuracy: 0.8866\n",
      "Epoch 3/100\n",
      "40160/40221 [============================>.] - ETA: 0s - loss: 0.3619 - categorical_accuracy: 0.8847\n",
      "Epoch 00003: val_loss improved from 0.35197 to 0.28197, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 226us/sample - loss: 0.3616 - categorical_accuracy: 0.8848 - val_loss: 0.2820 - val_categorical_accuracy: 0.9085\n",
      "Epoch 4/100\n",
      "40000/40221 [============================>.] - ETA: 0s - loss: 0.2826 - categorical_accuracy: 0.9136\n",
      "Epoch 00004: val_loss improved from 0.28197 to 0.21319, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 227us/sample - loss: 0.2823 - categorical_accuracy: 0.9137 - val_loss: 0.2132 - val_categorical_accuracy: 0.9348\n",
      "Epoch 5/100\n",
      "40192/40221 [============================>.] - ETA: 0s - loss: 0.2458 - categorical_accuracy: 0.9248\n",
      "Epoch 00005: val_loss did not improve from 0.21319\n",
      "40221/40221 [==============================] - 9s 225us/sample - loss: 0.2458 - categorical_accuracy: 0.9248 - val_loss: 0.2224 - val_categorical_accuracy: 0.9304\n",
      "Epoch 6/100\n",
      "40192/40221 [============================>.] - ETA: 0s - loss: 0.2168 - categorical_accuracy: 0.9341\n",
      "Epoch 00006: val_loss improved from 0.21319 to 0.20524, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 226us/sample - loss: 0.2168 - categorical_accuracy: 0.9341 - val_loss: 0.2052 - val_categorical_accuracy: 0.9387\n",
      "Epoch 7/100\n",
      "40160/40221 [============================>.] - ETA: 0s - loss: 0.2036 - categorical_accuracy: 0.9396\n",
      "Epoch 00007: val_loss improved from 0.20524 to 0.18936, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 219us/sample - loss: 0.2037 - categorical_accuracy: 0.9396 - val_loss: 0.1894 - val_categorical_accuracy: 0.9449\n",
      "Epoch 8/100\n",
      "40128/40221 [============================>.] - ETA: 0s - loss: 0.1867 - categorical_accuracy: 0.9452\n",
      "Epoch 00008: val_loss improved from 0.18936 to 0.18215, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 220us/sample - loss: 0.1869 - categorical_accuracy: 0.9451 - val_loss: 0.1821 - val_categorical_accuracy: 0.9465\n",
      "Epoch 9/100\n",
      "39936/40221 [============================>.] - ETA: 0s - loss: 0.1775 - categorical_accuracy: 0.9477\n",
      "Epoch 00009: val_loss did not improve from 0.18215\n",
      "40221/40221 [==============================] - 9s 224us/sample - loss: 0.1776 - categorical_accuracy: 0.9477 - val_loss: 0.1919 - val_categorical_accuracy: 0.9409\n",
      "Epoch 10/100\n",
      "40032/40221 [============================>.] - ETA: 0s - loss: 0.1721 - categorical_accuracy: 0.9497\n",
      "Epoch 00010: val_loss did not improve from 0.18215\n",
      "40221/40221 [==============================] - 9s 227us/sample - loss: 0.1720 - categorical_accuracy: 0.9497 - val_loss: 0.1927 - val_categorical_accuracy: 0.9431\n",
      "Epoch 11/100\n",
      "39968/40221 [============================>.] - ETA: 0s - loss: 0.1621 - categorical_accuracy: 0.9530\n",
      "Epoch 00011: val_loss improved from 0.18215 to 0.16130, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 226us/sample - loss: 0.1621 - categorical_accuracy: 0.9530 - val_loss: 0.1613 - val_categorical_accuracy: 0.9527\n",
      "Epoch 12/100\n",
      "40000/40221 [============================>.] - ETA: 0s - loss: 0.1556 - categorical_accuracy: 0.9554\n",
      "Epoch 00012: val_loss improved from 0.16130 to 0.15988, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 226us/sample - loss: 0.1554 - categorical_accuracy: 0.9554 - val_loss: 0.1599 - val_categorical_accuracy: 0.9558\n",
      "Epoch 13/100\n",
      "39968/40221 [============================>.] - ETA: 0s - loss: 0.1520 - categorical_accuracy: 0.9561\n",
      "Epoch 00013: val_loss improved from 0.15988 to 0.15254, saving model to t_weights_0\n",
      "40221/40221 [==============================] - 9s 227us/sample - loss: 0.1518 - categorical_accuracy: 0.9561 - val_loss: 0.1525 - val_categorical_accuracy: 0.9566\n",
      "Epoch 14/100\n",
      "39968/40221 [============================>.] - ETA: 0s - loss: 0.1454 - categorical_accuracy: 0.9595\n",
      "Epoch 00014: val_loss did not improve from 0.15254\n",
      "40221/40221 [==============================] - 9s 227us/sample - loss: 0.1455 - categorical_accuracy: 0.9594 - val_loss: 0.1702 - val_categorical_accuracy: 0.9529\n",
      "Epoch 15/100\n",
      "39968/40221 [============================>.] - ETA: 0s - loss: 0.1435 - categorical_accuracy: 0.9595\n",
      "Epoch 00015: val_loss did not improve from 0.15254\n",
      "40221/40221 [==============================] - 9s 217us/sample - loss: 0.1435 - categorical_accuracy: 0.9595 - val_loss: 0.1737 - val_categorical_accuracy: 0.9495\n",
      "Epoch 16/100\n",
      "40128/40221 [============================>.] - ETA: 0s - loss: 0.1373 - categorical_accuracy: 0.9613\n",
      "Epoch 00016: val_loss did not improve from 0.15254\n",
      "40221/40221 [==============================] - 9s 226us/sample - loss: 0.1374 - categorical_accuracy: 0.9613 - val_loss: 0.1596 - val_categorical_accuracy: 0.9554\n",
      "Epoch 17/100\n",
      "40192/40221 [============================>.] - ETA: 0s - loss: 0.1347 - categorical_accuracy: 0.9617\n",
      "Epoch 00017: val_loss did not improve from 0.15254\n",
      "40221/40221 [==============================] - 9s 228us/sample - loss: 0.1347 - categorical_accuracy: 0.9617 - val_loss: 0.1694 - val_categorical_accuracy: 0.9525\n",
      "Epoch 18/100\n",
      "40032/40221 [============================>.] - ETA: 0s - loss: 0.1291 - categorical_accuracy: 0.9639\n",
      "Epoch 00018: val_loss did not improve from 0.15254\n",
      "40221/40221 [==============================] - 9s 212us/sample - loss: 0.1291 - categorical_accuracy: 0.9639 - val_loss: 0.1535 - val_categorical_accuracy: 0.9580\n",
      "0.9546449 0.7449106771915247\n",
      "\n",
      "\n",
      "nrx: 0 - real: 4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/Documents/txid_framework/dataset_scripts/working/data_utilities.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cls_weights = np.max(stat,axis=0)/stat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 2.2708 - categorical_accuracy: 0.1990\n",
      "Epoch 00001: val_loss improved from inf to 2.13156, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 1s 490us/sample - loss: 2.2681 - categorical_accuracy: 0.2013 - val_loss: 2.1316 - val_categorical_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "1344/1600 [========================>.....] - ETA: 0s - loss: 1.8939 - categorical_accuracy: 0.3490\n",
      "Epoch 00002: val_loss improved from 2.13156 to 1.30188, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 268us/sample - loss: 1.8358 - categorical_accuracy: 0.3694 - val_loss: 1.3019 - val_categorical_accuracy: 0.5300\n",
      "Epoch 3/100\n",
      "1440/1600 [==========================>...] - ETA: 0s - loss: 1.1994 - categorical_accuracy: 0.5681\n",
      "Epoch 00003: val_loss improved from 1.30188 to 0.67218, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 276us/sample - loss: 1.1830 - categorical_accuracy: 0.5681 - val_loss: 0.6722 - val_categorical_accuracy: 0.7750\n",
      "Epoch 4/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.7541 - categorical_accuracy: 0.7074\n",
      "Epoch 00004: val_loss improved from 0.67218 to 0.39071, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 241us/sample - loss: 0.7417 - categorical_accuracy: 0.7119 - val_loss: 0.3907 - val_categorical_accuracy: 0.8300\n",
      "Epoch 5/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.5297 - categorical_accuracy: 0.7863\n",
      "Epoch 00005: val_loss improved from 0.39071 to 0.27272, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 253us/sample - loss: 0.5174 - categorical_accuracy: 0.7937 - val_loss: 0.2727 - val_categorical_accuracy: 0.9500\n",
      "Epoch 6/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.3526 - categorical_accuracy: 0.8712\n",
      "Epoch 00006: val_loss improved from 0.27272 to 0.16983, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 267us/sample - loss: 0.3519 - categorical_accuracy: 0.8712 - val_loss: 0.1698 - val_categorical_accuracy: 0.9750\n",
      "Epoch 7/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.2743 - categorical_accuracy: 0.9102\n",
      "Epoch 00007: val_loss improved from 0.16983 to 0.15668, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 233us/sample - loss: 0.2718 - categorical_accuracy: 0.9106 - val_loss: 0.1567 - val_categorical_accuracy: 0.9950\n",
      "Epoch 8/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.2098 - categorical_accuracy: 0.9477\n",
      "Epoch 00008: val_loss improved from 0.15668 to 0.07025, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 288us/sample - loss: 0.2073 - categorical_accuracy: 0.9481 - val_loss: 0.0702 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.1431 - categorical_accuracy: 0.9701\n",
      "Epoch 00009: val_loss improved from 0.07025 to 0.04707, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 282us/sample - loss: 0.1441 - categorical_accuracy: 0.9706 - val_loss: 0.0471 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1472/1600 [==========================>...] - ETA: 0s - loss: 0.1133 - categorical_accuracy: 0.9776\n",
      "Epoch 00010: val_loss improved from 0.04707 to 0.04161, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.1181 - categorical_accuracy: 0.9731 - val_loss: 0.0416 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.0995 - categorical_accuracy: 0.9840\n",
      "Epoch 00011: val_loss improved from 0.04161 to 0.02491, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 261us/sample - loss: 0.1045 - categorical_accuracy: 0.9837 - val_loss: 0.0249 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0858 - categorical_accuracy: 0.9827\n",
      "Epoch 00012: val_loss did not improve from 0.02491\n",
      "1600/1600 [==============================] - 0s 238us/sample - loss: 0.0847 - categorical_accuracy: 0.9825 - val_loss: 0.0249 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0757 - categorical_accuracy: 0.9860\n",
      "Epoch 00013: val_loss improved from 0.02491 to 0.02052, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 284us/sample - loss: 0.0747 - categorical_accuracy: 0.9862 - val_loss: 0.0205 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 0.0648 - categorical_accuracy: 0.9902\n",
      "Epoch 00014: val_loss did not improve from 0.02052\n",
      "1600/1600 [==============================] - 0s 235us/sample - loss: 0.0652 - categorical_accuracy: 0.9900 - val_loss: 0.0210 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0679 - categorical_accuracy: 0.9907\n",
      "Epoch 00015: val_loss did not improve from 0.02052\n",
      "1600/1600 [==============================] - 0s 239us/sample - loss: 0.0662 - categorical_accuracy: 0.9912 - val_loss: 0.0215 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1376/1600 [========================>.....] - ETA: 0s - loss: 0.0658 - categorical_accuracy: 0.9891\n",
      "Epoch 00016: val_loss improved from 0.02052 to 0.01855, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 238us/sample - loss: 0.0617 - categorical_accuracy: 0.9900 - val_loss: 0.0185 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0495 - categorical_accuracy: 0.9936\n",
      "Epoch 00017: val_loss did not improve from 0.01855\n",
      "1600/1600 [==============================] - 0s 216us/sample - loss: 0.0561 - categorical_accuracy: 0.9900 - val_loss: 0.0353 - val_categorical_accuracy: 0.9950\n",
      "Epoch 18/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0533 - categorical_accuracy: 0.9887\n",
      "Epoch 00018: val_loss improved from 0.01855 to 0.01642, saving model to t_weights_0\n",
      "1600/1600 [==============================] - 0s 288us/sample - loss: 0.0526 - categorical_accuracy: 0.9887 - val_loss: 0.0164 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.0502 - categorical_accuracy: 0.9904\n",
      "Epoch 00019: val_loss did not improve from 0.01642\n",
      "1600/1600 [==============================] - 0s 234us/sample - loss: 0.0508 - categorical_accuracy: 0.9900 - val_loss: 0.0275 - val_categorical_accuracy: 0.9950\n",
      "Epoch 20/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0508 - categorical_accuracy: 0.9900\n",
      "Epoch 00020: val_loss did not improve from 0.01642\n",
      "1600/1600 [==============================] - 0s 243us/sample - loss: 0.0517 - categorical_accuracy: 0.9887 - val_loss: 0.0183 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1504/1600 [===========================>..] - ETA: 0s - loss: 0.0423 - categorical_accuracy: 0.9960\n",
      "Epoch 00021: val_loss did not improve from 0.01642\n",
      "1600/1600 [==============================] - 0s 236us/sample - loss: 0.0418 - categorical_accuracy: 0.9956 - val_loss: 0.0491 - val_categorical_accuracy: 0.9850\n",
      "Epoch 22/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0415 - categorical_accuracy: 0.9922\n",
      "Epoch 00022: val_loss did not improve from 0.01642\n",
      "1600/1600 [==============================] - 0s 217us/sample - loss: 0.0434 - categorical_accuracy: 0.9912 - val_loss: 0.0182 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1408/1600 [=========================>....] - ETA: 0s - loss: 0.0363 - categorical_accuracy: 0.9957\n",
      "Epoch 00023: val_loss did not improve from 0.01642\n",
      "1600/1600 [==============================] - 0s 212us/sample - loss: 0.0348 - categorical_accuracy: 0.9962 - val_loss: 0.0193 - val_categorical_accuracy: 1.0000\n",
      "0.985 0.3469387755102041\n",
      "\n",
      "\n",
      "nrx: 5 - real: 4 \n",
      "Train on 9416 samples, validate on 1177 samples\n",
      "Epoch 1/100\n",
      "9376/9416 [============================>.] - ETA: 0s - loss: 1.6435 - categorical_accuracy: 0.3776\n",
      "Epoch 00001: val_loss improved from inf to 0.92756, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 3s 281us/sample - loss: 1.6404 - categorical_accuracy: 0.3785 - val_loss: 0.9276 - val_categorical_accuracy: 0.6559\n",
      "Epoch 2/100\n",
      "9248/9416 [============================>.] - ETA: 0s - loss: 0.8435 - categorical_accuracy: 0.6630\n",
      "Epoch 00002: val_loss improved from 0.92756 to 0.62569, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 231us/sample - loss: 0.8444 - categorical_accuracy: 0.6640 - val_loss: 0.6257 - val_categorical_accuracy: 0.7664\n",
      "Epoch 3/100\n",
      "9184/9416 [============================>.] - ETA: 0s - loss: 0.6761 - categorical_accuracy: 0.7431\n",
      "Epoch 00003: val_loss improved from 0.62569 to 0.51817, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 234us/sample - loss: 0.6733 - categorical_accuracy: 0.7448 - val_loss: 0.5182 - val_categorical_accuracy: 0.7901\n",
      "Epoch 4/100\n",
      "9344/9416 [============================>.] - ETA: 0s - loss: 0.5381 - categorical_accuracy: 0.8108\n",
      "Epoch 00004: val_loss improved from 0.51817 to 0.42955, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 234us/sample - loss: 0.5384 - categorical_accuracy: 0.8106 - val_loss: 0.4296 - val_categorical_accuracy: 0.8420\n",
      "Epoch 5/100\n",
      "9248/9416 [============================>.] - ETA: 0s - loss: 0.4487 - categorical_accuracy: 0.8562\n",
      "Epoch 00005: val_loss improved from 0.42955 to 0.32361, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 234us/sample - loss: 0.4485 - categorical_accuracy: 0.8565 - val_loss: 0.3236 - val_categorical_accuracy: 0.8997\n",
      "Epoch 6/100\n",
      "9184/9416 [============================>.] - ETA: 0s - loss: 0.3783 - categorical_accuracy: 0.8806\n",
      "Epoch 00006: val_loss improved from 0.32361 to 0.27157, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 218us/sample - loss: 0.3777 - categorical_accuracy: 0.8805 - val_loss: 0.2716 - val_categorical_accuracy: 0.9176\n",
      "Epoch 7/100\n",
      "9408/9416 [============================>.] - ETA: 0s - loss: 0.3239 - categorical_accuracy: 0.9008\n",
      "Epoch 00007: val_loss improved from 0.27157 to 0.25594, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 217us/sample - loss: 0.3236 - categorical_accuracy: 0.9009 - val_loss: 0.2559 - val_categorical_accuracy: 0.9201\n",
      "Epoch 8/100\n",
      "9376/9416 [============================>.] - ETA: 0s - loss: 0.2900 - categorical_accuracy: 0.9125\n",
      "Epoch 00008: val_loss improved from 0.25594 to 0.22414, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 235us/sample - loss: 0.2908 - categorical_accuracy: 0.9125 - val_loss: 0.2241 - val_categorical_accuracy: 0.9278\n",
      "Epoch 9/100\n",
      "9344/9416 [============================>.] - ETA: 0s - loss: 0.2668 - categorical_accuracy: 0.9192\n",
      "Epoch 00009: val_loss improved from 0.22414 to 0.22007, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 234us/sample - loss: 0.2660 - categorical_accuracy: 0.9195 - val_loss: 0.2201 - val_categorical_accuracy: 0.9312\n",
      "Epoch 10/100\n",
      "9408/9416 [============================>.] - ETA: 0s - loss: 0.2416 - categorical_accuracy: 0.9281\n",
      "Epoch 00010: val_loss improved from 0.22007 to 0.21276, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 237us/sample - loss: 0.2414 - categorical_accuracy: 0.9282 - val_loss: 0.2128 - val_categorical_accuracy: 0.9354\n",
      "Epoch 11/100\n",
      "9376/9416 [============================>.] - ETA: 0s - loss: 0.2273 - categorical_accuracy: 0.9321\n",
      "Epoch 00011: val_loss did not improve from 0.21276\n",
      "9416/9416 [==============================] - 2s 226us/sample - loss: 0.2269 - categorical_accuracy: 0.9322 - val_loss: 0.2135 - val_categorical_accuracy: 0.9337\n",
      "Epoch 12/100\n",
      "9312/9416 [============================>.] - ETA: 0s - loss: 0.2141 - categorical_accuracy: 0.9359\n",
      "Epoch 00012: val_loss improved from 0.21276 to 0.20361, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 230us/sample - loss: 0.2142 - categorical_accuracy: 0.9361 - val_loss: 0.2036 - val_categorical_accuracy: 0.9371\n",
      "Epoch 13/100\n",
      "9216/9416 [============================>.] - ETA: 0s - loss: 0.1961 - categorical_accuracy: 0.9398\n",
      "Epoch 00013: val_loss improved from 0.20361 to 0.19642, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 231us/sample - loss: 0.1953 - categorical_accuracy: 0.9403 - val_loss: 0.1964 - val_categorical_accuracy: 0.9431\n",
      "Epoch 14/100\n",
      "9344/9416 [============================>.] - ETA: 0s - loss: 0.1851 - categorical_accuracy: 0.9427\n",
      "Epoch 00014: val_loss improved from 0.19642 to 0.18303, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 236us/sample - loss: 0.1849 - categorical_accuracy: 0.9428 - val_loss: 0.1830 - val_categorical_accuracy: 0.9473\n",
      "Epoch 15/100\n",
      "9248/9416 [============================>.] - ETA: 0s - loss: 0.1835 - categorical_accuracy: 0.9462\n",
      "Epoch 00015: val_loss did not improve from 0.18303\n",
      "9416/9416 [==============================] - 2s 226us/sample - loss: 0.1839 - categorical_accuracy: 0.9458 - val_loss: 0.1896 - val_categorical_accuracy: 0.9431\n",
      "Epoch 16/100\n",
      "9376/9416 [============================>.] - ETA: 0s - loss: 0.1717 - categorical_accuracy: 0.9507\n",
      "Epoch 00016: val_loss improved from 0.18303 to 0.18146, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 235us/sample - loss: 0.1715 - categorical_accuracy: 0.9507 - val_loss: 0.1815 - val_categorical_accuracy: 0.9482\n",
      "Epoch 17/100\n",
      "9344/9416 [============================>.] - ETA: 0s - loss: 0.1623 - categorical_accuracy: 0.9541\n",
      "Epoch 00017: val_loss did not improve from 0.18146\n",
      "9416/9416 [==============================] - 2s 226us/sample - loss: 0.1627 - categorical_accuracy: 0.9540 - val_loss: 0.1991 - val_categorical_accuracy: 0.9439\n",
      "Epoch 18/100\n",
      "9248/9416 [============================>.] - ETA: 0s - loss: 0.1582 - categorical_accuracy: 0.9548\n",
      "Epoch 00018: val_loss improved from 0.18146 to 0.17120, saving model to t_weights_0\n",
      "9416/9416 [==============================] - 2s 234us/sample - loss: 0.1584 - categorical_accuracy: 0.9547 - val_loss: 0.1712 - val_categorical_accuracy: 0.9533\n",
      "Epoch 19/100\n",
      "9344/9416 [============================>.] - ETA: 0s - loss: 0.1572 - categorical_accuracy: 0.9548\n",
      "Epoch 00019: val_loss did not improve from 0.17120\n",
      "9416/9416 [==============================] - 2s 231us/sample - loss: 0.1576 - categorical_accuracy: 0.9548 - val_loss: 0.2057 - val_categorical_accuracy: 0.9490\n",
      "Epoch 20/100\n",
      "9184/9416 [============================>.] - ETA: 0s - loss: 0.1430 - categorical_accuracy: 0.9606\n",
      "Epoch 00020: val_loss did not improve from 0.17120\n",
      "9416/9416 [==============================] - 2s 222us/sample - loss: 0.1441 - categorical_accuracy: 0.9602 - val_loss: 0.1937 - val_categorical_accuracy: 0.9473\n",
      "Epoch 21/100\n",
      "9248/9416 [============================>.] - ETA: 0s - loss: 0.1460 - categorical_accuracy: 0.9569\n",
      "Epoch 00021: val_loss did not improve from 0.17120\n",
      "9416/9416 [==============================] - 2s 226us/sample - loss: 0.1465 - categorical_accuracy: 0.9565 - val_loss: 0.2066 - val_categorical_accuracy: 0.9516\n",
      "Epoch 22/100\n",
      "9216/9416 [============================>.] - ETA: 0s - loss: 0.1428 - categorical_accuracy: 0.9590\n",
      "Epoch 00022: val_loss did not improve from 0.17120\n",
      "9416/9416 [==============================] - 2s 226us/sample - loss: 0.1431 - categorical_accuracy: 0.9589 - val_loss: 0.2119 - val_categorical_accuracy: 0.9473\n",
      "Epoch 23/100\n",
      "9376/9416 [============================>.] - ETA: 0s - loss: 0.1315 - categorical_accuracy: 0.9629\n",
      "Epoch 00023: val_loss did not improve from 0.17120\n",
      "9416/9416 [==============================] - 2s 227us/sample - loss: 0.1315 - categorical_accuracy: 0.9629 - val_loss: 0.2084 - val_categorical_accuracy: 0.9473\n",
      "0.9413764 0.5313265306122449\n",
      "\n",
      "\n",
      "nrx: 10 - real: 4 \n",
      "Train on 17256 samples, validate on 2157 samples\n",
      "Epoch 1/100\n",
      "17184/17256 [============================>.] - ETA: 0s - loss: 1.4896 - categorical_accuracy: 0.4210\n",
      "Epoch 00001: val_loss improved from inf to 0.81871, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 250us/sample - loss: 1.4884 - categorical_accuracy: 0.4215 - val_loss: 0.8187 - val_categorical_accuracy: 0.6931\n",
      "Epoch 2/100\n",
      "17152/17256 [============================>.] - ETA: 0s - loss: 0.7709 - categorical_accuracy: 0.7071\n",
      "Epoch 00002: val_loss improved from 0.81871 to 0.57243, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 229us/sample - loss: 0.7704 - categorical_accuracy: 0.7075 - val_loss: 0.5724 - val_categorical_accuracy: 0.8081\n",
      "Epoch 3/100\n",
      "17120/17256 [============================>.] - ETA: 0s - loss: 0.5556 - categorical_accuracy: 0.8051\n",
      "Epoch 00003: val_loss improved from 0.57243 to 0.42117, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 228us/sample - loss: 0.5549 - categorical_accuracy: 0.8054 - val_loss: 0.4212 - val_categorical_accuracy: 0.8618\n",
      "Epoch 4/100\n",
      "17088/17256 [============================>.] - ETA: 0s - loss: 0.4390 - categorical_accuracy: 0.8578\n",
      "Epoch 00004: val_loss improved from 0.42117 to 0.34273, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 234us/sample - loss: 0.4378 - categorical_accuracy: 0.8583 - val_loss: 0.3427 - val_categorical_accuracy: 0.8911\n",
      "Epoch 5/100\n",
      "17184/17256 [============================>.] - ETA: 0s - loss: 0.3690 - categorical_accuracy: 0.8806\n",
      "Epoch 00005: val_loss improved from 0.34273 to 0.32714, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 224us/sample - loss: 0.3689 - categorical_accuracy: 0.8806 - val_loss: 0.3271 - val_categorical_accuracy: 0.8846\n",
      "Epoch 6/100\n",
      "17248/17256 [============================>.] - ETA: 0s - loss: 0.3318 - categorical_accuracy: 0.8936\n",
      "Epoch 00006: val_loss improved from 0.32714 to 0.30711, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 218us/sample - loss: 0.3317 - categorical_accuracy: 0.8936 - val_loss: 0.3071 - val_categorical_accuracy: 0.8915\n",
      "Epoch 7/100\n",
      "17184/17256 [============================>.] - ETA: 0s - loss: 0.3025 - categorical_accuracy: 0.9044\n",
      "Epoch 00007: val_loss improved from 0.30711 to 0.27981, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 231us/sample - loss: 0.3022 - categorical_accuracy: 0.9045 - val_loss: 0.2798 - val_categorical_accuracy: 0.9045\n",
      "Epoch 8/100\n",
      "17152/17256 [============================>.] - ETA: 0s - loss: 0.2790 - categorical_accuracy: 0.9113\n",
      "Epoch 00008: val_loss did not improve from 0.27981\n",
      "17256/17256 [==============================] - 4s 224us/sample - loss: 0.2785 - categorical_accuracy: 0.9114 - val_loss: 0.2922 - val_categorical_accuracy: 0.9059\n",
      "Epoch 9/100\n",
      "17248/17256 [============================>.] - ETA: 0s - loss: 0.2639 - categorical_accuracy: 0.9152\n",
      "Epoch 00009: val_loss did not improve from 0.27981\n",
      "17256/17256 [==============================] - 4s 227us/sample - loss: 0.2640 - categorical_accuracy: 0.9151 - val_loss: 0.3096 - val_categorical_accuracy: 0.8943\n",
      "Epoch 10/100\n",
      "17184/17256 [============================>.] - ETA: 0s - loss: 0.2496 - categorical_accuracy: 0.9203\n",
      "Epoch 00010: val_loss improved from 0.27981 to 0.23914, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 229us/sample - loss: 0.2493 - categorical_accuracy: 0.9204 - val_loss: 0.2391 - val_categorical_accuracy: 0.9254\n",
      "Epoch 11/100\n",
      "17184/17256 [============================>.] - ETA: 0s - loss: 0.2317 - categorical_accuracy: 0.9250\n",
      "Epoch 00011: val_loss did not improve from 0.23914\n",
      "17256/17256 [==============================] - 4s 223us/sample - loss: 0.2314 - categorical_accuracy: 0.9251 - val_loss: 0.2403 - val_categorical_accuracy: 0.9230\n",
      "Epoch 12/100\n",
      "17120/17256 [============================>.] - ETA: 0s - loss: 0.2227 - categorical_accuracy: 0.9297\n",
      "Epoch 00012: val_loss improved from 0.23914 to 0.22975, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 230us/sample - loss: 0.2229 - categorical_accuracy: 0.9296 - val_loss: 0.2298 - val_categorical_accuracy: 0.9286\n",
      "Epoch 13/100\n",
      "17120/17256 [============================>.] - ETA: 0s - loss: 0.2151 - categorical_accuracy: 0.9324\n",
      "Epoch 00013: val_loss improved from 0.22975 to 0.21535, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 225us/sample - loss: 0.2149 - categorical_accuracy: 0.9324 - val_loss: 0.2154 - val_categorical_accuracy: 0.9351\n",
      "Epoch 14/100\n",
      "17216/17256 [============================>.] - ETA: 0s - loss: 0.2051 - categorical_accuracy: 0.9372\n",
      "Epoch 00014: val_loss improved from 0.21535 to 0.20853, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 230us/sample - loss: 0.2049 - categorical_accuracy: 0.9372 - val_loss: 0.2085 - val_categorical_accuracy: 0.9332\n",
      "Epoch 15/100\n",
      "17248/17256 [============================>.] - ETA: 0s - loss: 0.1966 - categorical_accuracy: 0.9382\n",
      "Epoch 00015: val_loss did not improve from 0.20853\n",
      "17256/17256 [==============================] - 4s 224us/sample - loss: 0.1967 - categorical_accuracy: 0.9382 - val_loss: 0.2412 - val_categorical_accuracy: 0.9263\n",
      "Epoch 16/100\n",
      "17056/17256 [============================>.] - ETA: 0s - loss: 0.1989 - categorical_accuracy: 0.9373\n",
      "Epoch 00016: val_loss did not improve from 0.20853\n",
      "17256/17256 [==============================] - 4s 224us/sample - loss: 0.1990 - categorical_accuracy: 0.9374 - val_loss: 0.2517 - val_categorical_accuracy: 0.9156\n",
      "Epoch 17/100\n",
      "17216/17256 [============================>.] - ETA: 0s - loss: 0.1891 - categorical_accuracy: 0.9398\n",
      "Epoch 00017: val_loss did not improve from 0.20853\n",
      "17256/17256 [==============================] - 4s 226us/sample - loss: 0.1890 - categorical_accuracy: 0.9398 - val_loss: 0.2228 - val_categorical_accuracy: 0.9337\n",
      "Epoch 18/100\n",
      "17184/17256 [============================>.] - ETA: 0s - loss: 0.1819 - categorical_accuracy: 0.9440\n",
      "Epoch 00018: val_loss did not improve from 0.20853\n",
      "17256/17256 [==============================] - 4s 223us/sample - loss: 0.1820 - categorical_accuracy: 0.9441 - val_loss: 0.2248 - val_categorical_accuracy: 0.9286\n",
      "Epoch 19/100\n",
      "17152/17256 [============================>.] - ETA: 0s - loss: 0.1878 - categorical_accuracy: 0.9415\n",
      "Epoch 00019: val_loss improved from 0.20853 to 0.20765, saving model to t_weights_0\n",
      "17256/17256 [==============================] - 4s 231us/sample - loss: 0.1874 - categorical_accuracy: 0.9416 - val_loss: 0.2076 - val_categorical_accuracy: 0.9383\n",
      "Epoch 20/100\n",
      "17120/17256 [============================>.] - ETA: 0s - loss: 0.1740 - categorical_accuracy: 0.9474\n",
      "Epoch 00020: val_loss did not improve from 0.20765\n",
      "17256/17256 [==============================] - 4s 223us/sample - loss: 0.1736 - categorical_accuracy: 0.9476 - val_loss: 0.2148 - val_categorical_accuracy: 0.9388\n",
      "Epoch 21/100\n",
      "17088/17256 [============================>.] - ETA: 0s - loss: 0.1659 - categorical_accuracy: 0.9489\n",
      "Epoch 00021: val_loss did not improve from 0.20765\n",
      "17256/17256 [==============================] - 4s 222us/sample - loss: 0.1655 - categorical_accuracy: 0.9490 - val_loss: 0.2120 - val_categorical_accuracy: 0.9416\n",
      "Epoch 22/100\n",
      "17120/17256 [============================>.] - ETA: 0s - loss: 0.1676 - categorical_accuracy: 0.9480\n",
      "Epoch 00022: val_loss did not improve from 0.20765\n",
      "17256/17256 [==============================] - 4s 217us/sample - loss: 0.1669 - categorical_accuracy: 0.9483 - val_loss: 0.2133 - val_categorical_accuracy: 0.9388\n",
      "Epoch 23/100\n",
      "17248/17256 [============================>.] - ETA: 0s - loss: 0.1589 - categorical_accuracy: 0.9522\n",
      "Epoch 00023: val_loss did not improve from 0.20765\n",
      "17256/17256 [==============================] - 4s 225us/sample - loss: 0.1589 - categorical_accuracy: 0.9522 - val_loss: 0.2187 - val_categorical_accuracy: 0.9379\n",
      "Epoch 24/100\n",
      "17120/17256 [============================>.] - ETA: 0s - loss: 0.1545 - categorical_accuracy: 0.9527\n",
      "Epoch 00024: val_loss did not improve from 0.20765\n",
      "17256/17256 [==============================] - 4s 206us/sample - loss: 0.1543 - categorical_accuracy: 0.9527 - val_loss: 0.2213 - val_categorical_accuracy: 0.9374\n",
      "0.9355586 0.6729591836734694\n",
      "\n",
      "\n",
      "nrx: 15 - real: 4 \n",
      "Train on 25096 samples, validate on 3137 samples\n",
      "Epoch 1/100\n",
      "24992/25096 [============================>.] - ETA: 0s - loss: 1.2873 - categorical_accuracy: 0.4856\n",
      "Epoch 00001: val_loss improved from inf to 0.73893, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 242us/sample - loss: 1.2853 - categorical_accuracy: 0.4860 - val_loss: 0.7389 - val_categorical_accuracy: 0.7042\n",
      "Epoch 2/100\n",
      "25088/25096 [============================>.] - ETA: 0s - loss: 0.6615 - categorical_accuracy: 0.7427\n",
      "Epoch 00002: val_loss improved from 0.73893 to 0.47277, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 226us/sample - loss: 0.6614 - categorical_accuracy: 0.7427 - val_loss: 0.4728 - val_categorical_accuracy: 0.8521\n",
      "Epoch 3/100\n",
      "25056/25096 [============================>.] - ETA: 0s - loss: 0.4529 - categorical_accuracy: 0.8491\n",
      "Epoch 00003: val_loss improved from 0.47277 to 0.34003, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 228us/sample - loss: 0.4530 - categorical_accuracy: 0.8491 - val_loss: 0.3400 - val_categorical_accuracy: 0.8884\n",
      "Epoch 4/100\n",
      "24960/25096 [============================>.] - ETA: 0s - loss: 0.3475 - categorical_accuracy: 0.8917\n",
      "Epoch 00004: val_loss improved from 0.34003 to 0.28334, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 230us/sample - loss: 0.3476 - categorical_accuracy: 0.8917 - val_loss: 0.2833 - val_categorical_accuracy: 0.9063\n",
      "Epoch 5/100\n",
      "25056/25096 [============================>.] - ETA: 0s - loss: 0.2976 - categorical_accuracy: 0.9083\n",
      "Epoch 00005: val_loss improved from 0.28334 to 0.24852, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 224us/sample - loss: 0.2977 - categorical_accuracy: 0.9082 - val_loss: 0.2485 - val_categorical_accuracy: 0.9200\n",
      "Epoch 6/100\n",
      "24992/25096 [============================>.] - ETA: 0s - loss: 0.2668 - categorical_accuracy: 0.9179\n",
      "Epoch 00006: val_loss improved from 0.24852 to 0.24001, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 229us/sample - loss: 0.2669 - categorical_accuracy: 0.9178 - val_loss: 0.2400 - val_categorical_accuracy: 0.9273\n",
      "Epoch 7/100\n",
      "25056/25096 [============================>.] - ETA: 0s - loss: 0.2432 - categorical_accuracy: 0.9254\n",
      "Epoch 00007: val_loss improved from 0.24001 to 0.20988, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 229us/sample - loss: 0.2430 - categorical_accuracy: 0.9254 - val_loss: 0.2099 - val_categorical_accuracy: 0.9366\n",
      "Epoch 8/100\n",
      "24864/25096 [============================>.] - ETA: 0s - loss: 0.2267 - categorical_accuracy: 0.9299\n",
      "Epoch 00008: val_loss improved from 0.20988 to 0.20638, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 224us/sample - loss: 0.2269 - categorical_accuracy: 0.9299 - val_loss: 0.2064 - val_categorical_accuracy: 0.9366\n",
      "Epoch 9/100\n",
      "25024/25096 [============================>.] - ETA: 0s - loss: 0.2126 - categorical_accuracy: 0.9347\n",
      "Epoch 00009: val_loss did not improve from 0.20638\n",
      "25096/25096 [==============================] - 5s 218us/sample - loss: 0.2127 - categorical_accuracy: 0.9347 - val_loss: 0.2128 - val_categorical_accuracy: 0.9378\n",
      "Epoch 10/100\n",
      "25024/25096 [============================>.] - ETA: 0s - loss: 0.2026 - categorical_accuracy: 0.9376\n",
      "Epoch 00010: val_loss improved from 0.20638 to 0.20079, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 230us/sample - loss: 0.2024 - categorical_accuracy: 0.9376 - val_loss: 0.2008 - val_categorical_accuracy: 0.9388\n",
      "Epoch 11/100\n",
      "24992/25096 [============================>.] - ETA: 0s - loss: 0.1920 - categorical_accuracy: 0.9419\n",
      "Epoch 00011: val_loss did not improve from 0.20079\n",
      "25096/25096 [==============================] - 6s 224us/sample - loss: 0.1918 - categorical_accuracy: 0.9421 - val_loss: 0.2031 - val_categorical_accuracy: 0.9420\n",
      "Epoch 12/100\n",
      "24992/25096 [============================>.] - ETA: 0s - loss: 0.1842 - categorical_accuracy: 0.9443\n",
      "Epoch 00012: val_loss improved from 0.20079 to 0.20018, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 228us/sample - loss: 0.1840 - categorical_accuracy: 0.9443 - val_loss: 0.2002 - val_categorical_accuracy: 0.9417\n",
      "Epoch 13/100\n",
      "25088/25096 [============================>.] - ETA: 0s - loss: 0.1805 - categorical_accuracy: 0.9461\n",
      "Epoch 00013: val_loss improved from 0.20018 to 0.18470, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 230us/sample - loss: 0.1804 - categorical_accuracy: 0.9462 - val_loss: 0.1847 - val_categorical_accuracy: 0.9442\n",
      "Epoch 14/100\n",
      "24992/25096 [============================>.] - ETA: 0s - loss: 0.1750 - categorical_accuracy: 0.9485\n",
      "Epoch 00014: val_loss did not improve from 0.18470\n",
      "25096/25096 [==============================] - 6s 223us/sample - loss: 0.1748 - categorical_accuracy: 0.9486 - val_loss: 0.1893 - val_categorical_accuracy: 0.9455\n",
      "Epoch 15/100\n",
      "25056/25096 [============================>.] - ETA: 0s - loss: 0.1654 - categorical_accuracy: 0.9497\n",
      "Epoch 00015: val_loss did not improve from 0.18470\n",
      "25096/25096 [==============================] - 6s 228us/sample - loss: 0.1652 - categorical_accuracy: 0.9497 - val_loss: 0.1915 - val_categorical_accuracy: 0.9474\n",
      "Epoch 16/100\n",
      "24960/25096 [============================>.] - ETA: 0s - loss: 0.1627 - categorical_accuracy: 0.9522\n",
      "Epoch 00016: val_loss did not improve from 0.18470\n",
      "25096/25096 [==============================] - 6s 224us/sample - loss: 0.1626 - categorical_accuracy: 0.9522 - val_loss: 0.1917 - val_categorical_accuracy: 0.9433\n",
      "Epoch 17/100\n",
      "24928/25096 [============================>.] - ETA: 0s - loss: 0.1617 - categorical_accuracy: 0.9513\n",
      "Epoch 00017: val_loss improved from 0.18470 to 0.17889, saving model to t_weights_0\n",
      "25096/25096 [==============================] - 6s 228us/sample - loss: 0.1616 - categorical_accuracy: 0.9515 - val_loss: 0.1789 - val_categorical_accuracy: 0.9493\n",
      "Epoch 18/100\n",
      "24960/25096 [============================>.] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9545\n",
      "Epoch 00018: val_loss did not improve from 0.17889\n",
      "25096/25096 [==============================] - 6s 228us/sample - loss: 0.1553 - categorical_accuracy: 0.9545 - val_loss: 0.2126 - val_categorical_accuracy: 0.9385\n",
      "Epoch 19/100\n",
      "24960/25096 [============================>.] - ETA: 0s - loss: 0.1496 - categorical_accuracy: 0.9560\n",
      "Epoch 00019: val_loss did not improve from 0.17889\n",
      "25096/25096 [==============================] - 6s 224us/sample - loss: 0.1495 - categorical_accuracy: 0.9560 - val_loss: 0.1792 - val_categorical_accuracy: 0.9509\n",
      "Epoch 20/100\n",
      "25088/25096 [============================>.] - ETA: 0s - loss: 0.1459 - categorical_accuracy: 0.9585\n",
      "Epoch 00020: val_loss did not improve from 0.17889\n",
      "25096/25096 [==============================] - 6s 220us/sample - loss: 0.1460 - categorical_accuracy: 0.9585 - val_loss: 0.1833 - val_categorical_accuracy: 0.9484\n",
      "Epoch 21/100\n",
      "25024/25096 [============================>.] - ETA: 0s - loss: 0.1407 - categorical_accuracy: 0.9593\n",
      "Epoch 00021: val_loss did not improve from 0.17889\n",
      "25096/25096 [==============================] - 6s 222us/sample - loss: 0.1406 - categorical_accuracy: 0.9593 - val_loss: 0.2040 - val_categorical_accuracy: 0.9417\n",
      "Epoch 22/100\n",
      "24992/25096 [============================>.] - ETA: 0s - loss: 0.1397 - categorical_accuracy: 0.9603\n",
      "Epoch 00022: val_loss did not improve from 0.17889\n",
      "25096/25096 [==============================] - 6s 226us/sample - loss: 0.1397 - categorical_accuracy: 0.9603 - val_loss: 0.1877 - val_categorical_accuracy: 0.9519\n",
      "0.9563277 0.7221428571428572\n",
      "\n",
      "\n",
      "nrx: 20 - real: 4 \n",
      "Train on 32682 samples, validate on 4085 samples\n",
      "Epoch 1/100\n",
      "32512/32682 [============================>.] - ETA: 0s - loss: 1.2064 - categorical_accuracy: 0.5050\n",
      "Epoch 00001: val_loss improved from inf to 0.71626, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 8s 234us/sample - loss: 1.2046 - categorical_accuracy: 0.5060 - val_loss: 0.7163 - val_categorical_accuracy: 0.6891\n",
      "Epoch 2/100\n",
      "32640/32682 [============================>.] - ETA: 0s - loss: 0.6613 - categorical_accuracy: 0.7263\n",
      "Epoch 00002: val_loss improved from 0.71626 to 0.46251, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 223us/sample - loss: 0.6612 - categorical_accuracy: 0.7265 - val_loss: 0.4625 - val_categorical_accuracy: 0.8328\n",
      "Epoch 3/100\n",
      "32640/32682 [============================>.] - ETA: 0s - loss: 0.4561 - categorical_accuracy: 0.8340\n",
      "Epoch 00003: val_loss improved from 0.46251 to 0.30893, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 224us/sample - loss: 0.4562 - categorical_accuracy: 0.8340 - val_loss: 0.3089 - val_categorical_accuracy: 0.9072\n",
      "Epoch 4/100\n",
      "32448/32682 [============================>.] - ETA: 0s - loss: 0.3325 - categorical_accuracy: 0.8925\n",
      "Epoch 00004: val_loss improved from 0.30893 to 0.30036, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 224us/sample - loss: 0.3323 - categorical_accuracy: 0.8926 - val_loss: 0.3004 - val_categorical_accuracy: 0.8952\n",
      "Epoch 5/100\n",
      "32640/32682 [============================>.] - ETA: 0s - loss: 0.2767 - categorical_accuracy: 0.9137\n",
      "Epoch 00005: val_loss improved from 0.30036 to 0.21714, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 222us/sample - loss: 0.2767 - categorical_accuracy: 0.9137 - val_loss: 0.2171 - val_categorical_accuracy: 0.9366\n",
      "Epoch 6/100\n",
      "32672/32682 [============================>.] - ETA: 0s - loss: 0.2454 - categorical_accuracy: 0.9241\n",
      "Epoch 00006: val_loss improved from 0.21714 to 0.21218, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 218us/sample - loss: 0.2454 - categorical_accuracy: 0.9241 - val_loss: 0.2122 - val_categorical_accuracy: 0.9334\n",
      "Epoch 7/100\n",
      "32672/32682 [============================>.] - ETA: 0s - loss: 0.2243 - categorical_accuracy: 0.9320\n",
      "Epoch 00007: val_loss improved from 0.21218 to 0.19208, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 223us/sample - loss: 0.2243 - categorical_accuracy: 0.9320 - val_loss: 0.1921 - val_categorical_accuracy: 0.9432\n",
      "Epoch 8/100\n",
      "32576/32682 [============================>.] - ETA: 0s - loss: 0.2077 - categorical_accuracy: 0.9381\n",
      "Epoch 00008: val_loss did not improve from 0.19208\n",
      "32682/32682 [==============================] - 7s 222us/sample - loss: 0.2078 - categorical_accuracy: 0.9381 - val_loss: 0.2011 - val_categorical_accuracy: 0.9378\n",
      "Epoch 9/100\n",
      "32672/32682 [============================>.] - ETA: 0s - loss: 0.1998 - categorical_accuracy: 0.9388\n",
      "Epoch 00009: val_loss did not improve from 0.19208\n",
      "32682/32682 [==============================] - 7s 223us/sample - loss: 0.1998 - categorical_accuracy: 0.9388 - val_loss: 0.1994 - val_categorical_accuracy: 0.9415\n",
      "Epoch 10/100\n",
      "32640/32682 [============================>.] - ETA: 0s - loss: 0.1926 - categorical_accuracy: 0.9411\n",
      "Epoch 00010: val_loss did not improve from 0.19208\n",
      "32682/32682 [==============================] - 7s 221us/sample - loss: 0.1926 - categorical_accuracy: 0.9411 - val_loss: 0.2008 - val_categorical_accuracy: 0.9412\n",
      "Epoch 11/100\n",
      "32480/32682 [============================>.] - ETA: 0s - loss: 0.1795 - categorical_accuracy: 0.9464\n",
      "Epoch 00011: val_loss improved from 0.19208 to 0.18063, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 222us/sample - loss: 0.1794 - categorical_accuracy: 0.9464 - val_loss: 0.1806 - val_categorical_accuracy: 0.9491\n",
      "Epoch 12/100\n",
      "32480/32682 [============================>.] - ETA: 0s - loss: 0.1776 - categorical_accuracy: 0.9479\n",
      "Epoch 00012: val_loss improved from 0.18063 to 0.17917, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 206us/sample - loss: 0.1774 - categorical_accuracy: 0.9479 - val_loss: 0.1792 - val_categorical_accuracy: 0.9493\n",
      "Epoch 13/100\n",
      "32672/32682 [============================>.] - ETA: 0s - loss: 0.1638 - categorical_accuracy: 0.9529\n",
      "Epoch 00013: val_loss did not improve from 0.17917\n",
      "32682/32682 [==============================] - 7s 219us/sample - loss: 0.1638 - categorical_accuracy: 0.9529 - val_loss: 0.1803 - val_categorical_accuracy: 0.9498\n",
      "Epoch 14/100\n",
      "32576/32682 [============================>.] - ETA: 0s - loss: 0.1604 - categorical_accuracy: 0.9530\n",
      "Epoch 00014: val_loss did not improve from 0.17917\n",
      "32682/32682 [==============================] - 7s 219us/sample - loss: 0.1603 - categorical_accuracy: 0.9529 - val_loss: 0.1834 - val_categorical_accuracy: 0.9508\n",
      "Epoch 15/100\n",
      "32544/32682 [============================>.] - ETA: 0s - loss: 0.1577 - categorical_accuracy: 0.9538\n",
      "Epoch 00015: val_loss improved from 0.17917 to 0.17137, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 217us/sample - loss: 0.1576 - categorical_accuracy: 0.9539 - val_loss: 0.1714 - val_categorical_accuracy: 0.9520\n",
      "Epoch 16/100\n",
      "32576/32682 [============================>.] - ETA: 0s - loss: 0.1544 - categorical_accuracy: 0.9553\n",
      "Epoch 00016: val_loss did not improve from 0.17137\n",
      "32682/32682 [==============================] - 7s 221us/sample - loss: 0.1544 - categorical_accuracy: 0.9553 - val_loss: 0.1770 - val_categorical_accuracy: 0.9515\n",
      "Epoch 17/100\n",
      "32448/32682 [============================>.] - ETA: 0s - loss: 0.1472 - categorical_accuracy: 0.9581\n",
      "Epoch 00017: val_loss improved from 0.17137 to 0.17089, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 222us/sample - loss: 0.1471 - categorical_accuracy: 0.9580 - val_loss: 0.1709 - val_categorical_accuracy: 0.9535\n",
      "Epoch 18/100\n",
      "32576/32682 [============================>.] - ETA: 0s - loss: 0.1414 - categorical_accuracy: 0.9587\n",
      "Epoch 00018: val_loss improved from 0.17089 to 0.16690, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 222us/sample - loss: 0.1416 - categorical_accuracy: 0.9587 - val_loss: 0.1669 - val_categorical_accuracy: 0.9545\n",
      "Epoch 19/100\n",
      "32448/32682 [============================>.] - ETA: 0s - loss: 0.1374 - categorical_accuracy: 0.9612\n",
      "Epoch 00019: val_loss did not improve from 0.16690\n",
      "32682/32682 [==============================] - 7s 220us/sample - loss: 0.1373 - categorical_accuracy: 0.9612 - val_loss: 0.1765 - val_categorical_accuracy: 0.9537\n",
      "Epoch 20/100\n",
      "32576/32682 [============================>.] - ETA: 0s - loss: 0.1350 - categorical_accuracy: 0.9625\n",
      "Epoch 00020: val_loss improved from 0.16690 to 0.16369, saving model to t_weights_0\n",
      "32682/32682 [==============================] - 7s 221us/sample - loss: 0.1347 - categorical_accuracy: 0.9625 - val_loss: 0.1637 - val_categorical_accuracy: 0.9547\n",
      "Epoch 21/100\n",
      "32480/32682 [============================>.] - ETA: 0s - loss: 0.1327 - categorical_accuracy: 0.9629\n",
      "Epoch 00021: val_loss did not improve from 0.16369\n",
      "32682/32682 [==============================] - 7s 219us/sample - loss: 0.1326 - categorical_accuracy: 0.9629 - val_loss: 0.1816 - val_categorical_accuracy: 0.9530\n",
      "Epoch 22/100\n",
      "32640/32682 [============================>.] - ETA: 0s - loss: 0.1270 - categorical_accuracy: 0.9646\n",
      "Epoch 00022: val_loss did not improve from 0.16369\n",
      "32682/32682 [==============================] - 7s 219us/sample - loss: 0.1270 - categorical_accuracy: 0.9646 - val_loss: 0.1725 - val_categorical_accuracy: 0.9545\n",
      "Epoch 23/100\n",
      "32480/32682 [============================>.] - ETA: 0s - loss: 0.1243 - categorical_accuracy: 0.9658\n",
      "Epoch 00023: val_loss did not improve from 0.16369\n",
      "32682/32682 [==============================] - 7s 211us/sample - loss: 0.1244 - categorical_accuracy: 0.9657 - val_loss: 0.1656 - val_categorical_accuracy: 0.9535\n",
      "Epoch 24/100\n",
      "32576/32682 [============================>.] - ETA: 0s - loss: 0.1214 - categorical_accuracy: 0.9671\n",
      "Epoch 00024: val_loss did not improve from 0.16369\n",
      "32682/32682 [==============================] - 7s 219us/sample - loss: 0.1213 - categorical_accuracy: 0.9671 - val_loss: 0.1687 - val_categorical_accuracy: 0.9554\n",
      "Epoch 25/100\n",
      "32448/32682 [============================>.] - ETA: 0s - loss: 0.1183 - categorical_accuracy: 0.9683\n",
      "Epoch 00025: val_loss did not improve from 0.16369\n",
      "32682/32682 [==============================] - 7s 220us/sample - loss: 0.1180 - categorical_accuracy: 0.9683 - val_loss: 0.1752 - val_categorical_accuracy: 0.9569\n",
      "0.9554468 0.7504081632653061\n",
      "\n",
      "\n",
      "nrx: 25 - real: 4 \n",
      "Train on 39923 samples, validate on 4990 samples\n",
      "Epoch 1/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 1.0823 - categorical_accuracy: 0.5550\n",
      "Epoch 00001: val_loss improved from inf to 0.63917, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 233us/sample - loss: 1.0816 - categorical_accuracy: 0.5552 - val_loss: 0.6392 - val_categorical_accuracy: 0.7307\n",
      "Epoch 2/100\n",
      "39712/39923 [============================>.] - ETA: 0s - loss: 0.5854 - categorical_accuracy: 0.7728\n",
      "Epoch 00002: val_loss improved from 0.63917 to 0.40254, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 224us/sample - loss: 0.5854 - categorical_accuracy: 0.7728 - val_loss: 0.4025 - val_categorical_accuracy: 0.8577\n",
      "Epoch 3/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.3954 - categorical_accuracy: 0.8683\n",
      "Epoch 00003: val_loss improved from 0.40254 to 0.26759, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.3952 - categorical_accuracy: 0.8682 - val_loss: 0.2676 - val_categorical_accuracy: 0.9160\n",
      "Epoch 4/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.3085 - categorical_accuracy: 0.9046\n",
      "Epoch 00004: val_loss improved from 0.26759 to 0.22845, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 225us/sample - loss: 0.3084 - categorical_accuracy: 0.9046 - val_loss: 0.2284 - val_categorical_accuracy: 0.9267\n",
      "Epoch 5/100\n",
      "39712/39923 [============================>.] - ETA: 0s - loss: 0.2647 - categorical_accuracy: 0.9171\n",
      "Epoch 00005: val_loss improved from 0.22845 to 0.21553, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 218us/sample - loss: 0.2648 - categorical_accuracy: 0.9171 - val_loss: 0.2155 - val_categorical_accuracy: 0.9277\n",
      "Epoch 6/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.2391 - categorical_accuracy: 0.9262\n",
      "Epoch 00006: val_loss improved from 0.21553 to 0.20012, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.2391 - categorical_accuracy: 0.9262 - val_loss: 0.2001 - val_categorical_accuracy: 0.9369\n",
      "Epoch 7/100\n",
      "39840/39923 [============================>.] - ETA: 0s - loss: 0.2205 - categorical_accuracy: 0.9319\n",
      "Epoch 00007: val_loss improved from 0.20012 to 0.18722, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 225us/sample - loss: 0.2205 - categorical_accuracy: 0.9318 - val_loss: 0.1872 - val_categorical_accuracy: 0.9405\n",
      "Epoch 8/100\n",
      "39744/39923 [============================>.] - ETA: 0s - loss: 0.2034 - categorical_accuracy: 0.9383\n",
      "Epoch 00008: val_loss improved from 0.18722 to 0.18170, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 223us/sample - loss: 0.2032 - categorical_accuracy: 0.9385 - val_loss: 0.1817 - val_categorical_accuracy: 0.9459\n",
      "Epoch 9/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.1914 - categorical_accuracy: 0.9426\n",
      "Epoch 00009: val_loss improved from 0.18170 to 0.17948, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 224us/sample - loss: 0.1915 - categorical_accuracy: 0.9426 - val_loss: 0.1795 - val_categorical_accuracy: 0.9437\n",
      "Epoch 10/100\n",
      "39744/39923 [============================>.] - ETA: 0s - loss: 0.1866 - categorical_accuracy: 0.9440\n",
      "Epoch 00010: val_loss improved from 0.17948 to 0.17759, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1866 - categorical_accuracy: 0.9440 - val_loss: 0.1776 - val_categorical_accuracy: 0.9463\n",
      "Epoch 11/100\n",
      "39776/39923 [============================>.] - ETA: 0s - loss: 0.1742 - categorical_accuracy: 0.9481\n",
      "Epoch 00011: val_loss improved from 0.17759 to 0.17163, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 223us/sample - loss: 0.1741 - categorical_accuracy: 0.9482 - val_loss: 0.1716 - val_categorical_accuracy: 0.9503\n",
      "Epoch 12/100\n",
      "39712/39923 [============================>.] - ETA: 0s - loss: 0.1676 - categorical_accuracy: 0.9507\n",
      "Epoch 00012: val_loss did not improve from 0.17163\n",
      "39923/39923 [==============================] - 9s 218us/sample - loss: 0.1677 - categorical_accuracy: 0.9507 - val_loss: 0.1775 - val_categorical_accuracy: 0.9467\n",
      "Epoch 13/100\n",
      "39744/39923 [============================>.] - ETA: 0s - loss: 0.1608 - categorical_accuracy: 0.9526\n",
      "Epoch 00013: val_loss improved from 0.17163 to 0.16614, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1608 - categorical_accuracy: 0.9526 - val_loss: 0.1661 - val_categorical_accuracy: 0.9523\n",
      "Epoch 14/100\n",
      "39680/39923 [============================>.] - ETA: 0s - loss: 0.1572 - categorical_accuracy: 0.9545\n",
      "Epoch 00014: val_loss did not improve from 0.16614\n",
      "39923/39923 [==============================] - 9s 223us/sample - loss: 0.1571 - categorical_accuracy: 0.9545 - val_loss: 0.1749 - val_categorical_accuracy: 0.9505\n",
      "Epoch 15/100\n",
      "39744/39923 [============================>.] - ETA: 0s - loss: 0.1512 - categorical_accuracy: 0.9560\n",
      "Epoch 00015: val_loss did not improve from 0.16614\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1515 - categorical_accuracy: 0.9559 - val_loss: 0.1676 - val_categorical_accuracy: 0.9513\n",
      "Epoch 16/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.1441 - categorical_accuracy: 0.9592\n",
      "Epoch 00016: val_loss improved from 0.16614 to 0.16536, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 224us/sample - loss: 0.1441 - categorical_accuracy: 0.9592 - val_loss: 0.1654 - val_categorical_accuracy: 0.9531\n",
      "Epoch 17/100\n",
      "39680/39923 [============================>.] - ETA: 0s - loss: 0.1413 - categorical_accuracy: 0.9602\n",
      "Epoch 00017: val_loss improved from 0.16536 to 0.16126, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 223us/sample - loss: 0.1411 - categorical_accuracy: 0.9603 - val_loss: 0.1613 - val_categorical_accuracy: 0.9565\n",
      "Epoch 18/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.1371 - categorical_accuracy: 0.9622\n",
      "Epoch 00018: val_loss improved from 0.16126 to 0.16119, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1371 - categorical_accuracy: 0.9622 - val_loss: 0.1612 - val_categorical_accuracy: 0.9539\n",
      "Epoch 19/100\n",
      "39712/39923 [============================>.] - ETA: 0s - loss: 0.1314 - categorical_accuracy: 0.9634\n",
      "Epoch 00019: val_loss improved from 0.16119 to 0.14931, saving model to t_weights_0\n",
      "39923/39923 [==============================] - 9s 217us/sample - loss: 0.1315 - categorical_accuracy: 0.9633 - val_loss: 0.1493 - val_categorical_accuracy: 0.9585\n",
      "Epoch 20/100\n",
      "39872/39923 [============================>.] - ETA: 0s - loss: 0.1275 - categorical_accuracy: 0.9650\n",
      "Epoch 00020: val_loss did not improve from 0.14931\n",
      "39923/39923 [==============================] - 9s 223us/sample - loss: 0.1274 - categorical_accuracy: 0.9651 - val_loss: 0.1534 - val_categorical_accuracy: 0.9561\n",
      "Epoch 21/100\n",
      "39904/39923 [============================>.] - ETA: 0s - loss: 0.1235 - categorical_accuracy: 0.9664\n",
      "Epoch 00021: val_loss did not improve from 0.14931\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1234 - categorical_accuracy: 0.9664 - val_loss: 0.1662 - val_categorical_accuracy: 0.9557\n",
      "Epoch 22/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.1196 - categorical_accuracy: 0.9673\n",
      "Epoch 00022: val_loss did not improve from 0.14931\n",
      "39923/39923 [==============================] - 9s 221us/sample - loss: 0.1195 - categorical_accuracy: 0.9674 - val_loss: 0.1529 - val_categorical_accuracy: 0.9567\n",
      "Epoch 23/100\n",
      "39744/39923 [============================>.] - ETA: 0s - loss: 0.1204 - categorical_accuracy: 0.9671\n",
      "Epoch 00023: val_loss did not improve from 0.14931\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1204 - categorical_accuracy: 0.9671 - val_loss: 0.1636 - val_categorical_accuracy: 0.9567\n",
      "Epoch 24/100\n",
      "39808/39923 [============================>.] - ETA: 0s - loss: 0.1149 - categorical_accuracy: 0.9692\n",
      "Epoch 00024: val_loss did not improve from 0.14931\n",
      "39923/39923 [==============================] - 9s 222us/sample - loss: 0.1149 - categorical_accuracy: 0.9692 - val_loss: 0.1518 - val_categorical_accuracy: 0.9585\n",
      "0.9597194 0.7018367346938775\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "continue_training = True\n",
    "nreal = 5\n",
    "\n",
    "real_list = list(range(nreal))\n",
    "nrx_list =  list(range( 0,len(rx_list_real[0])-n_test_rx+1,5)) \n",
    "\n",
    "patience = 5\n",
    "n_epochs = 100\n",
    "\n",
    "smTest_results = []\n",
    "dfTest_results = []\n",
    "dfTestBal_results = []\n",
    "\n",
    "for real in real_list:\n",
    "    rx_list = rx_list_real[real]\n",
    "    rx_test_list = rx_list[-n_test_rx:]\n",
    "    test_dataset =  merge_compact_dataset(compact_dataset,capture_date,tx_list,rx_test_list,equalized=equalized)\n",
    "    test_augset_dfRx,_,_ = prepare_dataset(test_dataset,tx_list,val_frac=0.0, test_frac=0.0)\n",
    "\n",
    "    [sig_dfTest,txidNum_dfTest,txid_dfTest,cls_weights] = test_augset_dfRx\n",
    "\n",
    "    cnt=np.histogram(txidNum_dfTest,bins=np.arange(len(tx_list)+1)-0.5)\n",
    "    n_test_samples = int(np.min(cnt[0]))\n",
    "\n",
    "    smTest_results_real = []\n",
    "    dfTest_results_real = []\n",
    "    dfTestBal_results_real = []\n",
    "    for nrx in nrx_list:\n",
    "        print(\"\");print(\"\")\n",
    "        print(\"nrx: {} - real: {} \".format(nrx,real))\n",
    "        fname_w = 'weights/d011_{:02d}_{:02d}.hd5'.format(nrx,real)\n",
    "        rx_train_list= rx_list[:nrx+1]\n",
    "\n",
    "        dataset =  merge_compact_dataset(compact_dataset,capture_date,tx_list,rx_train_list,equalized=equalized)\n",
    "\n",
    "        train_augset,val_augset,test_augset_smRx =  prepare_dataset(dataset,tx_list,\n",
    "                                                            val_frac=0.1, test_frac=0.1)\n",
    "        [sig_train,txidNum_train,txid_train,cls_weights] = train_augset\n",
    "        [sig_valid,txidNum_valid,txid_valid,_] = val_augset\n",
    "        [sig_smTest,txidNum_smTest,txid_smTest,cls_weights] = test_augset_smRx\n",
    "        \n",
    "        if continue_training:\n",
    "            skip = os.path.isfile(fname_w) or os.path.isfile(fname_w+'.index')\n",
    "        else:\n",
    "            skip = False\n",
    "        classifier = create_net()\n",
    "        if TRAIN and not skip:\n",
    "            filepath = 't_weights_'+GPU\n",
    "            c=[ keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "              keras.callbacks.EarlyStopping(monitor='val_loss',  patience=patience)]\n",
    "            history = classifier.fit(sig_train,txid_train,class_weight=cls_weights,\n",
    "                                     validation_data=(sig_valid , txid_valid),callbacks=c, epochs=n_epochs)\n",
    "            classifier.load_weights(filepath)\n",
    "            classifier.save_weights(fname_w)\n",
    "        else:\n",
    "            classifier.load_weights(fname_w).expect_partial()\n",
    "\n",
    "        smTest_r = classifier.evaluate(sig_smTest,txid_smTest,verbose=0)[1]\n",
    "    #     dfTest_r = classifier.evaluate(sig_dfTest,txid_dfTest)[1]\n",
    "        dfTest_r,dfTestBal_r = evaluate_test(classifier)\n",
    "\n",
    "        print(smTest_r,dfTest_r)\n",
    "        smTest_results_real.append(smTest_r)\n",
    "        dfTest_results_real.append(dfTest_r)\n",
    "        dfTestBal_results_real.append(dfTestBal_r)\n",
    "        K.clear_session()\n",
    "    smTest_results.append(smTest_results_real)\n",
    "    dfTest_results.append(dfTest_results_real)\n",
    "    dfTestBal_results.append(dfTestBal_results_real)    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 10, 15, 20, 25]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32337888499973955, 0.5424953835515883, 0.672989333323014, 0.7467292965345177, 0.7870676739712519, 0.7908500635310862]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "plt.errorbar(np.array(nrx_list)+1,np.mean(smTest_results,0),np.std(smTest_results,0),capsize=4)\n",
    "plt.errorbar(np.array(nrx_list)+1,np.mean(dfTest_results,0),np.std(dfTest_results,0),capsize=4)\n",
    "plt.legend(['Same Rx(s)','Diff. Rx'])\n",
    "plt.xlabel('N Train Rx')\n",
    "plt.ylabel('Class. Accuracy')\n",
    "#plt.xticks(range(0,len(nrx_list),2))\n",
    "plt.grid()\n",
    "print(np.mean(dfTest_results,0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1-10', '11-1', '14-10', '14-7', '17-11', '20-15', '20-19', '7-11', '7-14', '8-20']\n",
      "[0, 5, 10, 15, 20, 25]\n",
      "[0, 1, 2, 3, 4]\n",
      "[[0.93, 0.8940678, 0.93426573, 0.9349855, 0.9390813, 0.9486974], [1.0, 0.9813333, 0.98435974, 0.97603416, 0.95185554, 0.9362086], [0.995, 0.9895105, 0.9354078, 0.9435563, 0.94364804, 0.94269794], [0.98888886, 0.98706895, 0.9722222, 0.95170176, 0.9557984, 0.9546449], [0.985, 0.9413764, 0.9355586, 0.9563277, 0.9554468, 0.9597194]]\n",
      "[[0.3165, 0.5837, 0.767, 0.8163, 0.8418, 0.8252], [0.3539, 0.6334, 0.6731, 0.8027, 0.8417, 0.8553], [0.3097758405977584, 0.5153590701535907, 0.7284142797841427, 0.7468866749688667, 0.7787463677874636, 0.8270029057700291], [0.28977980889073535, 0.44869131699210635, 0.5234732031574574, 0.6456169505608641, 0.7226838388034899, 0.7449106771915247], [0.3469387755102041, 0.5313265306122449, 0.6729591836734694, 0.7221428571428572, 0.7504081632653061, 0.7018367346938775]]\n",
      "[[0.3165, 0.5837, 0.767, 0.8163, 0.8418, 0.8252], [0.3539, 0.6334, 0.6731, 0.8027, 0.8417, 0.8553], [0.312125, 0.514, 0.72475, 0.742125, 0.776, 0.821125], [0.28694267515923566, 0.43471337579617836, 0.5210191082802548, 0.639171974522293, 0.7125796178343949, 0.745063694267516], [0.336875, 0.534375, 0.67575, 0.725875, 0.75375, 0.708]]\n"
     ]
    }
   ],
   "source": [
    "print(tx_list)\n",
    "print(nrx_list)\n",
    "print(real_list)\n",
    "print(smTest_results)\n",
    "print(dfTest_results)\n",
    "print(dfTestBal_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['19-20', '24-13', '19-2', '1-20', '20-20', '20-1', '7-7', '3-19', '23-6', '2-19', '24-5', '14-7', '23-1', '19-1', '8-7', '24-6', '24-16', '1-19', '8-8', '18-19', '13-7', '23-3', '8-14', '23-5', '19-19', '18-2', '7-14', '13-14', '1-1', '23-7', '20-19', '2-1'], ['1-1', '23-1', '24-6', '8-8', '1-19', '23-3', '23-5', '7-14', '19-19', '13-14', '23-6', '7-7', '19-1', '20-1', '20-20', '24-16', '8-14', '19-2', '14-7', '1-20', '13-7', '24-5', '18-2', '2-19', '24-13', '19-20', '3-19', '8-7', '20-19', '2-1', '23-7', '18-19'], ['24-5', '23-7', '18-19', '23-3', '24-6', '8-14', '2-1', '13-7', '19-19', '19-2', '18-2', '1-20', '20-1', '24-16', '3-19', '1-19', '24-13', '23-6', '19-1', '8-7', '20-19', '1-1', '14-7', '20-20', '7-7', '2-19', '23-5', '8-8', '19-20', '13-14', '7-14', '23-1'], ['1-20', '1-19', '20-19', '23-7', '2-1', '20-1', '19-19', '14-7', '23-1', '3-19', '8-8', '7-7', '13-7', '20-20', '24-16', '18-2', '8-7', '23-5', '7-14', '18-19', '24-6', '19-1', '13-14', '2-19', '19-20', '24-5', '23-3', '19-2', '8-14', '23-6', '24-13', '1-1'], ['18-19', '20-1', '13-14', '18-2', '14-7', '20-20', '13-7', '19-20', '2-19', '19-19', '19-1', '1-20', '24-5', '20-19', '8-7', '23-1', '7-7', '8-8', '2-1', '1-19', '3-19', '23-3', '23-6', '23-5', '24-6', '8-14', '24-16', '7-14', '1-1', '19-2', '24-13', '23-7']]\n"
     ]
    }
   ],
   "source": [
    "print(rx_list_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
